{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical LSTM End-to-End with Power (Which is the Game Score) Feature\n",
    "\n",
    "We are implementing this as a base model for the interim submissoin . The configurations which we are using are as follows :\n",
    "\n",
    "- **Dataset Reader:** It is implementation of `diplomacy_reader` from `game_reader.py`, reading conversations from JSONL files using `sender_labels` and extracting `game_score_delta` as a power feature.\n",
    "- **Embedding:** A 200-dimensional embedding layer is initialized . \n",
    "- **Message Encoder:** A bidirectional LSTM (hidden size=100) with max pooling over tokens yields a 200-dimensional message vector.\n",
    "- **Conversation Encoder:** A unidirectional LSTM (hidden size=200) processes the sequence of message vectors.\n",
    "- **Power Feature:** The game score delta is concatenated (as a scalar feature) to the conversation encoder output for each message before classification.\n",
    "- **Dropout:** 0.2\n",
    "- **Loss Weighting:** pos_weight=10 for the positive class.\n",
    "- **Optimizer:** Adam with lr=0.003 and gradient clipping (max norm 1).\n",
    "- **Training:** 80 epochs, batch size 1 (which remains fixed due to memory constraints).\n",
    "\n",
    "To help improve the macro F1 score with minimal changes, we add a learning rate scheduler (ReduceLROnPlateau) that reduces the learning rate if the validation loss plateaus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:16:36.670187Z",
     "iopub.status.busy": "2025-03-23T07:16:36.669939Z",
     "iopub.status.idle": "2025-03-23T07:16:40.467875Z",
     "shell.execute_reply": "2025-03-23T07:16:40.466962Z",
     "shell.execute_reply.started": "2025-03-23T07:16:36.670166Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Set seeds as in the original code base . \n",
    "torch.manual_seed(1994)\n",
    "np.random.seed(1994)\n",
    "random.seed(1994)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File Paths & Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:16:40.469199Z",
     "iopub.status.busy": "2025-03-23T07:16:40.468755Z",
     "iopub.status.idle": "2025-03-23T07:16:40.474374Z",
     "shell.execute_reply": "2025-03-23T07:16:40.473378Z",
     "shell.execute_reply.started": "2025-03-23T07:16:40.469172Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data paths \n",
    "TRAIN_PATH = r\"C:\\Git\\NLP_Project\\NLP_Project\\data\\train.jsonl\"\n",
    "VAL_PATH = r\"C:\\Git\\NLP_Project\\NLP_Project\\data\\validation.jsonl\"\n",
    "TEST_PATH = r\"C:\\Git\\NLP_Project\\NLP_Project\\data\\test.jsonl\"\n",
    "\n",
    "# Pretrained GloVe embeddings file path\n",
    "GLOVE_PATH = \"/kaggle/input/glove-embeddings/glove.twitter.27B.200d.txt\" # 200d embeddings \n",
    "\n",
    "# Model hyperparameters\n",
    "EMBED_DIM = 200       # embedding dimension\n",
    "MSG_HIDDEN = 100      # hidden size for message encoder (bidirectional -> output 200)\n",
    "CONV_HIDDEN = 200     # hidden size for conversation encoder\n",
    "DROPOUT = 0.2        # dropout rate\n",
    "BATCH_SIZE = 1       # batch size\n",
    "EPOCHS = 80         # number of epochs \n",
    "LR = 0.003         # learning rate\n",
    "GRAD_CLIP = 1.0    # gradient clipping \n",
    "\n",
    "# Loss pos_weight (for positive class, e.g. lies)\n",
    "POS_WEIGHT = 10.0\n",
    "\n",
    "# Preprocessing tokens\n",
    "PAD_TOKEN = \"<PAD>\"\n",
    "UNK_TOKEN = \"<UNK>\"\n",
    "\n",
    "# Use game scores (power) as additional feature\n",
    "USE_GAME_SCORES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Making (Diplomacy Reader with Power)\n",
    "\n",
    "This dataset class reads a line at a time from a JSONL file and yields conversations as well as game score deltas. It emulates your `diplomacy_reader` by ignoring messages with incorrect labels and decodes game scores as floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:16:40.475752Z",
     "iopub.status.busy": "2025-03-23T07:16:40.475460Z",
     "iopub.status.idle": "2025-03-23T07:16:40.492372Z",
     "shell.execute_reply": "2025-03-23T07:16:40.491608Z",
     "shell.execute_reply.started": "2025-03-23T07:16:40.475723Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this is the diplomacy dataset class \n",
    "class DiplomacyDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    def __init__(self, path, max_tokens_per_msg=50, max_messages=50, use_game_scores=False):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.max_tokens_per_msg = max_tokens_per_msg\n",
    "        self.max_messages = max_messages\n",
    "        self.use_game_scores = use_game_scores\n",
    "        \n",
    "        # Read and process each line in the JSONL file.\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                record = json.loads(line)\n",
    "                messages = record.get(\"messages\", [])\n",
    "                labels = record.get(\"sender_labels\", [])\n",
    "                # Only use game_score_delta if use_game_scores is True, otherwise default to None.\n",
    "                game_scores = record.get(\"game_score_delta\", None) if use_game_scores else None\n",
    "                \n",
    "                filtered_msgs, filtered_lbls = [], []\n",
    "                filtered_scores = []\n",
    "                # If game scores are missing, set them to zero for every message.\n",
    "                if game_scores is None:\n",
    "                    game_scores = [0] * len(messages)\n",
    "                \n",
    "                # Iterate through messages, labels, and game scores in parallel.\n",
    "                for m, l, g in zip(messages, labels, game_scores):\n",
    "                    # Accept only valid boolean labels (or their string representations).\n",
    "                    if l in [True, False, \"true\", \"false\", \"True\", \"False\"]:\n",
    "                        filtered_msgs.append(m)\n",
    "                        # Convert string labels to 0/1 (0 for false, 1 for true).\n",
    "                        if isinstance(l, str):\n",
    "                            filtered_lbls.append(1 if l.lower() == \"true\" else 0)\n",
    "                        else:\n",
    "                            filtered_lbls.append(1 if l else 0)\n",
    "                        filtered_scores.append(g)\n",
    "                \n",
    "                # Skip records with no valid messages.\n",
    "                if len(filtered_msgs) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Store the processed conversation as a tuple of (messages, labels, game scores).\n",
    "                self.data.append((filtered_msgs, filtered_lbls, filtered_scores))\n",
    "        \n",
    "        # Build vocabulary from the dataset.\n",
    "        self._build_vocab()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "     \n",
    "        tokens = text.lower().replace(\"\\n\", \" \").split()\n",
    "        out = []\n",
    "        for t in tokens:\n",
    "            if any(ch.isdigit() for ch in t):\n",
    "                out.append(\"<NUM>\")\n",
    "            else:\n",
    "                out.append(t)\n",
    "        return out\n",
    "\n",
    "    def _build_vocab(self):\n",
    "      \n",
    "        token_freq = Counter()\n",
    "        for conv, _, _ in self.data:\n",
    "            for msg in conv:\n",
    "                tokens = self._tokenize(msg)\n",
    "                token_freq.update(tokens)\n",
    "        \n",
    "        # Initialize vocabulary with special tokens.\n",
    "        self.ix2tok = [PAD_TOKEN, UNK_TOKEN]\n",
    "        # Add tokens in order of decreasing frequency.\n",
    "        for tok, freq in token_freq.most_common():\n",
    "            self.ix2tok.append(tok)\n",
    "        # Create the token-to-index mapping.\n",
    "        self.tok2ix = {t: i for i, t in enumerate(self.ix2tok)}\n",
    "\n",
    "    def __len__(self):\n",
    "       \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        conv, lbls, scores = self.data[idx]\n",
    "        tokenized_conv = []\n",
    "        for msg in conv:\n",
    "            toks = self._tokenize(msg)\n",
    "            # Convert tokens to indices using the vocabulary, defaulting to UNK_TOKEN if not found.\n",
    "            tok_ix = [self.tok2ix.get(t, self.tok2ix[UNK_TOKEN]) for t in toks]\n",
    "            tokenized_conv.append(tok_ix)\n",
    "        # Convert each game score to a float.\n",
    "        scores = [float(s) for s in scores]\n",
    "        return tokenized_conv, lbls, scores\n",
    "\n",
    "\n",
    "# Custom collate function to pad sequences for a batch of conversations.\n",
    "\n",
    "def collate_fn(batch):\n",
    "   \n",
    "    # Determine maximum number of messages in any conversation in the batch.\n",
    "    max_msg_count = max(len(item[0]) for item in batch)\n",
    "    # Determine maximum number of tokens in any message.\n",
    "    max_token_count = 0\n",
    "    for item in batch:\n",
    "        for msg in item[0]:\n",
    "            max_token_count = max(max_token_count, len(msg))\n",
    "    \n",
    "    padded_tokens = []\n",
    "    padded_labels = []\n",
    "    mask = []\n",
    "    padded_scores = []\n",
    "    \n",
    "    # For each conversation in the batch...\n",
    "    for conv, lbls, scores in batch:\n",
    "        num_msgs = len(conv)\n",
    "        conv_tokens = []\n",
    "        conv_labels = []\n",
    "        conv_mask = []\n",
    "        conv_scores = []\n",
    "        # Pad or truncate each conversation to max_msg_count messages.\n",
    "        for i in range(max_msg_count):\n",
    "            if i < num_msgs:\n",
    "                # Pad the message to max_token_count tokens.\n",
    "                msg = conv[i] + [0]*(max_token_count - len(conv[i]))\n",
    "                conv_tokens.append(msg)\n",
    "                conv_labels.append(lbls[i])\n",
    "                conv_mask.append(1)  # Mark this message as valid.\n",
    "                conv_scores.append(scores[i])\n",
    "            else:\n",
    "                # Pad missing messages with zeros.\n",
    "                conv_tokens.append([0]*max_token_count)\n",
    "                conv_labels.append(0)\n",
    "                conv_mask.append(0)\n",
    "                conv_scores.append(0)\n",
    "        padded_tokens.append(conv_tokens)\n",
    "        padded_labels.append(conv_labels)\n",
    "        mask.append(conv_mask)\n",
    "        padded_scores.append(conv_scores)\n",
    "    \n",
    "    padded_tokens = torch.tensor(padded_tokens, dtype=torch.long)\n",
    "    padded_labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    mask = torch.tensor(mask, dtype=torch.long)\n",
    "    padded_scores = torch.tensor(padded_scores, dtype=torch.float)\n",
    "    return padded_tokens, padded_labels, mask, padded_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hierarchical LSTM Model with Power\n",
    "\n",
    "The model architecture is as follows :\n",
    "\n",
    "- **Embedder:** A randomly initialized embedding layer with dimension 200 (pretrained embeddings are not used in this version).\n",
    "- **Message Encoder:** A bidirectional LSTM (hidden size 100) whose outputs are max-pooled over tokens to yield a 200-dimensional vector per message.\n",
    "- **Conversation Encoder:** A unidirectional LSTM (hidden size 200) that processes the sequence of message vectors.\n",
    "- **Power Feature:** If enabled, the game score (power) is concatenated (as a scalar feature) to the conversation encoder output for each message (increasing the classifier input dimension by 1).\n",
    "- **Classifier:** A linear layer mapping the (conv_hidden [+1]) features to 2 classes.\n",
    "\n",
    "Dropout of 0.2 is applied, and the loss uses a pos_weight of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:16:40.494576Z",
     "iopub.status.busy": "2025-03-23T07:16:40.494363Z",
     "iopub.status.idle": "2025-03-23T07:16:40.511366Z",
     "shell.execute_reply": "2025-03-23T07:16:40.510632Z",
     "shell.execute_reply.started": "2025-03-23T07:16:40.494558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this is referenced from the orignal code base . \n",
    "\n",
    "class HierarchicalLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, msg_hidden, conv_hidden, dropout=0.3, num_layers=1, use_game_scores=False):\n",
    "\n",
    "        super().__init__()\n",
    "        self.use_game_scores = use_game_scores\n",
    "        \n",
    "        # Embedding layer: maps token indices to vectors.\n",
    "        # padding_idx=0 ensures that the PAD_TOKEN (index 0) gets a vector of zeros.\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        \n",
    "        # Dropout layer to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Message encoder: a bidirectional LSTM that processes each message.\n",
    "        # Because it's bidirectional, its output size is 2 * msg_hidden.\n",
    "        self.msg_encoder = nn.LSTM(\n",
    "            input_size=embed_dim,       # Input size is the embedding dimension.\n",
    "            hidden_size=msg_hidden,     # Hidden state dimension.\n",
    "            batch_first=True,           # Input and output tensors are provided as (batch, seq, feature).\n",
    "            bidirectional=True,         # Use bidirectional LSTM to capture context from both directions.\n",
    "            num_layers=num_layers       # Number of stacked LSTM layers.\n",
    "        )\n",
    "        \n",
    "        # Conversation encoder: a unidirectional LSTM that processes the sequence of message representations.\n",
    "        # The input size is 2*msg_hidden since each message vector is produced by the bidirectional LSTM.\n",
    "        self.conv_encoder = nn.LSTM(\n",
    "            input_size=msg_hidden * 2,  # Each message is represented as a concatenation from both directions.\n",
    "            hidden_size=conv_hidden,    # Hidden state dimension at conversation level.\n",
    "            batch_first=True,           # Batch dimension first.\n",
    "            bidirectional=False,        # Unidirectional for conversation encoding.\n",
    "            num_layers=num_layers       # Number of stacked LSTM layers.\n",
    "        )\n",
    "        \n",
    "        # If using game scores as an additional feature, increase the classifier input dimension by 1.\n",
    "        classifier_in = conv_hidden + 1 if self.use_game_scores else conv_hidden\n",
    "        \n",
    "        # Final classifier: a linear layer mapping the conversation encoder output (optionally with game scores concatenated)\n",
    "        # to 2 output classes.\n",
    "        self.classifier = nn.Linear(classifier_in, 2)\n",
    "\n",
    "    def forward(self, tokens, mask, game_scores=None):\n",
    "       \n",
    "        B, M, T = tokens.shape\n",
    "        # Reshape tokens from [B, M, T] to [B*M, T] so that each message is treated individually.\n",
    "        tokens = tokens.view(B * M, T)\n",
    "        \n",
    "        # Embed the tokens: output shape becomes [B*M, T, embed_dim].\n",
    "        emb = self.embedding(tokens)\n",
    "        emb = self.dropout(emb)  # Apply dropout to embeddings.\n",
    "        \n",
    "        # Process each message with the bidirectional LSTM.\n",
    "        # out has shape [B*M, T, 2*msg_hidden] (concatenation of forward and backward hidden states).\n",
    "        out, _ = self.msg_encoder(emb)\n",
    "        \n",
    "        # Apply max pooling over the token dimension (T) to obtain a fixed-length vector per message.\n",
    "        # The result has shape [B*M, 2*msg_hidden].\n",
    "        out, _ = torch.max(out, dim=1)\n",
    "        \n",
    "        # Reshape back to [B, M, 2*msg_hidden] to reassemble messages into conversations.\n",
    "        msg_vecs = out.view(B, M, -1)\n",
    "        msg_vecs = self.dropout(msg_vecs)  # Apply dropout to message representations.\n",
    "        \n",
    "        # Process the sequence of message vectors with the conversation encoder (unidirectional LSTM).\n",
    "        # conv_out has shape [B, M, conv_hidden].\n",
    "        conv_out, _ = self.conv_encoder(msg_vecs)\n",
    "        conv_out = self.dropout(conv_out)  # Apply dropout to conversation-level representations.\n",
    "        \n",
    "        # If the power (game scores) feature is used, concatenate it as an extra feature to conv_out.\n",
    "        # First, unsqueeze game_scores to shape [B, M, 1] then concatenate along the feature dimension.\n",
    "        if self.use_game_scores and game_scores is not None:\n",
    "            game_scores = game_scores.unsqueeze(-1)\n",
    "            conv_out = torch.cat([conv_out, game_scores], dim=2)\n",
    "        \n",
    "        # Pass the conversation-level representations (optionally with game scores) through the classifier.\n",
    "        # The output logits have shape [B, M, 2] (for 2 classes).\n",
    "        logits = self.classifier(conv_out)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training & Evaluation Functions\n",
    "\n",
    "We define functions to compute the sequence cross-entropy loss (ignoring padded messages), accuracy, and accumulate predictions and labels (ignoring padded items) to compute the macro F1 score using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:16:40.512535Z",
     "iopub.status.busy": "2025-03-23T07:16:40.512316Z",
     "iopub.status.idle": "2025-03-23T07:16:40.528234Z",
     "shell.execute_reply": "2025-03-23T07:16:40.527578Z",
     "shell.execute_reply.started": "2025-03-23T07:16:40.512516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sequence_cross_entropy_with_logits(logits, targets, mask):\n",
    "   \n",
    "    B, M, C = logits.shape  # B: batch size, M: number of messages, C: number of classes (2)\n",
    "    # Flatten logits and targets so that each valid message is considered individually.\n",
    "    logits_flat = logits.view(B * M, C)\n",
    "    targets_flat = targets.view(B * M)\n",
    "    mask_flat = mask.view(B * M).float()  # Convert mask to float for multiplication.\n",
    "    \n",
    "    # Compute the cross-entropy loss for each message.\n",
    "    # The weight tensor assigns higher weight (POS_WEIGHT) to the positive class.\n",
    "    ce = F.cross_entropy(logits_flat, targets_flat, reduction='none', weight=torch.tensor([1.0, POS_WEIGHT]).to(DEVICE))\n",
    "    # Zero out loss for padded messages by multiplying with the mask.\n",
    "    ce = ce * mask_flat\n",
    "    # Return the average loss over all valid messages.\n",
    "    return ce.sum() / (mask_flat.sum() + 1e-8)\n",
    "\n",
    "\n",
    "def compute_accuracy(logits, targets, mask):\n",
    "   \n",
    "    # Get predictions by taking the index of the maximum logit for each message.\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    # Compare predictions with targets and apply the mask to consider only valid messages.\n",
    "    correct = (preds == targets) * (mask == 1)\n",
    "    total = mask.sum()  # Total number of valid messages.\n",
    "    # Compute and return accuracy.\n",
    "    return (correct.sum().float() / (total.float() + 1e-8)).item()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer):\n",
    "   \n",
    "    model.train()  # Set the model to training mode.\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    all_preds = []  # List to store predictions for all valid messages.\n",
    "    all_labels = []  # List to store corresponding ground truth labels.\n",
    "    count = 0  # Number of batches processed.\n",
    "    \n",
    "    for tokens_batch, labels_batch, mask_batch, scores_batch in loader:\n",
    "        # Move tensors to the specified device.\n",
    "        tokens_batch = tokens_batch.to(DEVICE)\n",
    "        labels_batch = labels_batch.to(DEVICE)\n",
    "        mask_batch = mask_batch.to(DEVICE)\n",
    "        scores_batch = scores_batch.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients.\n",
    "        logits = model(tokens_batch, mask_batch, scores_batch)  # Forward pass.\n",
    "        loss = sequence_cross_entropy_with_logits(logits, labels_batch, mask_batch)  # Compute loss.\n",
    "        loss.backward()  # Backpropagation.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)  # Gradient clipping.\n",
    "        optimizer.step()  # Update parameters.\n",
    "        \n",
    "        # Compute batch accuracy.\n",
    "        acc = compute_accuracy(logits, labels_batch, mask_batch)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        \n",
    "        # Gather predictions and labels for computing macro F1 score later.\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "        labels_np = labels_batch.cpu().numpy()\n",
    "        mask_np = mask_batch.cpu().numpy()\n",
    "        for p_row, l_row, m_row in zip(preds, labels_np, mask_np):\n",
    "            for p, l, m in zip(p_row, l_row, m_row):\n",
    "                if m == 1:  # Consider only valid (non-padded) messages.\n",
    "                    all_preds.append(p)\n",
    "                    all_labels.append(l)\n",
    "        count += 1\n",
    "\n",
    "    # Compute macro F1 score using scikit-learn's f1_score function.\n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return total_loss / count, total_acc / count, macro_f1\n",
    "\n",
    "\n",
    "def eval_model(model, loader):\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode.\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    all_preds = []  # Store predictions over valid messages.\n",
    "    all_labels = []  # Store corresponding ground truth labels.\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation.\n",
    "        for tokens_batch, labels_batch, mask_batch, scores_batch in loader:\n",
    "            tokens_batch = tokens_batch.to(DEVICE)\n",
    "            labels_batch = labels_batch.to(DEVICE)\n",
    "            mask_batch = mask_batch.to(DEVICE)\n",
    "            scores_batch = scores_batch.to(DEVICE)\n",
    "            \n",
    "            logits = model(tokens_batch, mask_batch, scores_batch)  # Forward pass.\n",
    "            loss = sequence_cross_entropy_with_logits(logits, labels_batch, mask_batch)  # Compute loss.\n",
    "            acc = compute_accuracy(logits, labels_batch, mask_batch)  # Compute accuracy.\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_acc += acc\n",
    "            \n",
    "            preds = logits.argmax(dim=-1).cpu().numpy()\n",
    "            labels_np = labels_batch.cpu().numpy()\n",
    "            mask_np = mask_batch.cpu().numpy()\n",
    "            for p_row, l_row, m_row in zip(preds, labels_np, mask_np):\n",
    "                for p, l, m in zip(p_row, l_row, m_row):\n",
    "                    if m == 1:  # Only consider valid messages.\n",
    "                        all_preds.append(p)\n",
    "                        all_labels.append(l)\n",
    "            count += 1\n",
    "    \n",
    "    macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    return total_loss / count, total_acc / count, macro_f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting It All Together\n",
    "\n",
    "We load the datasets with our DiplomacyDataset (which now pulls game score deltas), build DataLoaders, initialize the model (with use_game_scores True), and execute the training loop. In this iteration, the embeddings are still randomly initialized . We further include a learning rate scheduler below to assist with better convergence and macro F1 without modifying the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:16:40.529118Z",
     "iopub.status.busy": "2025-03-23T07:16:40.528915Z",
     "iopub.status.idle": "2025-03-23T07:20:26.903658Z",
     "shell.execute_reply": "2025-03-23T07:20:26.902766Z",
     "shell.execute_reply.started": "2025-03-23T07:16:40.529101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80 | Train Loss: 0.4895, Train Acc: 0.9270, Train Macro F1: 0.4899\n",
      " Val Loss: 0.3876, Val Acc: 0.9374, Val Macro F1: 0.4899\n",
      "Epoch 2/80 | Train Loss: 0.4299, Train Acc: 0.9291, Train Macro F1: 0.4885\n",
      " Val Loss: 0.3786, Val Acc: 0.9374, Val Macro F1: 0.4899\n",
      "Epoch 3/80 | Train Loss: 0.4420, Train Acc: 0.9291, Train Macro F1: 0.4885\n",
      " Val Loss: 0.4024, Val Acc: 0.9374, Val Macro F1: 0.4899\n",
      "Epoch 4/80 | Train Loss: 0.4187, Train Acc: 0.9291, Train Macro F1: 0.4885\n",
      " Val Loss: 0.4215, Val Acc: 0.9374, Val Macro F1: 0.4899\n",
      "Epoch 5/80 | Train Loss: 0.3685, Train Acc: 0.9291, Train Macro F1: 0.4885\n",
      " Val Loss: 0.4469, Val Acc: 0.9374, Val Macro F1: 0.4899\n",
      "Epoch 6/80 | Train Loss: 0.2639, Train Acc: 0.9413, Train Macro F1: 0.5692\n",
      " Val Loss: 0.5718, Val Acc: 0.9374, Val Macro F1: 0.4899\n",
      "Epoch 7/80 | Train Loss: 0.1762, Train Acc: 0.9649, Train Macro F1: 0.6976\n",
      " Val Loss: 0.6129, Val Acc: 0.9327, Val Macro F1: 0.4895\n",
      "Epoch 8/80 | Train Loss: 0.1215, Train Acc: 0.9757, Train Macro F1: 0.7751\n",
      " Val Loss: 0.8486, Val Acc: 0.9209, Val Macro F1: 0.4871\n",
      "Epoch 9/80 | Train Loss: 0.0964, Train Acc: 0.9815, Train Macro F1: 0.8084\n",
      " Val Loss: 1.0397, Val Acc: 0.9166, Val Macro F1: 0.4853\n",
      "Epoch 10/80 | Train Loss: 0.0749, Train Acc: 0.9864, Train Macro F1: 0.8496\n",
      " Val Loss: 0.9712, Val Acc: 0.9198, Val Macro F1: 0.4866\n",
      "Epoch 11/80 | Train Loss: 0.0623, Train Acc: 0.9902, Train Macro F1: 0.8860\n",
      " Val Loss: 0.9639, Val Acc: 0.9213, Val Macro F1: 0.4875\n",
      "Epoch 12/80 | Train Loss: 0.0535, Train Acc: 0.9913, Train Macro F1: 0.8973\n",
      " Val Loss: 1.0952, Val Acc: 0.9197, Val Macro F1: 0.4864\n",
      "Epoch 13/80 | Train Loss: 0.0452, Train Acc: 0.9923, Train Macro F1: 0.9057\n",
      " Val Loss: 1.1137, Val Acc: 0.9204, Val Macro F1: 0.4871\n",
      "Epoch 14/80 | Train Loss: 0.0447, Train Acc: 0.9927, Train Macro F1: 0.9123\n",
      " Val Loss: 1.1850, Val Acc: 0.9198, Val Macro F1: 0.4866\n",
      "Epoch 15/80 | Train Loss: 0.0373, Train Acc: 0.9932, Train Macro F1: 0.9174\n",
      " Val Loss: 1.1771, Val Acc: 0.9187, Val Macro F1: 0.4864\n",
      "Epoch 16/80 | Train Loss: 0.0377, Train Acc: 0.9940, Train Macro F1: 0.9276\n",
      " Val Loss: 1.1400, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 17/80 | Train Loss: 0.0394, Train Acc: 0.9940, Train Macro F1: 0.9243\n",
      " Val Loss: 1.1827, Val Acc: 0.9196, Val Macro F1: 0.4864\n",
      "Epoch 18/80 | Train Loss: 0.0339, Train Acc: 0.9947, Train Macro F1: 0.9324\n",
      " Val Loss: 1.1589, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 19/80 | Train Loss: 0.0355, Train Acc: 0.9943, Train Macro F1: 0.9286\n",
      " Val Loss: 1.1935, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 20/80 | Train Loss: 0.0318, Train Acc: 0.9949, Train Macro F1: 0.9371\n",
      " Val Loss: 1.1987, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 21/80 | Train Loss: 0.0357, Train Acc: 0.9944, Train Macro F1: 0.9335\n",
      " Val Loss: 1.2381, Val Acc: 0.9196, Val Macro F1: 0.4864\n",
      "Epoch 22/80 | Train Loss: 0.0305, Train Acc: 0.9950, Train Macro F1: 0.9398\n",
      " Val Loss: 1.2450, Val Acc: 0.9198, Val Macro F1: 0.4866\n",
      "Epoch 23/80 | Train Loss: 0.0301, Train Acc: 0.9952, Train Macro F1: 0.9397\n",
      " Val Loss: 1.2458, Val Acc: 0.9198, Val Macro F1: 0.4866\n",
      "Epoch 24/80 | Train Loss: 0.0293, Train Acc: 0.9953, Train Macro F1: 0.9422\n",
      " Val Loss: 1.2431, Val Acc: 0.9198, Val Macro F1: 0.4866\n",
      "Epoch 25/80 | Train Loss: 0.0298, Train Acc: 0.9952, Train Macro F1: 0.9413\n",
      " Val Loss: 1.2118, Val Acc: 0.9211, Val Macro F1: 0.4871\n",
      "Epoch 26/80 | Train Loss: 0.0295, Train Acc: 0.9944, Train Macro F1: 0.9371\n",
      " Val Loss: 1.2351, Val Acc: 0.9211, Val Macro F1: 0.4871\n",
      "Epoch 27/80 | Train Loss: 0.0301, Train Acc: 0.9945, Train Macro F1: 0.9402\n",
      " Val Loss: 1.2397, Val Acc: 0.9211, Val Macro F1: 0.4871\n",
      "Epoch 28/80 | Train Loss: 0.0293, Train Acc: 0.9954, Train Macro F1: 0.9393\n",
      " Val Loss: 1.2260, Val Acc: 0.9211, Val Macro F1: 0.4871\n",
      "Epoch 29/80 | Train Loss: 0.0266, Train Acc: 0.9954, Train Macro F1: 0.9439\n",
      " Val Loss: 1.2419, Val Acc: 0.9202, Val Macro F1: 0.4870\n",
      "Epoch 30/80 | Train Loss: 0.0288, Train Acc: 0.9948, Train Macro F1: 0.9402\n",
      " Val Loss: 1.2733, Val Acc: 0.9198, Val Macro F1: 0.4866\n",
      "Epoch 31/80 | Train Loss: 0.0308, Train Acc: 0.9943, Train Macro F1: 0.9423\n",
      " Val Loss: 1.2600, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 32/80 | Train Loss: 0.0316, Train Acc: 0.9947, Train Macro F1: 0.9360\n",
      " Val Loss: 1.2447, Val Acc: 0.9211, Val Macro F1: 0.4871\n",
      "Epoch 33/80 | Train Loss: 0.0276, Train Acc: 0.9952, Train Macro F1: 0.9405\n",
      " Val Loss: 1.2474, Val Acc: 0.9211, Val Macro F1: 0.4871\n",
      "Epoch 34/80 | Train Loss: 0.0272, Train Acc: 0.9949, Train Macro F1: 0.9412\n",
      " Val Loss: 1.2616, Val Acc: 0.9202, Val Macro F1: 0.4870\n",
      "Epoch 35/80 | Train Loss: 0.0280, Train Acc: 0.9945, Train Macro F1: 0.9418\n",
      " Val Loss: 1.2704, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 36/80 | Train Loss: 0.0268, Train Acc: 0.9957, Train Macro F1: 0.9448\n",
      " Val Loss: 1.2713, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 37/80 | Train Loss: 0.0271, Train Acc: 0.9955, Train Macro F1: 0.9468\n",
      " Val Loss: 1.2667, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 38/80 | Train Loss: 0.0255, Train Acc: 0.9954, Train Macro F1: 0.9467\n",
      " Val Loss: 1.2631, Val Acc: 0.9202, Val Macro F1: 0.4870\n",
      "Epoch 39/80 | Train Loss: 0.0297, Train Acc: 0.9948, Train Macro F1: 0.9388\n",
      " Val Loss: 1.2656, Val Acc: 0.9202, Val Macro F1: 0.4870\n",
      "Epoch 40/80 | Train Loss: 0.0274, Train Acc: 0.9956, Train Macro F1: 0.9457\n",
      " Val Loss: 1.2647, Val Acc: 0.9202, Val Macro F1: 0.4870\n",
      "Epoch 41/80 | Train Loss: 0.0285, Train Acc: 0.9947, Train Macro F1: 0.9433\n",
      " Val Loss: 1.2667, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 42/80 | Train Loss: 0.0273, Train Acc: 0.9954, Train Macro F1: 0.9446\n",
      " Val Loss: 1.2647, Val Acc: 0.9202, Val Macro F1: 0.4870\n",
      "Epoch 43/80 | Train Loss: 0.0275, Train Acc: 0.9956, Train Macro F1: 0.9458\n",
      " Val Loss: 1.2659, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 44/80 | Train Loss: 0.0261, Train Acc: 0.9951, Train Macro F1: 0.9417\n",
      " Val Loss: 1.2675, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 45/80 | Train Loss: 0.0270, Train Acc: 0.9950, Train Macro F1: 0.9444\n",
      " Val Loss: 1.2687, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 46/80 | Train Loss: 0.0317, Train Acc: 0.9949, Train Macro F1: 0.9447\n",
      " Val Loss: 1.2709, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 47/80 | Train Loss: 0.0267, Train Acc: 0.9954, Train Macro F1: 0.9441\n",
      " Val Loss: 1.2717, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 48/80 | Train Loss: 0.0281, Train Acc: 0.9954, Train Macro F1: 0.9444\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 49/80 | Train Loss: 0.0267, Train Acc: 0.9955, Train Macro F1: 0.9455\n",
      " Val Loss: 1.2714, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 50/80 | Train Loss: 0.0272, Train Acc: 0.9953, Train Macro F1: 0.9420\n",
      " Val Loss: 1.2712, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 51/80 | Train Loss: 0.0316, Train Acc: 0.9944, Train Macro F1: 0.9405\n",
      " Val Loss: 1.2712, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 52/80 | Train Loss: 0.0278, Train Acc: 0.9950, Train Macro F1: 0.9438\n",
      " Val Loss: 1.2709, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 53/80 | Train Loss: 0.0256, Train Acc: 0.9956, Train Macro F1: 0.9438\n",
      " Val Loss: 1.2708, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 54/80 | Train Loss: 0.0272, Train Acc: 0.9952, Train Macro F1: 0.9468\n",
      " Val Loss: 1.2707, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 55/80 | Train Loss: 0.0283, Train Acc: 0.9957, Train Macro F1: 0.9488\n",
      " Val Loss: 1.2707, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 56/80 | Train Loss: 0.0274, Train Acc: 0.9944, Train Macro F1: 0.9424\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 57/80 | Train Loss: 0.0264, Train Acc: 0.9953, Train Macro F1: 0.9440\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 58/80 | Train Loss: 0.0285, Train Acc: 0.9947, Train Macro F1: 0.9444\n",
      " Val Loss: 1.2712, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 59/80 | Train Loss: 0.0252, Train Acc: 0.9957, Train Macro F1: 0.9452\n",
      " Val Loss: 1.2712, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 60/80 | Train Loss: 0.0288, Train Acc: 0.9956, Train Macro F1: 0.9440\n",
      " Val Loss: 1.2709, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 61/80 | Train Loss: 0.0269, Train Acc: 0.9952, Train Macro F1: 0.9466\n",
      " Val Loss: 1.2709, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 62/80 | Train Loss: 0.0278, Train Acc: 0.9950, Train Macro F1: 0.9439\n",
      " Val Loss: 1.2709, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 63/80 | Train Loss: 0.0258, Train Acc: 0.9953, Train Macro F1: 0.9418\n",
      " Val Loss: 1.2709, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 64/80 | Train Loss: 0.0268, Train Acc: 0.9952, Train Macro F1: 0.9448\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 65/80 | Train Loss: 0.0283, Train Acc: 0.9952, Train Macro F1: 0.9447\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 66/80 | Train Loss: 0.0271, Train Acc: 0.9957, Train Macro F1: 0.9450\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 67/80 | Train Loss: 0.0271, Train Acc: 0.9954, Train Macro F1: 0.9445\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 68/80 | Train Loss: 0.0283, Train Acc: 0.9950, Train Macro F1: 0.9404\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 69/80 | Train Loss: 0.0262, Train Acc: 0.9955, Train Macro F1: 0.9467\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 70/80 | Train Loss: 0.0244, Train Acc: 0.9956, Train Macro F1: 0.9466\n",
      " Val Loss: 1.2710, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 71/80 | Train Loss: 0.0268, Train Acc: 0.9953, Train Macro F1: 0.9451\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 72/80 | Train Loss: 0.0263, Train Acc: 0.9951, Train Macro F1: 0.9457\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 73/80 | Train Loss: 0.0261, Train Acc: 0.9953, Train Macro F1: 0.9483\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 74/80 | Train Loss: 0.0278, Train Acc: 0.9953, Train Macro F1: 0.9443\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 75/80 | Train Loss: 0.0283, Train Acc: 0.9955, Train Macro F1: 0.9451\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 76/80 | Train Loss: 0.0264, Train Acc: 0.9957, Train Macro F1: 0.9453\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 77/80 | Train Loss: 0.0262, Train Acc: 0.9955, Train Macro F1: 0.9442\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 78/80 | Train Loss: 0.0266, Train Acc: 0.9958, Train Macro F1: 0.9472\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 79/80 | Train Loss: 0.0247, Train Acc: 0.9953, Train Macro F1: 0.9463\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n",
      "Epoch 80/80 | Train Loss: 0.0258, Train Acc: 0.9960, Train Macro F1: 0.9493\n",
      " Val Loss: 1.2711, Val Acc: 0.9200, Val Macro F1: 0.4868\n"
     ]
    }
   ],
   "source": [
    "# Loading the  datasets\n",
    "train_dataset = DiplomacyDataset(TRAIN_PATH, use_game_scores=True)\n",
    "val_dataset = DiplomacyDataset(VAL_PATH, use_game_scores=True) if VAL_PATH else None\n",
    "test_dataset = DiplomacyDataset(TEST_PATH, use_game_scores=True)\n",
    "\n",
    "vocab_size = len(train_dataset.tok2ix)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn) if val_dataset else None\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Instantiate the model with use_game_scores=True\n",
    "model = HierarchicalLSTM(vocab_size=vocab_size, embed_dim=EMBED_DIM, msg_hidden=MSG_HIDDEN, conv_hidden=CONV_HIDDEN, dropout=DROPOUT, num_layers=1, use_game_scores=True).to(DEVICE)\n",
    "\n",
    "# Load pretrained GloVe embeddings if desired (not used in this version, so we skip this step)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "# Learning Rate Scheduler (ReduceLROnPlateau) based on validation loss\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "best_val_loss = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_f1 = train_one_epoch(model, train_loader, optimizer)\n",
    "    if val_loader:\n",
    "        val_loss, val_acc, val_f1 = eval_model(model, val_loader)\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train Macro F1: {train_f1:.4f}\\n\",\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val Macro F1: {val_f1:.4f}\")\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # Optionally, save the model\n",
    "            # torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Train Macro F1: {train_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test / Inference\n",
    "\n",
    "Finally, we evaluate on the test set and run inference on a sample conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T07:20:26.905084Z",
     "iopub.status.busy": "2025-03-23T07:20:26.904577Z",
     "iopub.status.idle": "2025-03-23T07:20:27.164067Z",
     "shell.execute_reply": "2025-03-23T07:20:27.163322Z",
     "shell.execute_reply.started": "2025-03-23T07:20:26.905049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.0863, Test Acc: 0.8964, Test Macro F1: 0.5020\n",
      "Conversation has 5 messages.\n",
      "Msg 0 -> Pred: True, Gold: True\n",
      "Msg 1 -> Pred: True, Gold: True\n",
      "Msg 2 -> Pred: True, Gold: True\n",
      "Msg 3 -> Pred: True, Gold: True\n",
      "Msg 4 -> Pred: True, Gold: True\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_f1 = eval_model(model, test_loader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, Test Macro F1: {test_f1:.4f}\")\n",
    "\n",
    "# Inference on one conversation from the test set\n",
    "model.eval()\n",
    "sample_conv, sample_lbls, sample_scores = test_dataset[0]\n",
    "\n",
    "tokens_batch, labels_batch, mask_batch, scores_batch = collate_fn([(sample_conv, sample_lbls, sample_scores)])\n",
    "tokens_batch = tokens_batch.to(DEVICE)\n",
    "labels_batch = labels_batch.to(DEVICE)\n",
    "mask_batch = mask_batch.to(DEVICE)\n",
    "scores_batch = scores_batch.to(DEVICE)\n",
    "\n",
    "logits = model(tokens_batch, mask_batch, scores_batch)  # [1, M, 2]\n",
    "preds = logits.argmax(dim=-1).squeeze(0)  # [M]\n",
    "\n",
    "print(\"Conversation has\", len(sample_conv), \"messages.\")\n",
    "for i, (msg, gold_label) in enumerate(zip(sample_conv, sample_lbls)):\n",
    "    predicted_label = preds[i].item()\n",
    "    predicted_bool = (predicted_label == 1)\n",
    "    gold_bool = bool(gold_label)\n",
    "    print(f\"Msg {i} -> Pred: {predicted_bool}, Gold: {gold_bool}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6935576,
     "sourceId": 11121931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6941592,
     "sourceId": 11130183,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "name": "contextlstm_end2end"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
