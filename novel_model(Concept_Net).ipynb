{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11354484,"sourceType":"datasetVersion","datasetId":7105583},{"sourceId":11375484,"sourceType":"datasetVersion","datasetId":7121728},{"sourceId":11381185,"sourceType":"datasetVersion","datasetId":7126198},{"sourceId":11121931,"sourceType":"datasetVersion","datasetId":6935576},{"sourceId":334893,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":280353,"modelId":301260}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0a890484","cell_type":"markdown","source":"# CODE FILE FOR ROBERTA + LSTM + GCN + CONCEPTNET  ","metadata":{}},{"id":"d390d5f7","cell_type":"markdown","source":"# Improved Deception Detection Model (With Conceptnet)\n\nIn this code file , we have implemented an end‐to‐end revised version of the deception detection code. In this version, we incorporate several improvements inspired by the research (e.g. Peskov et al. 2020) and related work on deception detection. In particular, we:\n\n- Introduce a new context encoder module that uses a bidirectional LSTM to summarize token‐level representations of the conversation context.\n- Fuse five streams of information: the current message representation (from RoBERTa), the contextual summary from the LSTM, graph features derived from conversation (via two graph attention layers), and game score features (as a proxy for power dynamics) and ConceptNet embeddings . \n- Use an ensemble of classifier heads (with learnable weights) to stabilize predictions.\n\nDue to limited data size, early transformer layers are frozen and extra regularization is applied.","metadata":{}},{"id":"23697291","cell_type":"code","source":"import json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter, defaultdict\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\nimport pandas as pd\nfrom transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, RobertaModel\nimport spacy\n\n# Set seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:28:08.498394Z","iopub.execute_input":"2025-04-13T01:28:08.498690Z","iopub.status.idle":"2025-04-13T01:28:08.505896Z","shell.execute_reply.started":"2025-04-13T01:28:08.498668Z","shell.execute_reply":"2025-04-13T01:28:08.505328Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"id":"85dc5a59","cell_type":"code","source":"\n# Data paths\nTRAIN_PATH = r\"/kaggle/input/nlp-proj-data/Data/train.jsonl\"\nVAL_PATH = r\"/kaggle/input/nlp-proj-data/Data/validation.jsonl\"\nTEST_PATH = r\"/kaggle/input/nlp-proj-data/Data/test.jsonl\"\n\n# Model and training parameters\nTRANSFORMER_MODEL = \"roberta-base\"  # Using RoBERTa base\nBATCH_SIZE = 32\nEPOCHS = 5\nLR = 5e-6\nUSE_GAME_SCORES = True\nOVERSAMPLING_FACTOR = 30  # Oversampling factor (tune if necessary)\nTRUTH_FOCAL_WEIGHT = 4.0  # Additional weight for truth class during loss calculation\nEARLY_STOPPING_PATIENCE = 5\nGRADIENT_ACCUMULATION_STEPS = 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:28:10.507621Z","iopub.execute_input":"2025-04-13T01:28:10.507917Z","iopub.status.idle":"2025-04-13T01:28:10.512297Z","shell.execute_reply.started":"2025-04-13T01:28:10.507894Z","shell.execute_reply":"2025-04-13T01:28:10.511395Z"}},"outputs":[],"execution_count":8},{"id":"ce238739","cell_type":"code","source":"# Load spaCy model for entity extraction\nnlp = spacy.load(\"en_core_web_sm\")\n\ndef load_numberbatch(path=\"/kaggle/input/concept-net/numberbatch-en.txt\"):\n    \"\"\"Load ConceptNet Numberbatch embeddings into a dictionary.\"\"\"\n    embeddings = {}\n    with open(path, 'r', encoding='utf-8') as f:\n        next(f)  # Skip header line if present\n        for line in f:\n            parts = line.strip().split()\n            key = parts[0]  # e.g., '/c/en/apple'\n            vector = np.array(parts[1:], dtype=np.float32)\n            embeddings[key] = vector\n    return embeddings\n\nclass EnhancedDeceptionDataset(Dataset):\n    def __init__(self, path, tokenizer, max_len=128, use_game_scores=True):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.use_game_scores = use_game_scores\n        self.texts = []\n        self.labels = []\n        self.scores = []\n        self.conversation_ids = []\n        self.message_positions = []\n        self.prior_context = []\n        self.conceptnet_features = []  # Store ConceptNet features\n        \n        # Load Numberbatch embeddings\n        self.numberbatch_embeddings = load_numberbatch()\n        self.conceptnet_dim = 300  # Numberbatch embedding size\n        \n        with open(path, 'r', encoding='utf-8') as f:\n            conv_id = 0\n            for line in f:\n                data = json.loads(line.strip())\n                messages = data.get('messages', [])\n                labels = data.get('sender_labels', [])\n                game_scores = data.get('game_score_delta', None) if use_game_scores else None\n                if game_scores is None:\n                    game_scores = [0] * len(messages)\n                \n                for pos, (msg, label, score) in enumerate(zip(messages, labels, game_scores)):\n                    if label in [True, False, \"true\", \"false\", \"True\", \"False\"]:\n                        if isinstance(label, str):\n                            is_lie = label.lower() == \"true\"\n                        else:\n                            is_lie = label\n                        self.texts.append(msg)\n                        self.labels.append(1 if is_lie else 0)\n                        self.scores.append(float(score))\n                        self.conversation_ids.append(conv_id)\n                        self.message_positions.append(pos)\n                        context = \"\"\n                        if pos > 0:\n                            context_msgs = messages[max(0, pos-2):pos]\n                            context = \" [SEP] \".join(context_msgs)\n                        self.prior_context.append(context)\n                        \n                        # Extract entities and compute ConceptNet features\n                        entities = self.extract_entities(msg)\n                        embeddings = [self.get_numberbatch_embedding(ent) for ent in entities]\n                        if embeddings:\n                            avg_embedding = np.mean(embeddings, axis=0)\n                        else:\n                            avg_embedding = np.zeros(self.conceptnet_dim)\n                        self.conceptnet_features.append(avg_embedding)\n                conv_id += 1\n        \n        self.class_counts = Counter(self.labels)\n        total = len(self.labels)\n        self.truth_indices = [i for i, label in enumerate(self.labels) if label == 0]\n        self.lie_indices = [i for i, label in enumerate(self.labels) if label == 1]\n        self.conv_to_msgs = defaultdict(list)\n        for i, cid in enumerate(self.conversation_ids):\n            self.conv_to_msgs[cid].append(i)\n        \n        print(f\"Dataset loaded from {path}\")\n        print(f\"Total messages: {total}\")\n        for label, count in sorted(self.class_counts.items()):\n            label_name = \"Truth\" if label == 0 else \"Lie\"\n            print(f\"{label_name}: {count} ({count/total*100:.2f}%)\")\n\n    def extract_entities(self, text):\n        \"\"\"Extract entities or nouns from text using spaCy.\"\"\"\n        doc = nlp(text)\n        entities = [ent.text.replace(' ', '_').lower() for ent in doc.ents]\n        if not entities:\n            entities = [token.text.replace(' ', '_').lower() for token in doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n        return entities[:5]  # Limit to top 5 entities\n\n    def get_numberbatch_embedding(self, entity):\n        \"\"\"Retrieve Numberbatch embedding for an entity.\"\"\"\n        key = f\"/c/en/{entity}\"\n        return self.numberbatch_embeddings.get(key, np.zeros(self.conceptnet_dim))\n\n    def __getitem__(self, idx):\n        encoding = self.tokenizer(\n            self.texts[idx],\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        context = self.prior_context[idx]\n        if context:\n            context_encoding = self.tokenizer(\n                context,\n                max_length=self.max_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n        else:\n            context_encoding = {\n                'input_ids': torch.zeros((1, self.max_len), dtype=torch.long),\n                'attention_mask': torch.zeros((1, self.max_len), dtype=torch.long)\n            }\n        \n        return {\n            'text': self.texts[idx],\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'context_input_ids': context_encoding['input_ids'].flatten(),\n            'context_attention_mask': context_encoding['attention_mask'].flatten(),\n            'label': torch.tensor(self.labels[idx], dtype=torch.long),\n            'score': torch.tensor(self.scores[idx], dtype=torch.float),\n            'conv_id': self.conversation_ids[idx],\n            'position': self.message_positions[idx],\n            'relative_positions': [],\n            'conceptnet_features': torch.tensor(self.conceptnet_features[idx], dtype=torch.float)\n        }\n\n    def __len__(self):\n        return len(self.labels)\n\nclass EnhancedBalancedSampler(torch.utils.data.Sampler):\n    def __init__(self, dataset, oversample_factor=15):\n        self.dataset = dataset\n        self.oversample_factor = oversample_factor\n        self.truth_indices = dataset.truth_indices * oversample_factor  # Oversample truths\n        self.lie_indices = dataset.lie_indices\n        \n        min_samples = max(1000, len(self.truth_indices))\n        target_size = min(len(self.truth_indices), len(self.lie_indices))\n        target_size = max(target_size, min_samples)\n        \n        if len(self.truth_indices) < target_size:\n            self.truth_indices = self.truth_indices * (target_size // len(self.truth_indices) + 1)\n        if len(self.lie_indices) < target_size:\n            self.lie_indices = self.lie_indices * (target_size // len(self.lie_indices) + 1)\n        \n        self.truth_indices = random.sample(self.truth_indices, target_size)\n        self.lie_indices = random.sample(self.lie_indices, target_size)\n        \n        self.indices = self.truth_indices + self.lie_indices\n        random.shuffle(self.indices)\n    \n    def __iter__(self):\n        return iter(self.indices)\n    \n    def __len__(self):\n        return len(self.indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:28:12.203897Z","iopub.execute_input":"2025-04-13T01:28:12.204598Z","iopub.status.idle":"2025-04-13T01:28:12.788250Z","shell.execute_reply.started":"2025-04-13T01:28:12.204572Z","shell.execute_reply":"2025-04-13T01:28:12.787664Z"}},"outputs":[],"execution_count":9},{"id":"c5d6f5e9","cell_type":"code","source":"class SimpleAttention(nn.Module):\n    def __init__(self, hidden_size, dropout=0.1):\n        super(SimpleAttention, self).__init__()\n        self.hidden_size = hidden_size\n        self.query_proj = nn.Linear(hidden_size, hidden_size)\n        self.key_proj = nn.Linear(hidden_size, hidden_size)\n        self.value_proj = nn.Linear(hidden_size, hidden_size)\n        self.out_proj = nn.Linear(hidden_size, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        query = self.query_proj(query)\n        key = self.key_proj(key)\n        value = self.value_proj(value)\n        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.hidden_size ** 0.5)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        context = torch.matmul(attn_weights, value)\n        output = self.out_proj(context)\n        return output\n\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.1, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout = dropout\n        self.alpha = alpha\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        \n    def forward(self, features, adj_matrix):\n        h = torch.mm(features, self.W)  # (N, out_features)\n        N = h.size()[0]\n        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)\n        a_input = a_input.view(N, N, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj_matrix > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n        h_prime = torch.matmul(attention, h)\n        return h_prime\n\nclass ContextEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=True, dropout=0.1):\n        super(ContextEncoder, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, \n                            bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)\n        self.output_dim = hidden_dim * (2 if bidirectional else 1)\n\n    def forward(self, token_embeddings, attention_mask):\n        lengths = torch.clamp(attention_mask.sum(dim=1), min=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(token_embeddings, lengths, batch_first=True, enforce_sorted=False)\n        packed_outputs, (h_n, _) = self.lstm(packed)\n        num_directions = 2 if self.lstm.bidirectional else 1\n        h_n = h_n.view(self.lstm.num_layers, num_directions, token_embeddings.size(0), self.lstm.hidden_size)\n        h_final = torch.cat((h_n[-1, 0, :, :], h_n[-1, 1, :, :]), dim=1)\n        return h_final\n\nclass ImprovedDeceptionModel(nn.Module):\n    def __init__(self, model_name, use_game_scores=True):\n        super(ImprovedDeceptionModel, self).__init__()\n        self.transformer = RobertaModel.from_pretrained(model_name)\n        self.context_transformer = RobertaModel.from_pretrained(model_name)\n        \n        for layer in self.transformer.encoder.layer[:2]:\n            for param in layer.parameters():\n                param.requires_grad = False\n        for layer in self.context_transformer.encoder.layer[:2]:\n            for param in layer.parameters():\n                param.requires_grad = False\n        \n        self.hidden_size = self.transformer.config.hidden_size\n        self.use_game_scores = use_game_scores\n        self.conceptnet_dim = 300  # Size of Numberbatch embeddings\n        \n        self.dropout = nn.Dropout(0.3)\n        self.context_encoder = ContextEncoder(input_dim=self.hidden_size, hidden_dim=self.hidden_size//2, \n                                              num_layers=1, bidirectional=True, dropout=0.1)\n        \n        # Adjust input dimension for graph attention\n        self.combined_dim = self.hidden_size + self.context_encoder.output_dim + self.conceptnet_dim\n        self.gat1 = GraphAttentionLayer(self.combined_dim, 256)\n        self.gat2 = GraphAttentionLayer(256, 256)\n        \n        if use_game_scores:\n            self.score_proj = nn.Sequential(\n                nn.Linear(1, 32),\n                nn.LayerNorm(32),\n                nn.ReLU(),\n                nn.Dropout(0.2)\n            )\n            fusion_dim = self.combined_dim + 256 + 32\n        else:\n            fusion_dim = self.combined_dim + 256\n        \n        self.feature_norm = nn.LayerNorm(fusion_dim)\n        self.classifier1 = nn.Linear(fusion_dim, 2)\n        self.classifier2 = nn.Linear(self.combined_dim, 2)\n        self.classifier3 = nn.Linear(256, 2)\n        self.ensemble_weights = nn.Parameter(torch.ones(3))\n    \n    def forward(self, input_ids, attention_mask, context_input_ids=None, \n                context_attention_mask=None, game_scores=None, batch_adj_matrix=None, conceptnet_features=None):\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        text_features = self.dropout(outputs.last_hidden_state[:, 0, :])\n        \n        if context_input_ids is not None and torch.sum(context_input_ids) > 0:\n            ctx_outputs = self.context_transformer(input_ids=context_input_ids, attention_mask=context_attention_mask)\n            context_summary = self.context_encoder(ctx_outputs.last_hidden_state, context_attention_mask)\n        else:\n            context_summary = torch.zeros(text_features.size(0), self.context_encoder.output_dim, device=text_features.device)\n        \n        # Include ConceptNet features\n        combined_features = torch.cat([text_features, context_summary, conceptnet_features], dim=1)\n        \n        if batch_adj_matrix is not None:\n            graph_features = self.gat1(combined_features, batch_adj_matrix)\n            graph_features = F.elu(graph_features)\n            graph_features = self.gat2(graph_features, batch_adj_matrix)\n        else:\n            graph_features = torch.zeros(combined_features.size(0), 256, device=combined_features.device)\n        \n        if self.use_game_scores and game_scores is not None:\n            score_features = self.score_proj(game_scores.unsqueeze(1))\n            all_features = torch.cat([combined_features, graph_features, score_features], dim=1)\n        else:\n            all_features = torch.cat([combined_features, graph_features], dim=1)\n        \n        all_features = self.feature_norm(all_features)\n        logits1 = self.classifier1(all_features)\n        logits2 = self.classifier2(combined_features)\n        logits3 = self.classifier3(graph_features)\n        ensemble_weights = F.softmax(self.ensemble_weights, dim=0)\n        final_logits = (ensemble_weights[0] * logits1 +\n                        ensemble_weights[1] * logits2 +\n                        ensemble_weights[2] * logits3)\n        return final_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:28:16.663785Z","iopub.execute_input":"2025-04-13T01:28:16.664086Z","iopub.status.idle":"2025-04-13T01:28:16.686085Z","shell.execute_reply.started":"2025-04-13T01:28:16.664066Z","shell.execute_reply":"2025-04-13T01:28:16.685151Z"}},"outputs":[],"execution_count":10},{"id":"39d6eb4e","cell_type":"code","source":"class FocalWeightedLoss(nn.Module):\n    def __init__(self, class_weights, truth_focal_weight=4.0):\n        super(FocalWeightedLoss, self).__init__()\n        self.class_weights = class_weights\n        self.truth_focal_weight = truth_focal_weight\n    \n    def forward(self, logits, targets):\n        ce_loss = F.cross_entropy(logits, targets, weight=self.class_weights, reduction='none')\n        probs = F.softmax(logits, dim=1)\n        truth_probs = probs[:, 0]  # Probability for truth class\n        truth_mask = (targets == 0).float()\n        focal_weight = (1 - truth_probs) ** self.truth_focal_weight\n        focal_loss = truth_mask * focal_weight * ce_loss + (1 - truth_mask) * ce_loss\n        return focal_loss.mean()\n\ndef custom_collate_fn(batch):\n    # Collate all keys except 'relative_positions'\n    batch_without_relative = [{k: v for k, v in item.items() if k != 'relative_positions'} for item in batch]\n    collated = torch.utils.data.dataloader.default_collate(batch_without_relative)\n    collated['relative_positions'] = [item['relative_positions'] for item in batch]\n    return collated\n\ndef prepare_batch_for_model(batch, device):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    context_input_ids = batch['context_input_ids'].to(device) if 'context_input_ids' in batch else None\n    context_attention_mask = batch['context_attention_mask'].to(device) if 'context_attention_mask' in batch else None\n    labels = batch['label'].to(device)\n    scores = batch['score'].to(device) if 'score' in batch else None\n    conceptnet_features = batch['conceptnet_features'].to(device)\n    batch_size = input_ids.size(0)\n    \n    batch_adj_matrix = torch.zeros((batch_size, batch_size), device=device)\n    conv_ids = batch['conv_id'] if isinstance(batch['conv_id'], list) else batch['conv_id'].tolist()\n    positions = batch['position'] if isinstance(batch['position'], list) else batch['position'].tolist()\n    for i in range(batch_size):\n        for j in range(batch_size):\n            if conv_ids[i] == conv_ids[j]:\n                if i == j:\n                    batch_adj_matrix[i, j] = 1.0\n                else:\n                    distance = abs(positions[i] - positions[j])\n                    batch_adj_matrix[i, j] = 1.0 / (distance + 1)\n    \n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'context_input_ids': context_input_ids,\n        'context_attention_mask': context_attention_mask,\n        'labels': labels,\n        'scores': scores,\n        'batch_adj_matrix': batch_adj_matrix,\n        'conceptnet_features': conceptnet_features\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:28:20.523761Z","iopub.execute_input":"2025-04-13T01:28:20.524067Z","iopub.status.idle":"2025-04-13T01:28:20.533490Z","shell.execute_reply.started":"2025-04-13T01:28:20.524042Z","shell.execute_reply":"2025-04-13T01:28:20.532760Z"}},"outputs":[],"execution_count":11},{"id":"6b4106d1-d0ff-4a6f-81de-fc61d49dcefa","cell_type":"code","source":"\ndef train(model, dataloader, optimizer, scheduler, device, class_weights, truth_focal_weight=4.0, gradient_accumulation_steps=1):\n    model.train()\n    total_loss = 0\n    all_labels = []\n    all_preds = []\n    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n    optimizer.zero_grad()\n    accumulated_steps = 0\n    \n    for batch in tqdm(dataloader, desc=\"Training\"):\n        batch_data = prepare_batch_for_model(batch, device)\n        outputs = model(\n            input_ids=batch_data['input_ids'], \n            attention_mask=batch_data['attention_mask'],\n            context_input_ids=batch_data['context_input_ids'],\n            context_attention_mask=batch_data['context_attention_mask'],\n            game_scores=batch_data['scores'],\n            batch_adj_matrix=batch_data['batch_adj_matrix'],\n            conceptnet_features=batch_data['conceptnet_features']\n        )\n        loss = loss_fn(outputs, batch_data['labels'])\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n        total_loss += loss.item() * gradient_accumulation_steps\n        _, preds = torch.max(outputs, dim=1)\n        all_labels.extend(batch_data['labels'].cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n        accumulated_steps += 1\n        if accumulated_steps % gradient_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            optimizer.zero_grad()\n    \n    if accumulated_steps % gradient_accumulation_steps != 0:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n    \n    avg_loss = total_loss / len(dataloader)\n    truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n    lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    cm = confusion_matrix(all_labels, all_preds)\n    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n\ndef evaluate(model, dataloader, device, class_weights, truth_focal_weight=4.0):\n    model.eval()\n    total_loss = 0\n    all_labels = []\n    all_preds = []\n    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            batch_data = prepare_batch_for_model(batch, device)\n            outputs = model(\n                input_ids=batch_data['input_ids'], \n                attention_mask=batch_data['attention_mask'],\n                context_input_ids=batch_data['context_input_ids'],\n                context_attention_mask=batch_data['context_attention_mask'],\n                game_scores=batch_data['scores'],\n                batch_adj_matrix=batch_data['batch_adj_matrix'],\n                conceptnet_features=batch_data['conceptnet_features']\n            )\n            loss = loss_fn(outputs, batch_data['labels'])\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, dim=1)\n            all_labels.extend(batch_data['labels'].cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n    lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    print(classification_report(all_labels, all_preds, target_names=['Truth', 'Lie'], digits=4, zero_division=0))\n    cm = confusion_matrix(all_labels, all_preds)\n    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n\ndef plot_confusion_matrix(cm, epoch, split='val'):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=['Truth', 'Lie'], yticklabels=['Truth', 'Lie'])\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title(f'Confusion Matrix - {split.capitalize()} (Epoch {epoch+1})')\n    plt.savefig(f'confusion_matrix_{split}_epoch_{epoch+1}.png')\n    plt.close()\n\ndef plot_metrics(train_metric, val_metric, metric_name):\n    plt.figure(figsize=(10, 6))\n    epochs = range(1, len(train_metric) + 1)\n    plt.plot(epochs, train_metric, 'b-', label=f'Train {metric_name}')\n    plt.plot(epochs, val_metric, 'r-', label=f'Val {metric_name}')\n    plt.title(f'{metric_name} over Training')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric_name)\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f'{metric_name.lower()}_plot.png')\n    plt.close()\n\ndef main():\n    try:\n        print(\"Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n        \n        print(\"Loading datasets...\")\n        train_dataset = EnhancedDeceptionDataset(TRAIN_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n        val_dataset = EnhancedDeceptionDataset(VAL_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n        test_dataset = EnhancedDeceptionDataset(TEST_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n        \n        train_sampler = EnhancedBalancedSampler(train_dataset, oversample_factor=OVERSAMPLING_FACTOR)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=BATCH_SIZE,\n            sampler=train_sampler,\n            num_workers=2,\n            collate_fn=custom_collate_fn\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=False,\n            num_workers=2,\n            collate_fn=custom_collate_fn\n        )\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=False,\n            num_workers=2,\n            collate_fn=custom_collate_fn\n        )\n        \n        train_class_counts = train_dataset.class_counts\n        total_samples = sum(train_class_counts.values())\n        weight_0 = total_samples / (train_class_counts.get(0, 1) * 2)\n        weight_1 = total_samples / (train_class_counts.get(1, 1) * 2)\n        class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(DEVICE)\n        print(f\"Class weights: Truth = {weight_0:.4f}, Lie = {weight_1:.4f}\")\n        \n        print(\"Initializing improved model...\")\n        model = ImprovedDeceptionModel(TRANSFORMER_MODEL, use_game_scores=USE_GAME_SCORES).to(DEVICE)\n        \n        optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n        total_steps = len(train_loader) * EPOCHS // GRADIENT_ACCUMULATION_STEPS\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=int(total_steps * 0.1),\n            num_training_steps=total_steps\n        )\n        \n        print(f\"Starting training for {EPOCHS} epochs...\")\n        best_truth_f1 = 0\n        best_macro_f1 = 0\n        no_improvement_count = 0\n        \n        train_losses = []\n        train_truth_f1s = []\n        train_lie_f1s = []\n        train_macro_f1s = []\n        val_losses = []\n        val_truth_f1s = []\n        val_lie_f1s = []\n        val_macro_f1s = []\n        \n        for epoch in range(EPOCHS):\n            train_loss, train_truth_f1, train_lie_f1, train_macro_f1, train_cm = train(\n                model, train_loader, optimizer, scheduler, DEVICE, class_weights,\n                truth_focal_weight=TRUTH_FOCAL_WEIGHT,\n                gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n            )\n            val_loss, val_truth_f1, val_lie_f1, val_macro_f1, val_cm = evaluate(\n                model, val_loader, DEVICE, class_weights,\n                truth_focal_weight=TRUTH_FOCAL_WEIGHT\n            )\n            \n            train_losses.append(train_loss)\n            train_truth_f1s.append(train_truth_f1)\n            train_lie_f1s.append(train_lie_f1)\n            train_macro_f1s.append(train_macro_f1)\n            \n            val_losses.append(val_loss)\n            val_truth_f1s.append(val_truth_f1)\n            val_lie_f1s.append(val_lie_f1)\n            val_macro_f1s.append(val_macro_f1)\n            \n            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n            print(f\"Train - Loss: {train_loss:.4f}, Truth F1: {train_truth_f1:.4f}, Lie F1: {train_lie_f1:.4f}, Macro F1: {train_macro_f1:.4f}\")\n            print(f\"Val   - Loss: {val_loss:.4f}, Truth F1: {val_truth_f1:.4f}, Lie F1: {val_lie_f1:.4f}, Macro F1: {val_macro_f1:.4f}\")\n            print(\"Confusion Matrix (Val):\")\n            print(val_cm)\n            print(\"-\" * 50)\n            plot_confusion_matrix(val_cm, epoch, 'val')\n            \n            improved = False\n            if val_truth_f1 > best_truth_f1:\n                best_truth_f1 = val_truth_f1\n                torch.save(model.state_dict(), 'best_truth_f1_model.pt')\n                print(f\"Saved new best model with Truth F1: {val_truth_f1:.4f}\")\n                improved = True\n            if val_macro_f1 > best_macro_f1:\n                best_macro_f1 = val_macro_f1\n                torch.save(model.state_dict(), 'best_macro_f1_model.pt')\n                print(f\"Saved new best model with Macro F1: {val_macro_f1:.4f}\")\n                improved = True\n            \n            if not improved:\n                no_improvement_count += 1\n                if no_improvement_count >= EARLY_STOPPING_PATIENCE:\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    break\n            else:\n                no_improvement_count = 0\n        \n        plot_metrics(train_losses, val_losses, 'Loss')\n        plot_metrics(train_truth_f1s, val_truth_f1s, 'Truth F1')\n        plot_metrics(train_lie_f1s, val_lie_f1s, 'Lie F1')\n        plot_metrics(train_macro_f1s, val_macro_f1s, 'Macro F1')\n        \n        print(\"\\nEvaluating best model (by Truth F1) on test set:\")\n        model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n            model, test_loader, DEVICE, class_weights,\n            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n        )\n        print(f\"\\nTest Results - Truth F1 Model:\")\n        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n        print(\"Confusion Matrix:\")\n        print(test_cm)\n        \n        print(\"\\nEvaluating best model (by Macro F1) on test set:\")\n        model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n            model, test_loader, DEVICE, class_weights,\n            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n        )\n        print(f\"\\nTest Results - Macro F1 Model:\")\n        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n        print(\"Confusion Matrix:\")\n        print(test_cm)\n        \n    except Exception as e:\n        print(f\"An error occurred during execution: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"execution":{"iopub.execute_input":"2025-04-12T18:01:12.683858Z","iopub.status.busy":"2025-04-12T18:01:12.683629Z","iopub.status.idle":"2025-04-12T19:14:40.141694Z","shell.execute_reply":"2025-04-12T19:14:40.140906Z","shell.execute_reply.started":"2025-04-12T18:01:12.683841Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-04-12 18:01:26.408568: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1744480886.615921      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1744480886.673489      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]},{"name":"stdout","output_type":"stream","text":["Using device: cuda\n","Loading tokenizer...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"80d050da82fa4b28874bbb731a1ddcb0","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2fd55127bbf549b3be414b4830c39e9e","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b640500324244c4a5394e7b47f194a9","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33762378eac74375a685059b4d3ae280","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"201e106308474c918e3838be72db5efd","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Loading datasets...\n","Dataset loaded from /kaggle/input/deception-data/data/train.jsonl\n","Total messages: 13132\n","Truth: 591 (4.50%)\n","Lie: 12541 (95.50%)\n","Dataset loaded from /kaggle/input/deception-data/data/validation.jsonl\n","Total messages: 1416\n","Truth: 56 (3.95%)\n","Lie: 1360 (96.05%)\n","Dataset loaded from /kaggle/input/deception-data/data/test.jsonl\n","Total messages: 2741\n","Truth: 240 (8.76%)\n","Lie: 2501 (91.24%)\n","Class weights: Truth = 11.1100, Lie = 0.5236\n","Initializing improved model...\n"]},{"name":"stderr","output_type":"stream","text":["Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b5693db3bf94b8dbc103e0f46713dc8","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["Starting training for 5 epochs...\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1109/1109 [13:37<00:00,  1.36it/s]\n","Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Truth     0.0714    0.6250    0.1282        56\n","         Lie     0.9773    0.6654    0.7918      1360\n","\n","    accuracy                         0.6638      1416\n","   macro avg     0.5244    0.6452    0.4600      1416\n","weighted avg     0.9415    0.6638    0.7655      1416\n","\n","Epoch 1/5\n","Train - Loss: 0.2857, Truth F1: 0.7075, Lie F1: 0.4361, Macro F1: 0.5718\n","Val   - Loss: 0.3325, Truth F1: 0.1282, Lie F1: 0.7918, Macro F1: 0.4600\n","Confusion Matrix (Val):\n","[[ 35  21]\n"," [455 905]]\n","--------------------------------------------------\n","Saved new best model with Truth F1: 0.1282\n","Saved new best model with Macro F1: 0.4600\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1109/1109 [13:36<00:00,  1.36it/s]\n","Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.17it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Truth     0.0964    0.1429    0.1151        56\n","         Lie     0.9640    0.9449    0.9543      1360\n","\n","    accuracy                         0.9131      1416\n","   macro avg     0.5302    0.5439    0.5347      1416\n","weighted avg     0.9297    0.9131    0.9211      1416\n","\n","Epoch 2/5\n","Train - Loss: 0.1155, Truth F1: 0.9290, Lie F1: 0.9221, Macro F1: 0.9255\n","Val   - Loss: 0.8132, Truth F1: 0.1151, Lie F1: 0.9543, Macro F1: 0.5347\n","Confusion Matrix (Val):\n","[[   8   48]\n"," [  75 1285]]\n","--------------------------------------------------\n","Saved new best model with Macro F1: 0.5347\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1109/1109 [13:36<00:00,  1.36it/s]\n","Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.18it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Truth     0.0678    0.1429    0.0920        56\n","         Lie     0.9630    0.9191    0.9406      1360\n","\n","    accuracy                         0.8884      1416\n","   macro avg     0.5154    0.5310    0.5163      1416\n","weighted avg     0.9276    0.8884    0.9070      1416\n","\n","Epoch 3/5\n","Train - Loss: 0.0521, Truth F1: 0.9736, Lie F1: 0.9730, Macro F1: 0.9733\n","Val   - Loss: 1.1728, Truth F1: 0.0920, Lie F1: 0.9406, Macro F1: 0.5163\n","Confusion Matrix (Val):\n","[[   8   48]\n"," [ 110 1250]]\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1109/1109 [13:36<00:00,  1.36it/s]\n","Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Truth     0.0870    0.0714    0.0784        56\n","         Lie     0.9620    0.9691    0.9656      1360\n","\n","    accuracy                         0.9336      1416\n","   macro avg     0.5245    0.5203    0.5220      1416\n","weighted avg     0.9274    0.9336    0.9305      1416\n","\n","Epoch 4/5\n","Train - Loss: 0.0313, Truth F1: 0.9868, Lie F1: 0.9867, Macro F1: 0.9868\n","Val   - Loss: 2.2120, Truth F1: 0.0784, Lie F1: 0.9656, Macro F1: 0.5220\n","Confusion Matrix (Val):\n","[[   4   52]\n"," [  42 1318]]\n","--------------------------------------------------\n"]},{"name":"stderr","output_type":"stream","text":["Training: 100%|██████████| 1109/1109 [13:35<00:00,  1.36it/s]\n","Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.19it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Truth     0.1042    0.0893    0.0962        56\n","         Lie     0.9627    0.9684    0.9655      1360\n","\n","    accuracy                         0.9336      1416\n","   macro avg     0.5334    0.5288    0.5308      1416\n","weighted avg     0.9288    0.9336    0.9312      1416\n","\n","Epoch 5/5\n","Train - Loss: 0.0221, Truth F1: 0.9923, Lie F1: 0.9923, Macro F1: 0.9923\n","Val   - Loss: 2.3023, Truth F1: 0.0962, Lie F1: 0.9655, Macro F1: 0.5308\n","Confusion Matrix (Val):\n","[[   5   51]\n"," [  43 1317]]\n","--------------------------------------------------\n","\n","Evaluating best model (by Truth F1) on test set:\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_31/3032531545.py:633: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n","Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.21it/s]\n","/tmp/ipykernel_31/3032531545.py:644: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Truth     0.1421    0.5917    0.2292       240\n","         Lie     0.9437    0.6573    0.7749      2501\n","\n","    accuracy                         0.6516      2741\n","   macro avg     0.5429    0.6245    0.5021      2741\n","weighted avg     0.8736    0.6516    0.7271      2741\n","\n","\n","Test Results - Truth F1 Model:\n","Loss: 0.4124, Truth F1: 0.2292, Lie F1: 0.7749, Macro F1: 0.5021\n","Confusion Matrix:\n","[[ 142   98]\n"," [ 857 1644]]\n","\n","Evaluating best model (by Macro F1) on test set:\n"]},{"name":"stderr","output_type":"stream","text":["Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","       Truth     0.2235    0.1667    0.1909       240\n","         Lie     0.9219    0.9444    0.9330      2501\n","\n","    accuracy                         0.8763      2741\n","   macro avg     0.5727    0.5555    0.5620      2741\n","weighted avg     0.8608    0.8763    0.8681      2741\n","\n","\n","Test Results - Macro F1 Model:\n","Loss: 1.8916, Truth F1: 0.1909, Lie F1: 0.9330, Macro F1: 0.5620\n","Confusion Matrix:\n","[[  40  200]\n"," [ 139 2362]]\n"]}],"execution_count":null},{"id":"60526383","cell_type":"markdown","source":"# INFERENCE CODE","metadata":{}},{"id":"89e306ad","cell_type":"code","source":"def evaluate(model, dataloader, device, class_weights, truth_focal_weight=4.0):\n    model.eval()\n    total_loss = 0\n    all_labels = []\n    all_preds = []\n    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            batch_data = prepare_batch_for_model(batch, device)\n            outputs = model(\n                input_ids=batch_data['input_ids'], \n                attention_mask=batch_data['attention_mask'],\n                context_input_ids=batch_data['context_input_ids'],\n                context_attention_mask=batch_data['context_attention_mask'],\n                game_scores=batch_data['scores'],\n                batch_adj_matrix=batch_data['batch_adj_matrix'],\n                conceptnet_features=batch_data['conceptnet_features']\n            )\n            loss = loss_fn(outputs, batch_data['labels'])\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, dim=1)\n            all_labels.extend(batch_data['labels'].cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n    lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    print(classification_report(all_labels, all_preds, target_names=['Truth', 'Lie'], digits=4, zero_division=0))\n    cm = confusion_matrix(all_labels, all_preds)\n    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n\ndef plot_confusion_matrix(cm, epoch, split='val'):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=['Truth', 'Lie'], yticklabels=['Truth', 'Lie'])\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title(f'Confusion Matrix - {split.capitalize()} (Epoch {epoch+1})')\n    plt.savefig(f'confusion_matrix_{split}_epoch_{epoch+1}.png')\n    plt.close()\n\ndef plot_metrics(train_metric, val_metric, metric_name):\n    plt.figure(figsize=(10, 6))\n    epochs = range(1, len(train_metric) + 1)\n    plt.plot(epochs, train_metric, 'b-', label=f'Train {metric_name}')\n    plt.plot(epochs, val_metric, 'r-', label=f'Val {metric_name}')\n    plt.title(f'{metric_name} over Training')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric_name)\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f'{metric_name.lower()}_plot.png')\n    plt.close()\n    \nprint(\"Loading tokenizer...\")\ntokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n\nprint(\"Loading datasets...\")\ntrain_dataset = EnhancedDeceptionDataset(TRAIN_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\nval_dataset = EnhancedDeceptionDataset(VAL_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\ntest_dataset = EnhancedDeceptionDataset(TEST_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n\ntrain_sampler = EnhancedBalancedSampler(train_dataset, oversample_factor=OVERSAMPLING_FACTOR)\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    sampler=train_sampler,\n    num_workers=2,\n    collate_fn=custom_collate_fn\n)\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    collate_fn=custom_collate_fn\n)\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=2,\n    collate_fn=custom_collate_fn\n)\n\ntrain_class_counts = train_dataset.class_counts\ntotal_samples = sum(train_class_counts.values())\nweight_0 = total_samples / (train_class_counts.get(0, 1) * 2)\nweight_1 = total_samples / (train_class_counts.get(1, 1) * 2)\nclass_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(DEVICE)\nprint(f\"Class weights: Truth = {weight_0:.4f}, Lie = {weight_1:.4f}\")\n\nprint(\"Initializing improved model...\")\nmodel = ImprovedDeceptionModel(TRANSFORMER_MODEL, use_game_scores=USE_GAME_SCORES).to(DEVICE)\n\nprint(\"\\nEvaluating best model (by Truth F1) on test set:\")\nmodel.load_state_dict(torch.load('/kaggle/input/novel_models_conceptnet/pytorch/default/1/best_truth_f1_model.pt'))\ntest_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n    model, test_loader, DEVICE, class_weights,\n    truth_focal_weight=TRUTH_FOCAL_WEIGHT\n)\nprint(f\"\\nTest Results - Truth F1 Model:\")\nprint(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\nprint(\"Confusion Matrix:\")\nprint(test_cm)\n\nprint(\"\\nEvaluating best model (by Macro F1) on test set:\")\nmodel.load_state_dict(torch.load('/kaggle/input/novel_models_conceptnet/pytorch/default/1/best_macro_f1_model.pt'))\ntest_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n    model, test_loader, DEVICE, class_weights,\n    truth_focal_weight=TRUTH_FOCAL_WEIGHT\n)\nprint(f\"\\nTest Results - Macro F1 Model:\")\nprint(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\nprint(\"Confusion Matrix:\")\nprint(test_cm)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T01:33:58.171460Z","iopub.execute_input":"2025-04-13T01:33:58.172174Z","iopub.status.idle":"2025-04-13T01:37:52.447425Z","shell.execute_reply.started":"2025-04-13T01:33:58.172149Z","shell.execute_reply":"2025-04-13T01:37:52.446570Z"}},"outputs":[{"name":"stdout","text":"Loading tokenizer...\nLoading datasets...\nDataset loaded from /kaggle/input/nlp-proj-data/Data/train.jsonl\nTotal messages: 13132\nTruth: 591 (4.50%)\nLie: 12541 (95.50%)\nDataset loaded from /kaggle/input/nlp-proj-data/Data/validation.jsonl\nTotal messages: 1416\nTruth: 56 (3.95%)\nLie: 1360 (96.05%)\nDataset loaded from /kaggle/input/nlp-proj-data/Data/test.jsonl\nTotal messages: 2741\nTruth: 240 (8.76%)\nLie: 2501 (91.24%)\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Class weights: Truth = 11.1100, Lie = 0.5236\nInitializing improved model...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating best model (by Truth F1) on test set:\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/626108610.py:99: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/novel_models_conceptnet/pytorch/default/1/best_truth_f1_model.pt'))\nEvaluating: 100%|██████████| 86/86 [00:21<00:00,  3.98it/s]\n/tmp/ipykernel_31/626108610.py:110: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/novel_models_conceptnet/pytorch/default/1/best_macro_f1_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.1421    0.5917    0.2292       240\n         Lie     0.9437    0.6573    0.7749      2501\n\n    accuracy                         0.6516      2741\n   macro avg     0.5429    0.6245    0.5021      2741\nweighted avg     0.8736    0.6516    0.7271      2741\n\n\nTest Results - Truth F1 Model:\nLoss: 0.4124, Truth F1: 0.2292, Lie F1: 0.7749, Macro F1: 0.5021\nConfusion Matrix:\n[[ 142   98]\n [ 857 1644]]\n\nEvaluating best model (by Macro F1) on test set:\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.16it/s]","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.2235    0.1667    0.1909       240\n         Lie     0.9219    0.9444    0.9330      2501\n\n    accuracy                         0.8763      2741\n   macro avg     0.5727    0.5555    0.5620      2741\nweighted avg     0.8608    0.8763    0.8681      2741\n\n\nTest Results - Macro F1 Model:\nLoss: 1.8916, Truth F1: 0.1909, Lie F1: 0.9330, Macro F1: 0.5620\nConfusion Matrix:\n[[  40  200]\n [ 139 2362]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":14},{"id":"0f1a2082-4065-411e-83f6-6f1e1af4712e","cell_type":"code","source":"! zip -r /kaggle/working/archive.zip /kaggle/working/","metadata":{"execution":{"iopub.execute_input":"2025-04-12T19:15:26.768497Z","iopub.status.busy":"2025-04-12T19:15:26.768206Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/ (stored 0%)\n","  adding: kaggle/working/best_macro_f1_model.pt"]}],"execution_count":null}]}