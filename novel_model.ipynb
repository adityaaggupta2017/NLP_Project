{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efce7ebe",
   "metadata": {},
   "source": [
    "# CODE FILE FOR ROBERTA + LSTM + GCN + NO CONCEPTNET  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c19470-1dee-4de7-9149-61aa68206de6",
   "metadata": {},
   "source": [
    "# Improved Deception Detection Model\n",
    "\n",
    "In this code file , we have implemented an end‐to‐end revised version of the deception detection code. In this version, we incorporate several improvements inspired by the research (e.g. Peskov et al. 2020) and related work on deception detection. In particular, we:\n",
    "\n",
    "- Introduce a new context encoder module that uses a bidirectional LSTM to summarize token‐level representations of the conversation context.\n",
    "- Fuse four streams of information: the current message representation (from RoBERTa), the contextual summary from the LSTM, graph features derived from conversation (via two graph attention layers), and game score features (as a proxy for power dynamics)x.\n",
    "- Use an ensemble of classifier heads (with learnable weights) to stabilize predictions.\n",
    "\n",
    "Due to limited data size, early transformer layers are frozen and extra regularization is applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79d70f-8202-4d45-bb1f-91448b0bbedb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:15:37.611635Z",
     "iopub.status.busy": "2025-04-12T15:15:37.611370Z",
     "iopub.status.idle": "2025-04-12T15:16:04.082600Z",
     "shell.execute_reply": "2025-04-12T15:16:04.081960Z",
     "shell.execute_reply.started": "2025-04-12T15:15:37.611612Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-12 15:15:52.293809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744470952.507225      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744470952.573682      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ALL IMPORTS ARE HERE . \n",
    "\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, RobertaModel\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6083df-7696-4b3e-962c-feb9f15b882c",
   "metadata": {},
   "source": [
    "## Data Paths and Model Parameters\n",
    "\n",
    "Set the paths for the training, validation, and test datasets and define the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a44ef2-c136-4583-b24f-65ddfbffa774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:16:04.083664Z",
     "iopub.status.busy": "2025-04-12T15:16:04.083193Z",
     "iopub.status.idle": "2025-04-12T15:16:04.088176Z",
     "shell.execute_reply": "2025-04-12T15:16:04.087540Z",
     "shell.execute_reply.started": "2025-04-12T15:16:04.083645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "TRAIN_PATH = r\"/kaggle/input/deception-data/data/train.jsonl\"\n",
    "VAL_PATH = r\"/kaggle/input/deception-data/data/validation.jsonl\"\n",
    "TEST_PATH = r\"/kaggle/input/deception-data/data/test.jsonl\"\n",
    "\n",
    "# Model and training parameters\n",
    "TRANSFORMER_MODEL = \"roberta-base\"  # Using RoBERTa base\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 5e-6\n",
    "USE_GAME_SCORES = True\n",
    "OVERSAMPLING_FACTOR = 30  # Oversampling factor (tune if necessary)\n",
    "TRUTH_FOCAL_WEIGHT = 4.0  # Additional weight for truth class during loss calculation\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "GRADIENT_ACCUMULATION_STEPS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e079dcf-b7e4-4e16-977b-41938f902634",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We define a custom collate function and a dataset class that loads the JSONL data. For each conversation, we extract the message, its label, game score, and prior context (up to two previous messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab59264-6a57-4a91-aa0b-f67f732a7c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:16:04.090477Z",
     "iopub.status.busy": "2025-04-12T15:16:04.090258Z",
     "iopub.status.idle": "2025-04-12T15:16:04.110850Z",
     "shell.execute_reply": "2025-04-12T15:16:04.110323Z",
     "shell.execute_reply.started": "2025-04-12T15:16:04.090452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# this is a custom collate function to handle the relative positions \n",
    "def custom_collate_fn(batch): \n",
    "    # Collate all keys except 'relative_positions'\n",
    "    batch_without_relative = [{k: v for k, v in item.items() if k != 'relative_positions'} for item in batch] # removing the relative positions \n",
    "    collated = torch.utils.data.dataloader.default_collate(batch_without_relative) # collating the rest of the data \n",
    "    collated['relative_positions'] = [item['relative_positions'] for item in batch] # collating the relative positions \n",
    "    return collated # Custom dataset class for Enhanced Deception Dataset \n",
    "\n",
    "# now , we define the custom dataset class for the enhanced deception dataset \n",
    "class EnhancedDeceptionDataset(Dataset): \n",
    "    def __init__(self, path, tokenizer, max_len=128, use_game_scores=True):\n",
    "        self.tokenizer = tokenizer # tokenizer for the model\n",
    "        self.max_len = max_len # maximum length of the input sequences \n",
    "        self.use_game_scores = use_game_scores # whether to use game scores or not \n",
    "        self.texts = [] # list to store the texts \n",
    "        self.labels = [] # list to store the labels \n",
    "        self.scores = [] # list to store the scores \n",
    "        self.conversation_ids = []  # Tracking conversation id\n",
    "        self.message_positions = []  # Message order in conversation\n",
    "        self.prior_context = []  # Prior context (concatenated previous messages)\n",
    "        \n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            conv_id = 0 # Initializing conversation ID \n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                messages = data.get('messages', []) # Extracting messages from the data \n",
    "                labels = data.get('sender_labels', []) # Extracting labels from the data \n",
    "                game_scores = data.get('game_score_delta', None) if use_game_scores else None # Extracting game scores from the data \n",
    "                if game_scores is None:\n",
    "                    game_scores = [0] * len(messages)\n",
    "                \n",
    "                for pos, (msg, label, score) in enumerate(zip(messages, labels, game_scores)): # Looping over the messages \n",
    "                    if label in [True, False, \"true\", \"false\", \"True\", \"False\"]: # Checking if the label is a boolean value \n",
    "                        # Convert string labels to boolean if needed\n",
    "                        if isinstance(label, str):  \n",
    "                            is_lie = label.lower() == \"true\" # Convert string to boolean \n",
    "                        else:\n",
    "                            is_lie = label\n",
    "                        # Convention: 1 = lie, 0 = truth\n",
    "                        self.texts.append(msg) # Adding the text to the list \n",
    "                        self.labels.append(1 if is_lie else 0) # Adding the label to the list \n",
    "                        self.scores.append(float(score)) # Adding the score to the list \n",
    "                        self.conversation_ids.append(conv_id) # Adding the conversation ID to the list \n",
    "                        self.message_positions.append(pos) # Adding the message position to the list\n",
    "                        # Build prior context: use up to two previous messages\n",
    "                        context = \"\" # Initializing the context \n",
    "                        if pos > 0:\n",
    "                            context_msgs = messages[max(0, pos-2):pos]\n",
    "                            context = \" [SEP] \".join(context_msgs)\n",
    "                        self.prior_context.append(context)\n",
    "                conv_id += 1\n",
    "        \n",
    "        self.class_counts = Counter(self.labels) # Counting the number of classes \n",
    "        total = len(self.labels) # Total number of messages \n",
    "        self.truth_indices = [i for i, label in enumerate(self.labels) if label == 0] # Getting the indices of truth messages \n",
    "        self.lie_indices = [i for i, label in enumerate(self.labels) if label == 1] # Getting the indices of lie messages \n",
    "        \n",
    "        # Group messages by conversation for potential graph construction\n",
    "        self.conv_to_msgs = defaultdict(list) # Dictionary to store conversation to messages mapping \n",
    "        for i, cid in enumerate(self.conversation_ids): # Looping over the conversation IDs \n",
    "            self.conv_to_msgs[cid].append(i)\n",
    "        \n",
    "        print(f\"Dataset loaded from {path}\") # Printing the dataset loaded message\n",
    "        print(f\"Total messages: {total}\")\n",
    "        for label, count in sorted(self.class_counts.items()):\n",
    "            label_name = \"Truth\" if label == 0 else \"Lie\"\n",
    "            print(f\"{label_name}: {count} ({count/total*100:.2f}%)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        score = self.scores[idx]\n",
    "        conv_id = self.conversation_ids[idx]\n",
    "        position = self.message_positions[idx]\n",
    "        context = self.prior_context[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        if context:\n",
    "            context_encoding = self.tokenizer(\n",
    "                context,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        else:\n",
    "            context_encoding = {\n",
    "                'input_ids': torch.zeros((1, self.max_len), dtype=torch.long),\n",
    "                'attention_mask': torch.zeros((1, self.max_len), dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'context_input_ids': context_encoding['input_ids'].flatten(),\n",
    "            'context_attention_mask': context_encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'score': torch.tensor(score, dtype=torch.float),\n",
    "            'conv_id': conv_id,\n",
    "            'position': position,\n",
    "            'relative_positions': []  # Reserved for legacy\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73263c5b-c12e-4782-9e10-eb39e83c5619",
   "metadata": {},
   "source": [
    "## Enhanced Balanced Sampler\n",
    "\n",
    "This sampler oversamples the truth class to counteract class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0939318-ad48-4d9d-b220-f6e6f385cecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:16:04.111676Z",
     "iopub.status.busy": "2025-04-12T15:16:04.111483Z",
     "iopub.status.idle": "2025-04-12T15:16:04.131094Z",
     "shell.execute_reply": "2025-04-12T15:16:04.130345Z",
     "shell.execute_reply.started": "2025-04-12T15:16:04.111661Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedBalancedSampler(torch.utils.data.Sampler): # we are using a custom sampler to balance the dataset \n",
    "    def __init__(self, dataset, oversample_factor=15):\n",
    "        self.dataset = dataset # the dataset object \n",
    "        self.oversample_factor = oversample_factor # the oversampling factor \n",
    "        self.truth_indices = dataset.truth_indices * oversample_factor  # Oversample truths\n",
    "        self.lie_indices = dataset.lie_indices  # Oversample lies \n",
    "        \n",
    "        min_samples = max(1000, len(self.truth_indices))    # Minimum samples for each class \n",
    "        target_size = min(len(self.truth_indices), len(self.lie_indices))\n",
    "        target_size = max(target_size, min_samples)\n",
    "        \n",
    "        if len(self.truth_indices) < target_size:\n",
    "            self.truth_indices = self.truth_indices * (target_size // len(self.truth_indices) + 1) # Oversample truths to reach target size \n",
    "        if len(self.lie_indices) < target_size:\n",
    "            self.lie_indices = self.lie_indices * (target_size // len(self.lie_indices) + 1) # Oversample lies to reach target size \n",
    "        \n",
    "        self.truth_indices = random.sample(self.truth_indices, target_size) # Randomly sample truths to target size \n",
    "        self.lie_indices = random.sample(self.lie_indices, target_size) # Randomly sample lies to target size \n",
    "        \n",
    "        self.indices = self.truth_indices + self.lie_indices\n",
    "        random.shuffle(self.indices)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce1fbe-e325-4946-baee-4505d2d6168b",
   "metadata": {},
   "source": [
    "## Model Components\n",
    "\n",
    "We define the following model components:\n",
    "\n",
    "1. **Simple Attention Module** (for potential auxiliary use).\n",
    "2. **Graph Attention Layer** (to compute relational features from conversation graphs).\n",
    "3. **Context Encoder**: a bidirectional LSTM that summarizes token-level representations of the conversation context. *(Note: This module now clamps any zero-length sequences to have length 1.)*\n",
    "4. **ImprovedDeceptionModel**: integrates the current message, context summary, graph features, and game score features; uses an ensemble of classifier heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982076c5-0d2a-4826-bff3-4da66f6bc4e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:16:04.132524Z",
     "iopub.status.busy": "2025-04-12T15:16:04.131921Z",
     "iopub.status.idle": "2025-04-12T15:16:04.153816Z",
     "shell.execute_reply": "2025-04-12T15:16:04.153109Z",
     "shell.execute_reply.started": "2025-04-12T15:16:04.132505Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module): # pylint: disable=invalid-name\n",
    "    def __init__(self, hidden_size, dropout=0.1): # Simple attention mechanism \n",
    "        super(SimpleAttention, self).__init__() \n",
    "        self.hidden_size = hidden_size \n",
    "        self.query_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.query_proj(query) # Projecting the query\n",
    "        key = self.key_proj(key) # Projecting the key \n",
    "        value = self.value_proj(value)  # Projecting the value \n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.hidden_size ** 0.5) # Computing the attention scores \n",
    "        if mask is not None: \n",
    "            scores = scores.masked_fill(mask == 0, -1e9)   # Applying the mask to the scores \n",
    "        attn_weights = F.softmax(scores, dim=-1) # Applying softmax to the scores \n",
    "        attn_weights = self.dropout(attn_weights) # Applying dropout to the weights \n",
    "        context = torch.matmul(attn_weights, value) # Computing the context vector \n",
    "        output = self.out_proj(context) # Projecting the context vector \n",
    "        return output # Graph attention layer for GAT (Graph Attention Network)\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1, alpha=0.2): \n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features \n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features))) # Weight matrix for linear transformation\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414) # Initializing the weight matrix \n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1))) # Weight matrix for attention mechanism \n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414) # Initializing the attention weight matrix \n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha) # Leaky ReLU activation function \n",
    "        \n",
    "    def forward(self, features, adj_matrix):\n",
    "        h = torch.mm(features, self.W)  # (N, out_features)\n",
    "        N = h.size()[0]\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1) # Concatenating the features for attention computation \n",
    "        a_input = a_input.view(N, N, 2 * self.out_features) # Reshaping the input for attention computation \n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2)) # Computing the attention scores \n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj_matrix > 0, e, zero_vec)  # Applying the adjacency matrix to the attention scores \n",
    "        attention = F.softmax(attention, dim=1) # Applying softmax to the attention scores\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training) # Applying dropout to the attention scores\n",
    "        h_prime = torch.matmul(attention, h) # Computing the new node features\n",
    "        return h_prime\n",
    "\n",
    "\n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=True, dropout=0.1):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, \n",
    "                            bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # Compute lengths from attention mask, clamp to minimum of 1 to avoid zero-length sequences\n",
    "        lengths = torch.clamp(attention_mask.sum(dim=1), min=1).cpu()\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(token_embeddings, lengths, batch_first=True, enforce_sorted=False) \n",
    "        packed_outputs, (h_n, _) = self.lstm(packed)\n",
    "        num_directions = 2 if self.lstm.bidirectional else 1\n",
    "        h_n = h_n.view(self.lstm.num_layers, num_directions, token_embeddings.size(0), self.lstm.hidden_size)\n",
    "        h_final = torch.cat((h_n[-1, 0, :, :], h_n[-1, 1, :, :]), dim=1)\n",
    "        return h_final\n",
    "\n",
    "\n",
    "class ImprovedDeceptionModel(nn.Module):\n",
    "    def __init__(self, model_name, use_game_scores=True):\n",
    "        super(ImprovedDeceptionModel, self).__init__()\n",
    "        # Pretrained transformers for current message and context\n",
    "        self.transformer = RobertaModel.from_pretrained(model_name)\n",
    "        self.context_transformer = RobertaModel.from_pretrained(model_name) \n",
    "        \n",
    "        # Freezing first 2 encoder layers for both transformers\n",
    "        for layer in self.transformer.encoder.layer[:2]: \n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        for layer in self.context_transformer.encoder.layer[:2]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.hidden_size = self.transformer.config.hidden_size  # Hidden size of the transformer model \n",
    "        self.use_game_scores = use_game_scores # Whether to use game scores or not\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.context_encoder = ContextEncoder(input_dim=self.hidden_size, hidden_dim=self.hidden_size//2, \n",
    "                                               num_layers=1, bidirectional=True, dropout=0.1)\n",
    "        \n",
    "        # Graph attention modules (two-layer GAT)\n",
    "        self.gat1 = GraphAttentionLayer(self.hidden_size + self.context_encoder.output_dim, 256)  # First GAT layer\n",
    "        self.gat2 = GraphAttentionLayer(256, 256) # Second GAT layer \n",
    "        \n",
    "        # Game score processing branch\n",
    "        if use_game_scores:\n",
    "            self.score_proj = nn.Sequential(\n",
    "                nn.Linear(1, 32),\n",
    "                nn.LayerNorm(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            )\n",
    "            fusion_dim = self.hidden_size + self.context_encoder.output_dim + 256 + 32\n",
    "        else:\n",
    "            fusion_dim = self.hidden_size + self.context_encoder.output_dim + 256\n",
    "        \n",
    "        self.feature_norm = nn.LayerNorm(fusion_dim)\n",
    "        \n",
    "        # Ensemble of classifier heads\n",
    "        self.classifier1 = nn.Linear(fusion_dim, 2)  # from fused features\n",
    "        self.classifier2 = nn.Linear(self.hidden_size + self.context_encoder.output_dim, 2)  # current+context\n",
    "        self.classifier3 = nn.Linear(256, 2)  # from graph features\n",
    "        self.ensemble_weights = nn.Parameter(torch.ones(3))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, context_input_ids=None, \n",
    "                context_attention_mask=None, game_scores=None, batch_adj_matrix=None):\n",
    "        # Current message representation\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_features = self.dropout(pooled_output)\n",
    "        \n",
    "        # Context processing: encode context tokens and summarize via LSTM\n",
    "        if context_input_ids is not None and torch.sum(context_input_ids) > 0:\n",
    "            ctx_outputs = self.context_transformer(input_ids=context_input_ids, attention_mask=context_attention_mask)  \n",
    "            ctx_token_embeddings = ctx_outputs.last_hidden_state   # [CLS] token  \n",
    "            context_summary = self.context_encoder(ctx_token_embeddings, context_attention_mask) # Summarizing context with LSTM\n",
    "        else:\n",
    "            batch_size = text_features.size(0)\n",
    "            context_summary = torch.zeros(batch_size, self.context_encoder.output_dim, device=text_features.device)\n",
    "        \n",
    "        # Combine current message and context summary\n",
    "        combined_features = torch.cat([text_features, context_summary], dim=1) # Concatenating the features\n",
    "        \n",
    "        # Graph-based relational features\n",
    "        if batch_adj_matrix is not None:\n",
    "            graph_features = self.gat1(combined_features, batch_adj_matrix)\n",
    "            graph_features = F.elu(graph_features)\n",
    "            graph_features = self.gat2(graph_features, batch_adj_matrix)\n",
    "        else:\n",
    "            graph_features = torch.zeros(combined_features.size(0), 256, device=combined_features.device)\n",
    "        \n",
    "        # Fuse features: current+context, graph, and optionally game scores\n",
    "        if self.use_game_scores and game_scores is not None:\n",
    "            score_features = self.score_proj(game_scores.unsqueeze(1))\n",
    "            all_features = torch.cat([combined_features, graph_features, score_features], dim=1)\n",
    "        else:\n",
    "            all_features = torch.cat([combined_features, graph_features], dim=1)\n",
    "        \n",
    "        all_features = self.feature_norm(all_features)\n",
    "        logits1 = self.classifier1(all_features)\n",
    "        logits2 = self.classifier2(combined_features)\n",
    "        logits3 = self.classifier3(graph_features)\n",
    "        ensemble_weights = F.softmax(self.ensemble_weights, dim=0)\n",
    "        final_logits = (ensemble_weights[0] * logits1 +\n",
    "                        ensemble_weights[1] * logits2 +\n",
    "                        ensemble_weights[2] * logits3)\n",
    "        return final_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca6dc5-fddb-4f53-be51-ed873d66e2fd",
   "metadata": {},
   "source": [
    "## Loss, Batch Preparation, and Training/Evaluation Functions\n",
    "\n",
    "We define the focal weighted loss, a batch preparation function (which builds a simple adjacency matrix for conversation graphs), and the training/evaluation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75914b-ebd4-4889-84b6-5a04e060c4a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:16:04.154774Z",
     "iopub.status.busy": "2025-04-12T15:16:04.154490Z",
     "iopub.status.idle": "2025-04-12T15:16:04.180761Z",
     "shell.execute_reply": "2025-04-12T15:16:04.180005Z",
     "shell.execute_reply.started": "2025-04-12T15:16:04.154749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalWeightedLoss(nn.Module):\n",
    "    def __init__(self, class_weights, truth_focal_weight=4.0):\n",
    "        super(FocalWeightedLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.truth_focal_weight = truth_focal_weight\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, weight=self.class_weights, reduction='none')\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        truth_probs = probs[:, 0]  # probability for truth class\n",
    "        truth_mask = (targets == 0).float() # mask for truth class\n",
    "        focal_weight = (1 - truth_probs) ** self.truth_focal_weight # focal weight for truth class\n",
    "        focal_loss = truth_mask * focal_weight * ce_loss + (1 - truth_mask) * ce_loss # focal loss for lie class\n",
    "        return focal_loss.mean() # Function to prepare batch for model input\n",
    "\n",
    "\n",
    "def prepare_batch_for_model(batch, device):\n",
    "    input_ids = batch['input_ids'].to(device) # input ids\n",
    "    attention_mask = batch['attention_mask'].to(device) # attention mask\n",
    "    context_input_ids = batch['context_input_ids'].to(device) if 'context_input_ids' in batch else None # context input ids\n",
    "    context_attention_mask = batch['context_attention_mask'].to(device) if 'context_attention_mask' in batch else None # context attention mask\n",
    "    labels = batch['label'].to(device) \n",
    "    scores = batch['score'].to(device) if 'score' in batch else None\n",
    "    batch_size = input_ids.size(0)\n",
    "    \n",
    "    # Build a simple adjacency matrix for messages in the same conversation\n",
    "    batch_adj_matrix = torch.zeros((batch_size, batch_size), device=device)\n",
    "    conv_ids = batch['conv_id'] if isinstance(batch['conv_id'], list) else batch['conv_id'].tolist()\n",
    "    positions = batch['position'] if isinstance(batch['position'], list) else batch['position'].tolist()\n",
    "    for i in range(batch_size):\n",
    "        for j in range(batch_size):\n",
    "            if conv_ids[i] == conv_ids[j]:\n",
    "                if i == j:\n",
    "                    batch_adj_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    distance = abs(positions[i] - positions[j])\n",
    "                    batch_adj_matrix[i, j] = 1.0 / (distance + 1)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'context_input_ids': context_input_ids,\n",
    "        'context_attention_mask': context_attention_mask,\n",
    "        'labels': labels,\n",
    "        'scores': scores,\n",
    "        'batch_adj_matrix': batch_adj_matrix\n",
    "    }\n",
    "\n",
    "def train(model, dataloader, optimizer, scheduler, device, class_weights, truth_focal_weight=4.0, gradient_accumulation_steps=1):\n",
    "    model.train() # set model to training mode\n",
    "    total_loss = 0\n",
    "    all_labels = [] # list to store all labels\n",
    "    all_preds = [] # initialize lists to store labels and predictions\n",
    "    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight) # loss function\n",
    "    optimizer.zero_grad() # zero the gradients\n",
    "    accumulated_steps = 0 \n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"): # loop over the batches\n",
    "        batch_data = prepare_batch_for_model(batch, device)\n",
    "        outputs = model(\n",
    "            input_ids=batch_data['input_ids'], \n",
    "            attention_mask=batch_data['attention_mask'],\n",
    "            context_input_ids=batch_data['context_input_ids'],\n",
    "            context_attention_mask=batch_data['context_attention_mask'],\n",
    "            game_scores=batch_data['scores'],\n",
    "            batch_adj_matrix=batch_data['batch_adj_matrix']\n",
    "        )\n",
    "        loss = loss_fn(outputs, batch_data['labels']) # calculate the loss\n",
    "        loss = loss / gradient_accumulation_steps # gradient accumulation \n",
    "        loss.backward() # backpropagate the loss\n",
    "        total_loss += loss.item() * gradient_accumulation_steps # accumulate the loss\n",
    "        _, preds = torch.max(outputs, dim=1) # get the predictions\n",
    "        all_labels.extend(batch_data['labels'].cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        accumulated_steps += 1\n",
    "        if accumulated_steps % gradient_accumulation_steps == 0: # update the model every gradient_accumulation_steps\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # clip the gradients\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    if accumulated_steps % gradient_accumulation_steps != 0: # update the model one last time\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip the gradients\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    try:\n",
    "        truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n",
    "        lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n",
    "        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    except Exception:\n",
    "        truth_f1 = lie_f1 = macro_f1 = 0\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n",
    "\n",
    "def evaluate(model, dataloader, device, class_weights, truth_focal_weight=4.0):\n",
    "    model.eval() # set model to evaluation mode \n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = [] # initialize lists to store the labels and predictions\n",
    "    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n",
    "     \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_data = prepare_batch_for_model(batch, device)\n",
    "            outputs = model(\n",
    "                input_ids=batch_data['input_ids'], \n",
    "                attention_mask=batch_data['attention_mask'],\n",
    "                context_input_ids=batch_data['context_input_ids'],\n",
    "                context_attention_mask=batch_data['context_attention_mask'],\n",
    "                game_scores=batch_data['scores'],\n",
    "                batch_adj_matrix=batch_data['batch_adj_matrix']\n",
    "            )\n",
    "            loss = loss_fn(outputs, batch_data['labels'])\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_labels.extend(batch_data['labels'].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    try:\n",
    "        truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n",
    "        lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n",
    "        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        print(classification_report(all_labels, all_preds, target_names=['Truth', 'Lie'], digits=4, zero_division=0))\n",
    "    except Exception:\n",
    "        truth_f1 = lie_f1 = macro_f1 = 0\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, epoch, split='val'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Truth', 'Lie'], yticklabels=['Truth', 'Lie'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix - {split.capitalize()} (Epoch {epoch+1})')\n",
    "    plt.savefig(f'confusion_matrix_{split}_epoch_{epoch+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics(train_metric, val_metric, metric_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_metric) + 1)\n",
    "    plt.plot(epochs, train_metric, 'b-', label=f'Train {metric_name}')\n",
    "    plt.plot(epochs, val_metric, 'r-', label=f'Val {metric_name}')\n",
    "    plt.title(f'{metric_name} over Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{metric_name.lower()}_plot.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a282430-ea4b-4757-8e4d-7603105ba8b8",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "\n",
    "We load the tokenizer and datasets, define data loaders (with oversampling), set up the optimizer and scheduler, and train the model. We monitor performance on the validation set, apply early stopping, and finally evaluate the best models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0640622c-1887-46e2-9a9b-71c32b3dfa01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:16:04.181600Z",
     "iopub.status.busy": "2025-04-12T15:16:04.181391Z",
     "iopub.status.idle": "2025-04-12T16:27:03.566026Z",
     "shell.execute_reply": "2025-04-12T16:27:03.565184Z",
     "shell.execute_reply.started": "2025-04-12T15:16:04.181579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b9e2fe9bf0f4e10a8192bd7b619b0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5912f123a6d74dddb9534d652e357fd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0188babbb09347889a91793659dcfc62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932dc54edb3f42ffb3d7ef768fd9200a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f75c17ed91c47b8a2ec86e6c85c1bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Dataset loaded from /kaggle/input/deception-data/data/train.jsonl\n",
      "Total messages: 13132\n",
      "Truth: 591 (4.50%)\n",
      "Lie: 12541 (95.50%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/validation.jsonl\n",
      "Total messages: 1416\n",
      "Truth: 56 (3.95%)\n",
      "Lie: 1360 (96.05%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/test.jsonl\n",
      "Total messages: 2741\n",
      "Truth: 240 (8.76%)\n",
      "Lie: 2501 (91.24%)\n",
      "Class weights: Truth = 11.1100, Lie = 0.5236\n",
      "Initializing improved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "070db46253f94754ac1e142b9cc7a4ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1109/1109 [13:46<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0556    0.7321    0.1034        56\n",
      "         Lie     0.9779    0.4882    0.6513      1360\n",
      "\n",
      "    accuracy                         0.4979      1416\n",
      "   macro avg     0.5168    0.6102    0.3774      1416\n",
      "weighted avg     0.9414    0.4979    0.6296      1416\n",
      "\n",
      "Epoch 1/5\n",
      "Train - Loss: 0.2762, Truth F1: 0.7111, Lie F1: 0.3555, Macro F1: 0.5333\n",
      "Val   - Loss: 0.3908, Truth F1: 0.1034, Lie F1: 0.6513, Macro F1: 0.3774\n",
      "Confusion Matrix (Val):\n",
      "[[ 41  15]\n",
      " [696 664]]\n",
      "--------------------------------------------------\n",
      "Saved new best model with Truth F1: 0.1034\n",
      "Saved new best model with Macro F1: 0.3774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1109/1109 [13:47<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0946    0.1250    0.1077        56\n",
      "         Lie     0.9635    0.9507    0.9571      1360\n",
      "\n",
      "    accuracy                         0.9181      1416\n",
      "   macro avg     0.5290    0.5379    0.5324      1416\n",
      "weighted avg     0.9291    0.9181    0.9235      1416\n",
      "\n",
      "Epoch 2/5\n",
      "Train - Loss: 0.1277, Truth F1: 0.9113, Lie F1: 0.8994, Macro F1: 0.9053\n",
      "Val   - Loss: 0.7872, Truth F1: 0.1077, Lie F1: 0.9571, Macro F1: 0.5324\n",
      "Confusion Matrix (Val):\n",
      "[[   7   49]\n",
      " [  67 1293]]\n",
      "--------------------------------------------------\n",
      "Saved new best model with Truth F1: 0.1077\n",
      "Saved new best model with Macro F1: 0.5324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1109/1109 [13:48<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1034    0.1607    0.1259        56\n",
      "         Lie     0.9646    0.9426    0.9535      1360\n",
      "\n",
      "    accuracy                         0.9117      1416\n",
      "   macro avg     0.5340    0.5517    0.5397      1416\n",
      "weighted avg     0.9306    0.9117    0.9208      1416\n",
      "\n",
      "Epoch 3/5\n",
      "Train - Loss: 0.0692, Truth F1: 0.9641, Lie F1: 0.9631, Macro F1: 0.9636\n",
      "Val   - Loss: 1.3411, Truth F1: 0.1259, Lie F1: 0.9535, Macro F1: 0.5397\n",
      "Confusion Matrix (Val):\n",
      "[[   9   47]\n",
      " [  78 1282]]\n",
      "--------------------------------------------------\n",
      "Saved new best model with Truth F1: 0.1259\n",
      "Saved new best model with Macro F1: 0.5397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1109/1109 [13:49<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0930    0.0714    0.0808        56\n",
      "         Lie     0.9621    0.9713    0.9667      1360\n",
      "\n",
      "    accuracy                         0.9357      1416\n",
      "   macro avg     0.5276    0.5214    0.5238      1416\n",
      "weighted avg     0.9278    0.9357    0.9317      1416\n",
      "\n",
      "Epoch 4/5\n",
      "Train - Loss: 0.0380, Truth F1: 0.9828, Lie F1: 0.9826, Macro F1: 0.9827\n",
      "Val   - Loss: 1.9773, Truth F1: 0.0808, Lie F1: 0.9667, Macro F1: 0.5238\n",
      "Confusion Matrix (Val):\n",
      "[[   4   52]\n",
      " [  39 1321]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1109/1109 [13:48<00:00,  1.34it/s]\n",
      "Evaluating: 100%|██████████| 45/45 [00:10<00:00,  4.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0882    0.0536    0.0667        56\n",
      "         Lie     0.9616    0.9772    0.9694      1360\n",
      "\n",
      "    accuracy                         0.9407      1416\n",
      "   macro avg     0.5249    0.5154    0.5180      1416\n",
      "weighted avg     0.9271    0.9407    0.9337      1416\n",
      "\n",
      "Epoch 5/5\n",
      "Train - Loss: 0.0270, Truth F1: 0.9898, Lie F1: 0.9897, Macro F1: 0.9898\n",
      "Val   - Loss: 2.2377, Truth F1: 0.0667, Lie F1: 0.9694, Macro F1: 0.5180\n",
      "Confusion Matrix (Val):\n",
      "[[   3   53]\n",
      " [  31 1329]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating best model (by Truth F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/122680207.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n",
      "Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.16it/s]\n",
      "/tmp/ipykernel_31/122680207.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1907    0.1708    0.1802       240\n",
      "         Lie     0.9212    0.9304    0.9258      2501\n",
      "\n",
      "    accuracy                         0.8639      2741\n",
      "   macro avg     0.5560    0.5506    0.5530      2741\n",
      "weighted avg     0.8573    0.8639    0.8605      2741\n",
      "\n",
      "\n",
      "Test Results - Truth F1 Model:\n",
      "Loss: 2.4442, Truth F1: 0.1802, Lie F1: 0.9258, Macro F1: 0.5530\n",
      "Confusion Matrix:\n",
      "[[  41  199]\n",
      " [ 174 2327]]\n",
      "\n",
      "Evaluating best model (by Macro F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1907    0.1708    0.1802       240\n",
      "         Lie     0.9212    0.9304    0.9258      2501\n",
      "\n",
      "    accuracy                         0.8639      2741\n",
      "   macro avg     0.5560    0.5506    0.5530      2741\n",
      "weighted avg     0.8573    0.8639    0.8605      2741\n",
      "\n",
      "\n",
      "Test Results - Macro F1 Model:\n",
      "Loss: 2.4442, Truth F1: 0.1802, Lie F1: 0.9258, Macro F1: 0.5530\n",
      "Confusion Matrix:\n",
      "[[  41  199]\n",
      " [ 174 2327]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main(): \n",
    "    try:\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL) # Load the tokenizer for the transformer model\n",
    "        \n",
    "        print(\"Loading datasets...\") \n",
    "        train_dataset = EnhancedDeceptionDataset(TRAIN_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "        val_dataset = EnhancedDeceptionDataset(VAL_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "        test_dataset = EnhancedDeceptionDataset(TEST_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "        \n",
    "        train_sampler = EnhancedBalancedSampler(train_dataset, oversample_factor=OVERSAMPLING_FACTOR)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        \n",
    "        train_class_counts = train_dataset.class_counts\n",
    "        total_samples = sum(train_class_counts.values())\n",
    "        weight_0 = total_samples / (train_class_counts.get(0, 1) * 2)\n",
    "        weight_1 = total_samples / (train_class_counts.get(1, 1) * 2)\n",
    "        class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(DEVICE)\n",
    "        print(f\"Class weights: Truth = {weight_0:.4f}, Lie = {weight_1:.4f}\")\n",
    "        \n",
    "        print(\"Initializing improved model...\")\n",
    "        model = ImprovedDeceptionModel(TRANSFORMER_MODEL, use_game_scores=USE_GAME_SCORES).to(DEVICE) # Initialize the model\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * EPOCHS // GRADIENT_ACCUMULATION_STEPS\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(total_steps * 0.1),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "        best_truth_f1 = 0\n",
    "        best_macro_f1 = 0\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        train_losses = []\n",
    "        train_truth_f1s = []\n",
    "        train_lie_f1s = []\n",
    "        train_macro_f1s = []\n",
    "        val_losses = []\n",
    "        val_truth_f1s = []\n",
    "        val_lie_f1s = []\n",
    "        val_macro_f1s = []\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_truth_f1, train_lie_f1, train_macro_f1, train_cm = train(\n",
    "                model, train_loader, optimizer, scheduler, DEVICE, class_weights,\n",
    "                truth_focal_weight=TRUTH_FOCAL_WEIGHT,\n",
    "                gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n",
    "            )\n",
    "            val_loss, val_truth_f1, val_lie_f1, val_macro_f1, val_cm = evaluate(\n",
    "                model, val_loader, DEVICE, class_weights,\n",
    "                truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    "            )\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            train_truth_f1s.append(train_truth_f1)\n",
    "            train_lie_f1s.append(train_lie_f1)\n",
    "            train_macro_f1s.append(train_macro_f1)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_truth_f1s.append(val_truth_f1)\n",
    "            val_lie_f1s.append(val_lie_f1)\n",
    "            val_macro_f1s.append(val_macro_f1)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train - Loss: {train_loss:.4f}, Truth F1: {train_truth_f1:.4f}, Lie F1: {train_lie_f1:.4f}, Macro F1: {train_macro_f1:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_loss:.4f}, Truth F1: {val_truth_f1:.4f}, Lie F1: {val_lie_f1:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
    "            print(\"Confusion Matrix (Val):\")\n",
    "            print(val_cm)\n",
    "            print(\"-\" * 50)\n",
    "            plot_confusion_matrix(val_cm, epoch, 'val')\n",
    "            \n",
    "            improved = False\n",
    "            if val_truth_f1 > best_truth_f1:\n",
    "                best_truth_f1 = val_truth_f1\n",
    "                torch.save(model.state_dict(), 'best_truth_f1_model.pt')\n",
    "                print(f\"Saved new best model with Truth F1: {val_truth_f1:.4f}\")\n",
    "                improved = True\n",
    "            if val_macro_f1 > best_macro_f1:\n",
    "                best_macro_f1 = val_macro_f1\n",
    "                torch.save(model.state_dict(), 'best_macro_f1_model.pt')\n",
    "                print(f\"Saved new best model with Macro F1: {val_macro_f1:.4f}\")\n",
    "                improved = True\n",
    "            \n",
    "            if not improved:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= EARLY_STOPPING_PATIENCE:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "        \n",
    "        plot_metrics(train_losses, val_losses, 'Loss')\n",
    "        plot_metrics(train_truth_f1s, val_truth_f1s, 'Truth F1')\n",
    "        plot_metrics(train_lie_f1s, val_lie_f1s, 'Lie F1')\n",
    "        plot_metrics(train_macro_f1s, val_macro_f1s, 'Macro F1')\n",
    "        \n",
    "        print(\"\\nEvaluating best model (by Truth F1) on test set:\")\n",
    "        model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n",
    "        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n",
    "            model, test_loader, DEVICE, class_weights,\n",
    "            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    "        )\n",
    "        print(f\"\\nTest Results - Truth F1 Model:\")\n",
    "        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(test_cm)\n",
    "        \n",
    "        print(\"\\nEvaluating best model (by Macro F1) on test set:\")\n",
    "        model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n",
    "        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n",
    "            model, test_loader, DEVICE, class_weights,\n",
    "            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    "        )\n",
    "        print(f\"\\nTest Results - Macro F1 Model:\")\n",
    "        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(test_cm)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a066da-9884-4b48-aa8a-9e52f3c319d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T16:27:03.567966Z",
     "iopub.status.busy": "2025-04-12T16:27:03.567734Z",
     "iopub.status.idle": "2025-04-12T16:27:47.699932Z",
     "shell.execute_reply": "2025-04-12T16:27:47.699025Z",
     "shell.execute_reply.started": "2025-04-12T16:27:03.567942Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading datasets...\n",
      "Dataset loaded from /kaggle/input/deception-data/data/train.jsonl\n",
      "Total messages: 13132\n",
      "Truth: 591 (4.50%)\n",
      "Lie: 12541 (95.50%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/validation.jsonl\n",
      "Total messages: 1416\n",
      "Truth: 56 (3.95%)\n",
      "Lie: 1360 (96.05%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/test.jsonl\n",
      "Total messages: 2741\n",
      "Truth: 240 (8.76%)\n",
      "Lie: 2501 (91.24%)\n",
      "Class weights: Truth = 11.1100, Lie = 0.5236\n",
      "Initializing improved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model (by Truth F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3638066931.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n",
      "Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.17it/s]\n",
      "/tmp/ipykernel_31/3638066931.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1907    0.1708    0.1802       240\n",
      "         Lie     0.9212    0.9304    0.9258      2501\n",
      "\n",
      "    accuracy                         0.8639      2741\n",
      "   macro avg     0.5560    0.5506    0.5530      2741\n",
      "weighted avg     0.8573    0.8639    0.8605      2741\n",
      "\n",
      "\n",
      "Test Results - Truth F1 Model:\n",
      "Loss: 2.4442, Truth F1: 0.1802, Lie F1: 0.9258, Macro F1: 0.5530\n",
      "Confusion Matrix:\n",
      "[[  41  199]\n",
      " [ 174 2327]]\n",
      "\n",
      "Evaluating best model (by Macro F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1907    0.1708    0.1802       240\n",
      "         Lie     0.9212    0.9304    0.9258      2501\n",
      "\n",
      "    accuracy                         0.8639      2741\n",
      "   macro avg     0.5560    0.5506    0.5530      2741\n",
      "weighted avg     0.8573    0.8639    0.8605      2741\n",
      "\n",
      "\n",
      "Test Results - Macro F1 Model:\n",
      "Loss: 2.4442, Truth F1: 0.1802, Lie F1: 0.9258, Macro F1: 0.5530\n",
      "Confusion Matrix:\n",
      "[[  41  199]\n",
      " [ 174 2327]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL) # Load the tokenizer for the transformer model \n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = EnhancedDeceptionDataset(TRAIN_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)  # Load the training dataset\n",
    "val_dataset = EnhancedDeceptionDataset(VAL_PATH, tokenizer, use_game_scores=USE_GAME_SCORES) # Load the validation dataset\n",
    "test_dataset = EnhancedDeceptionDataset(TEST_PATH, tokenizer, use_game_scores=USE_GAME_SCORES) # Load the test dataset\n",
    "\n",
    "train_sampler = EnhancedBalancedSampler(train_dataset, oversample_factor=OVERSAMPLING_FACTOR) # Create a sampler for the training dataset\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=2,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "train_class_counts = train_dataset.class_counts # Count the number of classes in the training dataset\n",
    "total_samples = sum(train_class_counts.values()) # Total number of samples in the training dataset\n",
    "weight_0 = total_samples / (train_class_counts.get(0, 1) * 2) # Weight for class 0\n",
    "weight_1 = total_samples / (train_class_counts.get(1, 1) * 2) # Weight for class 1\n",
    "class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(DEVICE)    # Class weights for the loss function\n",
    "print(f\"Class weights: Truth = {weight_0:.4f}, Lie = {weight_1:.4f}\")\n",
    "\n",
    "print(\"Initializing improved model...\")\n",
    "model = ImprovedDeceptionModel(TRANSFORMER_MODEL, use_game_scores=USE_GAME_SCORES).to(DEVICE) # Initialize the improved model\n",
    "\n",
    "print(\"\\nEvaluating best model (by Truth F1) on test set:\") # Evaluate the best model on the test set\n",
    "model.load_state_dict(torch.load('best_truth_f1_model.pt')) # Load the best model\n",
    "test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate( \n",
    "    model, test_loader, DEVICE, class_weights,\n",
    "    truth_focal_weight=TRUTH_FOCAL_WEIGHT \n",
    ")\n",
    "print(f\"\\nTest Results - Truth F1 Model:\")\n",
    "print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "print(\"\\nEvaluating best model (by Macro F1) on test set:\")\n",
    "model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n",
    "test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n",
    "    model, test_loader, DEVICE, class_weights,\n",
    "    truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    ")\n",
    "print(f\"\\nTest Results - Macro F1 Model:\")\n",
    "print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a945954-02b1-4215-b3d1-07d3eafe9ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T16:30:15.975375Z",
     "iopub.status.busy": "2025-04-12T16:30:15.974615Z",
     "iopub.status.idle": "2025-04-12T16:33:59.883353Z",
     "shell.execute_reply": "2025-04-12T16:33:59.882563Z",
     "shell.execute_reply.started": "2025-04-12T16:30:15.975344Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/ (stored 0%)\n",
      "  adding: kaggle/working/truth f1_plot.png (deflated 19%)\n",
      "  adding: kaggle/working/confusion_matrix_val_epoch_2.png (deflated 21%)\n",
      "  adding: kaggle/working/confusion_matrix_val_epoch_5.png (deflated 22%)\n",
      "  adding: kaggle/working/confusion_matrix_val_epoch_3.png (deflated 22%)\n",
      "  adding: kaggle/working/confusion_matrix_val_epoch_4.png (deflated 22%)\n",
      "  adding: kaggle/working/lie f1_plot.png (deflated 14%)\n",
      "  adding: kaggle/working/macro f1_plot.png (deflated 14%)\n",
      "  adding: kaggle/working/best_truth_f1_model.pt (deflated 19%)\n",
      "  adding: kaggle/working/confusion_matrix_val_epoch_1.png (deflated 21%)\n",
      "  adding: kaggle/working/best_macro_f1_model.pt (deflated 19%)\n",
      "  adding: kaggle/working/.virtual_documents/ (stored 0%)\n",
      "  adding: kaggle/working/loss_plot.png (deflated 13%)\n"
     ]
    }
   ],
   "source": [
    "! zip -r /kaggle/working/archive.zip /kaggle/working/ \n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7105583,
     "sourceId": 11354484,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7121728,
     "sourceId": 11375484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
