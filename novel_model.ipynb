{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11354606,"sourceType":"datasetVersion","datasetId":7105692}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"49752bef","cell_type":"markdown","source":"# Deception Detection","metadata":{}},{"id":"081f14af-560a-404d-a0a1-465bcf797ef9","cell_type":"code","source":"# import torch\n# print(torch.__version__)\n# print(torch.version.cuda)\n!pip install torch_geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:47:57.022541Z","iopub.execute_input":"2025-04-10T14:47:57.022801Z","iopub.status.idle":"2025-04-10T14:48:02.078079Z","shell.execute_reply.started":"2025-04-10T14:47:57.022777Z","shell.execute_reply":"2025-04-10T14:48:02.077393Z"}},"outputs":[{"name":"stdout","text":"Collecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.11.16)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.19.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch_geometric) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch_geometric) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch_geometric) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch_geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch_geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n","output_type":"stream"}],"execution_count":1},{"id":"6fab158d","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport tqdm.auto as tqdm\nimport os\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nfrom transformers import BartForConditionalGeneration\nfrom transformers import BertTokenizer, BartModel, BartForConditionalGeneration\nfrom torch_geometric.nn import GATConv\nimport torch_geometric\nfrom torch_geometric.data import Data as GeoData\nfrom torch.utils.data import Dataset, DataLoader\nimport networkx as nx\nimport numpy as np\nimport random as random\nimport json\n\n# Set seeds as in the original code base . \ntorch.manual_seed(1994)\nnp.random.seed(1994)\nrandom.seed(1994)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using device:\", DEVICE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:02.079639Z","iopub.execute_input":"2025-04-10T14:48:02.079902Z","iopub.status.idle":"2025-04-10T14:48:27.497314Z","shell.execute_reply.started":"2025-04-10T14:48:02.079873Z","shell.execute_reply":"2025-04-10T14:48:27.496561Z"}},"outputs":[{"name":"stderr","text":"2025-04-10 14:48:14.856331: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744296495.034804      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744296495.087777      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":2},{"id":"80ba8274","cell_type":"code","source":"# Data paths \nTRAIN_PATH = r\"/kaggle/input/quanta-diplomacy/train.jsonl\"\nVAL_PATH = r\"/kaggle/input/quanta-diplomacy/validation.jsonl\"\nTEST_PATH = r\"/kaggle/input/quanta-diplomacy/test.jsonl\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.497990Z","iopub.execute_input":"2025-04-10T14:48:27.498524Z","iopub.status.idle":"2025-04-10T14:48:27.502367Z","shell.execute_reply.started":"2025-04-10T14:48:27.498505Z","shell.execute_reply":"2025-04-10T14:48:27.501531Z"}},"outputs":[],"execution_count":3},{"id":"e5c5824b","cell_type":"markdown","source":"## Custom Dataset Class","metadata":{}},{"id":"048325dc","cell_type":"code","source":"class ConversationDataset(Dataset):\n    def __init__(self, path, max_tokens_per_msg=50, max_messages=50, use_game_scores=True):\n        \"\"\"\n        Args:\n            path (str): Path to the JSONL file\n            max_tokens_per_msg (int): Maximum tokens per message\n            max_messages (int): Maximum messages per conversation\n            use_game_scores (bool): Whether to use game scores\n        \"\"\"\n        super().__init__()\n        self.data = []\n        self.max_tokens_per_msg = max_tokens_per_msg\n        self.max_messages = max_messages\n        self.use_game_scores = use_game_scores\n        \n        # Read and process each line in the JSONL file\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                record = json.loads(line)\n                messages = record.get(\"messages\", [])\n                labels = record.get(\"sender_labels\", [])\n                game_scores = record.get(\"game_score_delta\", None) if use_game_scores else None\n                \n                filtered_msgs, filtered_lbls = [], []\n                filtered_scores = []\n                \n                if game_scores is None:\n                    game_scores = [0] * len(messages)\n                \n                for m, l, g in zip(messages, labels, game_scores):\n                    if l in [True, False, \"true\", \"false\", \"True\", \"False\"]:\n                        # Ensure messages are strings\n                        if isinstance(m, dict) and \"text\" in m:\n                            m = m[\"text\"]\n                        elif not isinstance(m, str):\n                            m = str(m)\n                        \n                        filtered_msgs.append(m)\n                        if isinstance(l, str):\n                            filtered_lbls.append(1 if l.lower() == \"true\" else 0)\n                        else:\n                            filtered_lbls.append(1 if l else 0)\n                        \n                        # Convert score to float\n                        try:\n                            if isinstance(g, (str, bool)):\n                                g = float(g) if g and g.strip() and g.lower() != \"false\" else 0.0\n                            elif g is None:\n                                g = 0.0\n                            else:\n                                g = float(g)\n                        except (ValueError, TypeError):\n                            g = 0.0\n                            \n                        filtered_scores.append(g)\n                \n                if len(filtered_msgs) == 0:\n                    continue\n                \n                # Limit number of messages if needed\n                if len(filtered_msgs) > self.max_messages:\n                    filtered_msgs = filtered_msgs[:self.max_messages]\n                    filtered_lbls = filtered_lbls[:self.max_messages]\n                    filtered_scores = filtered_scores[:self.max_messages]\n                \n                # Create edges for the conversation graph\n                edges = [(i, i + 1) for i in range(len(filtered_msgs) - 1)]\n                \n                self.data.append({\n                    'messages': filtered_msgs,\n                    'labels': filtered_lbls,\n                    'scores': filtered_scores,\n                    'edges': edges\n                })\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        # Ensure scores are all float values\n        scores = [float(s) if not isinstance(s, float) else s for s in item['scores']]\n        \n        return {\n            'messages': item['messages'],\n            'labels': torch.tensor(item['labels'], dtype=torch.long),\n            'scores': torch.tensor(scores, dtype=torch.float),\n            'edge_index': torch.tensor(item['edges'], dtype=torch.long).t().contiguous() if item['edges'] else torch.zeros((2, 0), dtype=torch.long)\n        }\n\ndef collate_fn(batch):\n    # Get batch size\n    batch_size = len(batch)\n    \n    # Get max number of messages in this batch\n    max_msgs = max(len(item['messages']) for item in batch)\n    \n    # Initialize lists for batch items\n    all_messages = []\n    all_labels = []\n    all_scores = []\n    attention_masks = []\n    edge_indices = []\n    batch_indices = []\n    \n    # Process each item in the batch\n    for batch_idx, item in enumerate(batch):\n        num_msgs = len(item['messages'])\n        \n        # Add messages\n        all_messages.extend(item['messages'])\n        \n        # Add padding indicators to attention mask (1 for real message, 0 for padding)\n        attention_masks.append([1] * num_msgs + [0] * (max_msgs - num_msgs))\n        \n        # Pad and add labels\n        labels = item['labels'].tolist() + [0] * (max_msgs - num_msgs)\n        all_labels.append(labels)\n        \n        # Pad and add scores\n        scores = item['scores'].tolist() + [0.0] * (max_msgs - num_msgs)\n        all_scores.append(scores)\n        \n        # Add edge indices with batch offset\n        if item['edge_index'].size(1) > 0:  # Only if there are edges\n            edges = item['edge_index'].clone()\n            edges[0] += batch_idx * max_msgs  # Add batch offset\n            edges[1] += batch_idx * max_msgs\n            edge_indices.append(edges)\n        \n        # Add batch indices for each message\n        batch_indices.extend([batch_idx] * num_msgs)\n        batch_indices.extend([-1] * (max_msgs - num_msgs))  # Use -1 to mark padding\n    \n    # Combine edge indices\n    edge_index = torch.cat(edge_indices, dim=1) if edge_indices else torch.zeros((2, 0), dtype=torch.long)\n    \n    # Create batch dictionary\n    result = {\n        'messages': all_messages,  # List of strings, flattened\n        'labels': torch.tensor(all_labels, dtype=torch.long),  # [batch_size, max_msgs]\n        'scores': torch.tensor(all_scores, dtype=torch.float),  # [batch_size, max_msgs]\n        'attention_mask': torch.tensor(attention_masks, dtype=torch.long),  # [batch_size, max_msgs]\n        'batch_indices': torch.tensor(batch_indices, dtype=torch.long),  # [batch_size * max_msgs]\n        'edge_index': edge_index,  # [2, num_edges]\n        'batch_size': batch_size,\n        'max_msgs': max_msgs\n    }\n    \n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.503762Z","iopub.execute_input":"2025-04-10T14:48:27.503989Z","iopub.status.idle":"2025-04-10T14:48:27.522549Z","shell.execute_reply.started":"2025-04-10T14:48:27.503966Z","shell.execute_reply":"2025-04-10T14:48:27.521894Z"}},"outputs":[],"execution_count":4},{"id":"ff77b22f","cell_type":"markdown","source":"## Message Encoder","metadata":{}},{"id":"5218b78f-a202-42d3-ae91-2de138133553","cell_type":"code","source":"import math\n\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout=0.1, max_len=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n\n        position = torch.arange(max_len).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n        pe = torch.zeros(max_len, 1, d_model)\n        pe[:, 0, 0::2] = torch.sin(position * div_term)\n        pe[:, 0, 1::2] = torch.cos(position * div_term)\n        self.register_buffer('pe', pe)\n\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n        \"\"\"\n        x = x + self.pe[:x.size(0)]\n        return self.dropout(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.523343Z","iopub.execute_input":"2025-04-10T14:48:27.523625Z","iopub.status.idle":"2025-04-10T14:48:27.539502Z","shell.execute_reply.started":"2025-04-10T14:48:27.523608Z","shell.execute_reply":"2025-04-10T14:48:27.538786Z"}},"outputs":[],"execution_count":5},{"id":"4a269d31","cell_type":"code","source":"class MessageEncoder(nn.Module):\n    \"\"\"\n    Encodes each message using a 6-layer Transformer Encoder.\n    \"\"\"\n    def __init__(self, hidden_dim=256, n_heads=8, ff_dim=512, num_layers=6, max_len=128):\n        super().__init__()\n        self.max_len = max_len\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        self.vocab_size = len(self.tokenizer.vocab)\n        self.embedding = nn.Embedding(self.vocab_size, hidden_dim)\n        self.pos_encoder = PositionalEncoding(hidden_dim, dropout=0.1, max_len=max_len)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=hidden_dim, nhead=n_heads, dim_feedforward=ff_dim, dropout=0.1\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.pool = nn.AdaptiveMaxPool1d(1)\n\n    def forward(self, texts):\n        # Tokenize\n        toks = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=self.max_len)\n        \n        # Move tokenized inputs to the same device as the embedding layer\n        device = self.embedding.weight.device\n        input_ids = toks['input_ids'].to(device)\n        attention_mask = toks['attention_mask'].to(device)\n        \n        x = self.embedding(input_ids)  # (B, L, D)\n        \n        # Transformer expects (L, B, D)\n        x = x.permute(1, 0, 2)\n        \n        # Add positional encoding\n        x = self.pos_encoder(x)\n        \n        x = self.transformer(x)  # (L, B, D)\n        x = x.permute(1, 2, 0)    # (B, D, L)\n        \n        # Pool over sequence\n        x = self.pool(x).squeeze(-1)  # (B, D)\n        return x, attention_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.540374Z","iopub.execute_input":"2025-04-10T14:48:27.540618Z","iopub.status.idle":"2025-04-10T14:48:27.553091Z","shell.execute_reply.started":"2025-04-10T14:48:27.540601Z","shell.execute_reply":"2025-04-10T14:48:27.552463Z"}},"outputs":[],"execution_count":6},{"id":"da31aca7","cell_type":"markdown","source":"## Conversation Graph with World Knowledge","metadata":{}},{"id":"c3dc3cad","cell_type":"code","source":"class ConversationGAT(nn.Module):\n    def __init__(self, in_dim, hidden_dim=128, heads=4, layers=3):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        self.layers.append(GATConv(in_dim + 1, hidden_dim, heads=heads, dropout=0.2))\n        \n        for _ in range(layers-2):\n            self.layers.append(GATConv(hidden_dim*heads, hidden_dim, heads=heads, dropout=0.2))\n            \n        self.layers.append(GATConv(hidden_dim*heads, hidden_dim, heads=1, concat=False, dropout=0.2))\n        self.norm = nn.LayerNorm(hidden_dim)\n        \n    def forward(self, node_feats, edge_index, power_deltas):\n        pd = power_deltas.unsqueeze(-1)\n        pd = pd.squeeze(0)\n        x = torch.cat([node_feats, pd], dim=-1)\n        \n        for i, layer in enumerate(self.layers[:-1]):\n            x = F.elu(layer(x, edge_index))\n            x = F.dropout(x, p=0.2, training=self.training)\n            \n        x = self.layers[-1](x, edge_index)\n        return self.norm(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.553699Z","iopub.execute_input":"2025-04-10T14:48:27.553919Z","iopub.status.idle":"2025-04-10T14:48:27.565530Z","shell.execute_reply.started":"2025-04-10T14:48:27.553903Z","shell.execute_reply":"2025-04-10T14:48:27.564862Z"}},"outputs":[],"execution_count":7},{"id":"8380c1e4","cell_type":"markdown","source":"## Dececption Classifier","metadata":{}},{"id":"c0be40fb","cell_type":"code","source":"class PolicyNetwork(nn.Module):\n    def __init__(self, in_dim, hidden_dim=256):\n        super().__init__()\n        self.in_dim = in_dim\n        self.attention = nn.MultiheadAttention(embed_dim=in_dim, num_heads=4, batch_first=False)\n        self.fc1 = nn.Linear(in_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, 2)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x, mask=None):\n        # Check dimensions and reshape if needed\n        if len(x.shape) == 2:  # If x is [batch_size, features]\n            x = x.unsqueeze(0)  # Make it [1, batch_size, features]\n            \n        # Ensure x is [seq_len, batch_size, features]\n        if x.shape[-1] != self.in_dim:\n            raise ValueError(f\"Expected features dimension {self.in_dim}, got {x.shape[-1]}\")\n            \n        # Self-attention for context (x should be [seq_len, batch, dim])\n        attn_out, _ = self.attention(x, x, x)\n        x = x + attn_out  # Residual connection\n        \n        # Take the first token's representation or mean\n        x = x.mean(dim=0)  # [batch, dim]\n        \n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.566350Z","iopub.execute_input":"2025-04-10T14:48:27.566596Z","iopub.status.idle":"2025-04-10T14:48:27.581759Z","shell.execute_reply.started":"2025-04-10T14:48:27.566576Z","shell.execute_reply":"2025-04-10T14:48:27.581115Z"}},"outputs":[],"execution_count":8},{"id":"d53e9010","cell_type":"markdown","source":"## Generator using BART Decoder based on word context, world knowledge, deception label and power score","metadata":{}},{"id":"d8ebe6a1","cell_type":"code","source":"class ResponseGenerator(nn.Module):\n    def __init__(self, bart_model_name='facebook/bart-base'):\n        super().__init__()\n        self.tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n        self.decoder = BartForConditionalGeneration.from_pretrained(bart_model_name)\n\n    def forward(self, encoder_outputs, truth_labels, power_deltas, concept_feats=None, max_length=50):\n        prompts = []\n        for t, pd in zip(truth_labels, power_deltas):\n            label_str = 'Truth' if t == 1 else 'Lie'\n            prompts.append(f\"[{label_str}|Delta:{pd.item():.2f}]\")\n        toks = self.tokenizer(prompts, return_tensors='pt', padding=True)\n        out = self.decoder.generate(\n            input_ids=toks['input_ids'],\n            attention_mask=toks['attention_mask'],\n            encoder_outputs=(encoder_outputs.unsqueeze(0),),\n            max_length=max_length\n        )\n        return [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in out]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.582399Z","iopub.execute_input":"2025-04-10T14:48:27.582592Z","iopub.status.idle":"2025-04-10T14:48:27.598123Z","shell.execute_reply.started":"2025-04-10T14:48:27.582577Z","shell.execute_reply":"2025-04-10T14:48:27.597561Z"}},"outputs":[],"execution_count":9},{"id":"d24b722b","cell_type":"markdown","source":"## Full Deception Detection Model","metadata":{}},{"id":"6cd65fb5","cell_type":"code","source":"class DeceptionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = MessageEncoder()\n        self.gat = ConversationGAT(in_dim=256)\n        self.policy = PolicyNetwork(in_dim=128)\n        # self.generator = ResponseGenerator()\n\n    def forward(self, messages, edge_index, power_deltas, truth_labels=None, rl_mode=False):\n        # 1) Encode messages with BART encoder\n        msg_feats, attn_mask = self.encoder(messages)\n        # 2) GAT with power deltas\n        node_feats = self.gat(msg_feats, edge_index, power_deltas)\n        # 3) Policy head\n        logits = self.policy(node_feats)\n        probs = F.softmax(logits, dim=-1)\n\n        actions, log_probs = None, None\n        if rl_mode:\n            dist = torch.distributions.Categorical(probs)\n            actions = dist.sample()\n            log_probs = dist.log_prob(actions)\n\n        responses = None\n        # if truth_labels is not None:\n        #     responses = self.generator(node_feats, truth_labels, power_deltas)\n\n        if rl_mode:\n            return probs, actions, log_probs, responses\n        return probs, responses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:48:27.599924Z","iopub.execute_input":"2025-04-10T14:48:27.600169Z","iopub.status.idle":"2025-04-10T14:48:27.616252Z","shell.execute_reply.started":"2025-04-10T14:48:27.600148Z","shell.execute_reply":"2025-04-10T14:48:27.615526Z"}},"outputs":[],"execution_count":10},{"id":"37e99097","cell_type":"markdown","source":"## Training and Evaluation","metadata":{}},{"id":"559d34b7","cell_type":"code","source":"from torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm.auto import tqdm\n\ndef train_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.1):\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n    \n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n    \n    criterion = nn.CrossEntropyLoss()\n    \n    best_val_loss = float('inf')\n    \n    # Create a progress bar for epochs\n    epoch_pbar = tqdm(range(num_epochs), desc=\"Training\", position=0)\n    \n    for epoch in epoch_pbar:\n        # Training phase\n        model.train()\n        train_loss = 0\n        train_correct = 0\n        train_total = 0\n        \n        # Create a progress bar for train batches\n        train_pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", \n                          leave=False, position=1)\n        \n        for batch in train_pbar:\n            messages = batch['messages']\n            labels = batch['labels'].to(device)\n            scores = batch['scores'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            \n            # Create edge indices for the conversation graph\n            edge_index = batch['edge_index'].to(device)\n            \n            optimizer.zero_grad()\n            \n            # Forward pass\n            probs, _ = model(messages, edge_index, scores)\n            \n            # Apply mask to loss computation\n            loss = criterion(probs.view(-1, 2), labels.view(-1))\n            loss = (loss * attention_mask.view(-1)).sum() / attention_mask.sum()\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n            _, predicted = torch.max(probs, 1)\n            train_total += attention_mask.sum().item()\n            train_correct += ((predicted == labels) * attention_mask).sum().item()\n            \n            # Update progress bar with current loss\n            train_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n        \n        train_loss = train_loss / len(train_loader)\n        train_acc = 100 * train_correct / train_total\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0\n        val_correct = 0\n        val_total = 0\n        \n        # Create a progress bar for validation batches\n        val_pbar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\", \n                        leave=False, position=1)\n        \n        with torch.no_grad():\n            for batch in val_pbar:\n                messages = batch['messages']\n                labels = batch['labels'].to(device)\n                scores = batch['scores'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                \n                edge_index = batch['edge_index'].to(device)\n                \n                probs, _ = model(messages, edge_index, scores)\n                \n                loss = criterion(probs.view(-1, 2), labels.view(-1))\n                loss = (loss * attention_mask.view(-1)).sum() / attention_mask.sum()\n                \n                val_loss += loss.item()\n                _, predicted = torch.max(probs, 1)\n                val_total += attention_mask.sum().item()\n                val_correct += ((predicted == labels) * attention_mask).sum().item()\n                \n                # Update progress bar with current loss\n                val_pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n        \n        val_loss = val_loss / len(val_loader)\n        val_acc = 100 * val_correct / val_total\n\n        # In training loop after validation\n        scheduler.step(val_loss)\n        \n        # Update the epoch progress bar\n        epoch_pbar.set_postfix({\n            \"train_loss\": f\"{train_loss:.4f}\",\n            \"train_acc\": f\"{train_acc:.2f}%\",\n            \"val_loss\": f\"{val_loss:.4f}\",\n            \"val_acc\": f\"{val_acc:.2f}%\"\n        })\n        \n        # Save best model\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pth')\n            epoch_pbar.write(f\"Epoch {epoch+1}: Saved new best model with val_loss: {val_loss:.4f}\")\n\ndef create_conversation_edges(num_messages):\n    # Create edges between consecutive messages\n    edges = []\n    for i in range(num_messages - 1):\n        edges.append([i, i + 1])\n    return torch.tensor(edges, dtype=torch.long).t()\n\n# Example usage:\nif __name__ == \"__main__\":\n    # Load datasets\n    train_dataset = ConversationDataset(TRAIN_PATH)\n    val_dataset = ConversationDataset(VAL_PATH)\n    \n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n    val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)\n    \n    # Initialize model\n    model = DeceptionModel()\n    \n    # Train model\n    train_model(model, train_loader, val_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-10T14:49:06.651700Z","iopub.execute_input":"2025-04-10T14:49:06.651980Z","iopub.status.idle":"2025-04-10T14:50:44.869073Z","shell.execute_reply.started":"2025-04-10T14:49:06.651961Z","shell.execute_reply":"2025-04-10T14:50:44.868223Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd66775ff1cb43a29d96557339bcee74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 1: Saved new best model with val_loss: 0.3794\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 5/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 6/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 7/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 8/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 9/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10 [Train]:   0%|          | 0/184 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 10/10 [Val]:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}}],"execution_count":12},{"id":"865a0bee-bf30-477f-8830-0c4556671a04","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}