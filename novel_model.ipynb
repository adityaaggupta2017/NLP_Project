{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49752bef",
   "metadata": {},
   "source": [
    "# Deception Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fab158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tqdm\n",
    "from transformers import BartTokenizer, BartModel, BartForConditionalGeneration\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random as random\n",
    "import json\n",
    "\n",
    "# Set seeds as in the original code base . \n",
    "torch.manual_seed(1994)\n",
    "np.random.seed(1994)\n",
    "random.seed(1994)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ba8274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths \n",
    "TRAIN_PATH = r\".\\Data\\train.jsonl\"\n",
    "VAL_PATH = r\".\\Data\\validation.jsonl\"\n",
    "TEST_PATH = r\".\\Data\\test.jsonl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5824b",
   "metadata": {},
   "source": [
    "## Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8438e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the diplomacy dataset class \n",
    "class DiplomacyDataset(torch.utils.data.Dataset):\n",
    "   \n",
    "    def __init__(self, path, max_tokens_per_msg=50, max_messages=50, use_game_scores=False):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        self.max_tokens_per_msg = max_tokens_per_msg\n",
    "        self.max_messages = max_messages\n",
    "        self.use_game_scores = use_game_scores\n",
    "        \n",
    "        # Read and process each line in the JSONL file.\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                record = json.loads(line)\n",
    "                messages = record.get(\"messages\", [])\n",
    "                labels = record.get(\"sender_labels\", [])\n",
    "                # Only use game_score_delta if use_game_scores is True, otherwise default to None.\n",
    "                game_scores = record.get(\"game_score_delta\", None) if use_game_scores else None\n",
    "                \n",
    "                filtered_msgs, filtered_lbls = [], []\n",
    "                filtered_scores = []\n",
    "                # If game scores are missing, set them to zero for every message.\n",
    "                if game_scores is None:\n",
    "                    game_scores = [0] * len(messages)\n",
    "                \n",
    "                # Iterate through messages, labels, and game scores in parallel.\n",
    "                for m, l, g in zip(messages, labels, game_scores):\n",
    "                    # Accept only valid boolean labels (or their string representations).\n",
    "                    if l in [True, False, \"true\", \"false\", \"True\", \"False\"]:\n",
    "                        filtered_msgs.append(m)\n",
    "                        # Convert string labels to 0/1 (0 for false, 1 for true).\n",
    "                        if isinstance(l, str):\n",
    "                            filtered_lbls.append(1 if l.lower() == \"true\" else 0)\n",
    "                        else:\n",
    "                            filtered_lbls.append(1 if l else 0)\n",
    "                        filtered_scores.append(g)\n",
    "                \n",
    "                # Skip records with no valid messages.\n",
    "                if len(filtered_msgs) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Store the processed conversation as a tuple of (messages, labels, game scores).\n",
    "                self.data.append((filtered_msgs, filtered_lbls, filtered_scores))\n",
    "        \n",
    "        # Build vocabulary from the dataset.\n",
    "        self._build_vocab()\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "     \n",
    "        tokens = text.lower().replace(\"\\n\", \" \").split()\n",
    "        out = []\n",
    "        for t in tokens:\n",
    "            if any(ch.isdigit() for ch in t):\n",
    "                out.append(\"<NUM>\")\n",
    "            else:\n",
    "                out.append(t)\n",
    "        return out\n",
    "\n",
    "    def _build_vocab(self):\n",
    "      \n",
    "        token_freq = Counter()\n",
    "        for conv, _, _ in self.data:\n",
    "            for msg in conv:\n",
    "                tokens = self._tokenize(msg)\n",
    "                token_freq.update(tokens)\n",
    "        \n",
    "        # Initialize vocabulary with special tokens.\n",
    "        self.ix2tok = [PAD_TOKEN, UNK_TOKEN]\n",
    "        # Add tokens in order of decreasing frequency.\n",
    "        for tok, freq in token_freq.most_common():\n",
    "            self.ix2tok.append(tok)\n",
    "        # Create the token-to-index mapping.\n",
    "        self.tok2ix = {t: i for i, t in enumerate(self.ix2tok)}\n",
    "\n",
    "    def __len__(self):\n",
    "       \n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        conv, lbls, scores = self.data[idx]\n",
    "        tokenized_conv = []\n",
    "        for msg in conv:\n",
    "            toks = self._tokenize(msg)\n",
    "            # Convert tokens to indices using the vocabulary, defaulting to UNK_TOKEN if not found.\n",
    "            tok_ix = [self.tok2ix.get(t, self.tok2ix[UNK_TOKEN]) for t in toks]\n",
    "            tokenized_conv.append(tok_ix)\n",
    "        # Convert each game score to a float.\n",
    "        scores = [float(s) for s in scores]\n",
    "        return tokenized_conv, lbls, scores\n",
    "\n",
    "\n",
    "# Custom collate function to pad sequences for a batch of conversations.\n",
    "\n",
    "def collate_fn(batch):\n",
    "   \n",
    "    # Determine maximum number of messages in any conversation in the batch.\n",
    "    max_msg_count = max(len(item[0]) for item in batch)\n",
    "    # Determine maximum number of tokens in any message.\n",
    "    max_token_count = 0\n",
    "    for item in batch:\n",
    "        for msg in item[0]:\n",
    "            max_token_count = max(max_token_count, len(msg))\n",
    "    \n",
    "    padded_tokens = []\n",
    "    padded_labels = []\n",
    "    mask = []\n",
    "    padded_scores = []\n",
    "    \n",
    "    # For each conversation in the batch...\n",
    "    for conv, lbls, scores in batch:\n",
    "        num_msgs = len(conv)\n",
    "        conv_tokens = []\n",
    "        conv_labels = []\n",
    "        conv_mask = []\n",
    "        conv_scores = []\n",
    "        # Pad or truncate each conversation to max_msg_count messages.\n",
    "        for i in range(max_msg_count):\n",
    "            if i < num_msgs:\n",
    "                # Pad the message to max_token_count tokens.\n",
    "                msg = conv[i] + [0]*(max_token_count - len(conv[i]))\n",
    "                conv_tokens.append(msg)\n",
    "                conv_labels.append(lbls[i])\n",
    "                conv_mask.append(1)  # Mark this message as valid.\n",
    "                conv_scores.append(scores[i])\n",
    "            else:\n",
    "                # Pad missing messages with zeros.\n",
    "                conv_tokens.append([0]*max_token_count)\n",
    "                conv_labels.append(0)\n",
    "                conv_mask.append(0)\n",
    "                conv_scores.append(0)\n",
    "        padded_tokens.append(conv_tokens)\n",
    "        padded_labels.append(conv_labels)\n",
    "        mask.append(conv_mask)\n",
    "        padded_scores.append(conv_scores)\n",
    "    \n",
    "    padded_tokens = torch.tensor(padded_tokens, dtype=torch.long)\n",
    "    padded_labels = torch.tensor(padded_labels, dtype=torch.long)\n",
    "    mask = torch.tensor(mask, dtype=torch.long)\n",
    "    padded_scores = torch.tensor(padded_scores, dtype=torch.float)\n",
    "    return padded_tokens, padded_labels, mask, padded_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cef60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_graph_data(diplomacy_dataset, world_knowledge_func):\n",
    "    \"\"\"\n",
    "    Convert DiplomacyDataset (conversation-wise) to a format usable by ConversationDataset.\n",
    "\n",
    "    Returns a list of ConversationDataset instances â€” one per conversation.\n",
    "    \"\"\"\n",
    "    conversation_graphs = []\n",
    "\n",
    "    for conv_msgs, labels, scores in diplomacy_dataset:\n",
    "        num_msgs = len(conv_msgs)\n",
    "\n",
    "        # Create edges: simple chain (msg[i] -> msg[i+1])\n",
    "        edges = [(i, i + 1) for i in range(num_msgs - 1)]\n",
    "\n",
    "        # Build ConversationDataset instance for this one conversation\n",
    "        conv_data = ConversationDataset(\n",
    "            conversations=conv_msgs,\n",
    "            power_deltas=scores,\n",
    "            edges=edges,\n",
    "            truth_labels=labels,\n",
    "            world_knowledge_func=world_knowledge_func,\n",
    "        )\n",
    "        conversation_graphs.append(conv_data)\n",
    "\n",
    "    return conversation_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048325dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationDataset(Dataset):\n",
    "    def __init__(self, conversations, power_deltas, edges, truth_labels, world_knowledge_func, max_length=128):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            conversations (List[str]): Messages\n",
    "            power_deltas (List[float]): Delta power values for each message\n",
    "            edges (List[Tuple[int, int]]): List of (src, dst) edges between messages\n",
    "            truth_labels (List[int]): 1 for truth, 0 for lie\n",
    "            world_knowledge_func (Callable[[str], np.ndarray]): Function that maps message to external knowledge embedding\n",
    "        \"\"\"\n",
    "        self.conversations = conversations\n",
    "        self.power_deltas = torch.tensor(power_deltas, dtype=torch.float)\n",
    "        self.truth_labels = torch.tensor(truth_labels, dtype=torch.long)\n",
    "        self.edges = torch.tensor(edges, dtype=torch.long).T  # shape: (2, num_edges)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "        self.world_knowledge_func = world_knowledge_func\n",
    "\n",
    "        # Precompute world knowledge features\n",
    "        # self.world_feats = self._get_world_feats()\n",
    "\n",
    "    def _get_world_feats(self):\n",
    "        feats = []\n",
    "        for msg in self.conversations:\n",
    "            feat = self.world_knowledge_func(msg)  # Expected shape: (world_dim,)\n",
    "            feats.append(torch.tensor(feat, dtype=torch.float))\n",
    "        return torch.stack(feats)\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1  # Since this dataset returns a single graph (whole conversation)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'messages': self.conversations,\n",
    "            'edge_index': self.edges,\n",
    "            'power_deltas': self.power_deltas,\n",
    "            'truth_labels': self.truth_labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff77b22f",
   "metadata": {},
   "source": [
    "## Message Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a269d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BartMessageEncoder(nn.Module):\n",
    "    def __init__(self, model_name='facebook/bart-base'):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "        self.bart = BartModel.from_pretrained(model_name)\n",
    "        self.hidden_dim = self.bart.config.d_model  # 768 for bart-base\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, texts):\n",
    "        toks = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        outputs = self.bart(**toks)\n",
    "        last_hidden = outputs.last_hidden_state  # (B, L, D)\n",
    "        pooled = self.pool(last_hidden.permute(0, 2, 1)).squeeze(-1)  # (B, D)\n",
    "        return pooled, toks['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31aca7",
   "metadata": {},
   "source": [
    "## Conversation Graph with World Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dc3cad",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1863\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\integrations\\integration_utils.py:36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1865\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load model once\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\__init__.py:14\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     export_dynamic_quantized_onnx_model,\n\u001b[0;32m     11\u001b[0m     export_optimized_onnx_model,\n\u001b[0;32m     12\u001b[0m     export_static_quantized_openvino_model,\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     15\u001b[0m     CrossEncoder,\n\u001b[0;32m     16\u001b[0m     CrossEncoderModelCardData,\n\u001b[0;32m     17\u001b[0m     CrossEncoderTrainer,\n\u001b[0;32m     18\u001b[0m     CrossEncoderTrainingArguments,\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainer\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:26\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfit_mixin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitMixin\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderModelCardData, generate_model_card\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     cross_encoder_init_args_decorator,\n\u001b[0;32m     30\u001b[0m     cross_encoder_predict_rank_args_decorator,\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\fit_mixin.py:19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenization_utils_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BatchEncoding\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoderTrainingArguments\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceEvaluator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceEvaluator\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\__init__.py:13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mDenoisingAutoEncoderDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DenoisingAutoEncoderDataset\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mNoDuplicatesDataLoader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NoDuplicatesDataLoader\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mParallelSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceLabelDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceLabelDataset\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentencesDataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentencesDataset\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\datasets\\ParallelSentencesDataset.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputExample\n\u001b[0;32m     22\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_torch_npu_available\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdynamic_module_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_class_from_dynamic_module, get_relative_import_files\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_card\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformerModelCardData, generate_model_card\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityFunction\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __MODEL_HUB_ORGANIZATION__, __version__\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\sentence_transformers\\model_card.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautonotebook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerCallback\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CodeCarbonCallback\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelcard\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m make_markdown_table\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer_callback\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TrainerControl, TrainerState\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1851\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1849\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[0;32m   1850\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1851\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1852\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\transformers\\utils\\import_utils.py:1865\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1864\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1865\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1866\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1867\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1868\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "class ConversationGAT(nn.Module):\n",
    "    def __init__(self, in_dim, gat_dim=128, heads=4, world_dim=128):\n",
    "        super().__init__()\n",
    "        self.world_dim = world_dim\n",
    "        total_input_dim = in_dim + 1 + world_dim  # msg + power + world\n",
    "        self.gat1 = GATConv(total_input_dim, gat_dim, heads=heads, dropout=0.1)\n",
    "        self.gat2 = GATConv(gat_dim * heads, gat_dim, heads=1, concat=False, dropout=0.1)\n",
    "\n",
    "    def forward(self, node_feats, edge_index, power_deltas, world_feats, num_random_edges=None):\n",
    "        pd = power_deltas.unsqueeze(-1)  # (N, 1)\n",
    "        x = torch.cat([node_feats, pd, world_feats], dim=-1)  # (N, D + 1 + W)\n",
    "\n",
    "        N = node_feats.size(0)\n",
    "        k = num_random_edges if num_random_edges is not None else N\n",
    "        src = torch.randint(0, N, (k,), device=node_feats.device)\n",
    "        dst = torch.randint(0, N, (k,), device=node_feats.device)\n",
    "        random_edge_index = torch.stack([src, dst], dim=0)\n",
    "        combined_edge_index = torch.cat([edge_index, random_edge_index], dim=1)\n",
    "\n",
    "        x = F.elu(self.gat1(x, combined_edge_index))\n",
    "        x = F.elu(self.gat2(x, combined_edge_index))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8380c1e4",
   "metadata": {},
   "source": [
    "## Dececption Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0be40fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e9010",
   "metadata": {},
   "source": [
    "## Generator using BART Decoder based on word context, world knowledge, deception label and power score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ebe6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseGenerator(nn.Module):\n",
    "    def __init__(self, bart_model_name='facebook/bart-base'):\n",
    "        super().__init__()\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
    "        self.decoder = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
    "\n",
    "    def forward(self, encoder_outputs, truth_labels, power_deltas, concept_feats=None, max_length=50):\n",
    "        prompts = []\n",
    "        for t, pd in zip(truth_labels, power_deltas):\n",
    "            label_str = 'Truth' if t == 1 else 'Lie'\n",
    "            prompts.append(f\"[{label_str}|Delta:{pd.item():.2f}]\")\n",
    "        toks = self.tokenizer(prompts, return_tensors='pt', padding=True)\n",
    "        out = self.decoder.generate(\n",
    "            input_ids=toks['input_ids'],\n",
    "            attention_mask=toks['attention_mask'],\n",
    "            encoder_outputs=(encoder_outputs.unsqueeze(0),),\n",
    "            max_length=max_length\n",
    "        )\n",
    "        return [self.tokenizer.decode(ids, skip_special_tokens=True) for ids in out]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24b722b",
   "metadata": {},
   "source": [
    "## Full Deception Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd65fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeceptionModel(nn.Module):\n",
    "    def __init__(self, world_dim=128):\n",
    "        super().__init__()\n",
    "        self.encoder = BartMessageEncoder()\n",
    "        self.gat = ConversationGAT(in_dim=self.encoder.hidden_dim, world_dim=world_dim)\n",
    "        self.policy = PolicyNetwork(in_dim=128)\n",
    "        self.generator = ResponseGenerator()\n",
    "\n",
    "    def forward(self, messages, edge_index, power_deltas, world_feats, truth_labels=None, rl_mode=False):\n",
    "        # 1) Encode messages with BART encoder\n",
    "        msg_feats, attn_mask = self.encoder(messages)\n",
    "        # 2) GAT with added world knowledge\n",
    "        node_feats = self.gat(msg_feats, edge_index, power_deltas, world_feats)\n",
    "        # 3) Policy head\n",
    "        logits = self.policy(node_feats)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        actions, log_probs = None, None\n",
    "        if rl_mode:\n",
    "            dist = torch.distributions.Categorical(probs)\n",
    "            actions = dist.sample()\n",
    "            log_probs = dist.log_prob(actions)\n",
    "\n",
    "        responses = None\n",
    "        if truth_labels is not None:\n",
    "            responses = self.generator(node_feats, truth_labels, power_deltas)\n",
    "\n",
    "        if rl_mode:\n",
    "            return probs, actions, log_probs, responses\n",
    "        return probs, responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e99097",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming everything is already defined:\n",
    "# - ConversationDataset\n",
    "# - DeceptionModel\n",
    "# - sbert_world_knowledge (or other function)\n",
    "# - Data: messages, power_deltas, edges, truth_labels\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 128\n",
    "gat_heads = 4\n",
    "world_dim = 384  # SBERT\n",
    "learning_rate = 1e-4\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "\n",
    "# Dataset and model\n",
    "dataset = ConversationDataset(\n",
    "    conversations=messages,\n",
    "    power_deltas=power_deltas,\n",
    "    edges=edges,\n",
    "    truth_labels=truth_labels,\n",
    "    world_knowledge_func=sbert_world_knowledge\n",
    ")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "model = DeceptionModel(hidden_dim=hidden_dim, gat_heads=gat_heads, world_dim=world_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        messages = batch['messages'][0]\n",
    "        edge_index = batch['edge_index'].to(device)\n",
    "        power_deltas = batch['power_deltas'].to(device)\n",
    "        truth_labels = batch['truth_labels'].float().to(device)\n",
    "        world_feats = batch['world_feats'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(messages, edge_index, power_deltas, world_feats, truth_labels)\n",
    "        loss = criterion(outputs, truth_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
