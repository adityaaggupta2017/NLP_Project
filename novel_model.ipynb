{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11354484,"sourceType":"datasetVersion","datasetId":7105583},{"sourceId":11375484,"sourceType":"datasetVersion","datasetId":7121728},{"sourceId":334779,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":280282,"modelId":301190}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"a9c19470-1dee-4de7-9149-61aa68206de6","cell_type":"markdown","source":"# Improved Deception Detection Model\n\nThis notebook implements an end‐to‐end revised version of the deception detection code. In this version, we incorporate several improvements inspired by the research (e.g. Peskov et al. 2020) and related work on deception detection. In particular, we:\n\n- Introduce a new context encoder module that uses a bidirectional LSTM to summarize token‐level representations of the conversation context.\n- Fuse four streams of information: the current message representation (from RoBERTa), the contextual summary from the LSTM, graph features derived from conversation (via two graph attention layers), and game score features (as a proxy for power dynamics).\n- Use an ensemble of classifier heads (with learnable weights) to stabilize predictions.\n\nDue to limited data size, early transformer layers are frozen and extra regularization is applied.\n","metadata":{}},{"id":"ca79d70f-8202-4d45-bb1f-91448b0bbedb","cell_type":"code","source":"import json\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom collections import Counter, defaultdict\nfrom sklearn.metrics import f1_score, confusion_matrix, classification_report\nimport pandas as pd\nfrom transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, RobertaModel\n\n# Set seeds for reproducibility\nSEED = 42\ntorch.manual_seed(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(SEED)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {DEVICE}\")","metadata":{"execution":{"iopub.status.busy":"2025-04-14T09:24:18.019036Z","iopub.execute_input":"2025-04-14T09:24:18.019271Z","iopub.status.idle":"2025-04-14T09:24:41.622412Z","shell.execute_reply.started":"2025-04-14T09:24:18.019241Z","shell.execute_reply":"2025-04-14T09:24:41.621826Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2025-04-14 09:24:31.474407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1744622671.654876      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1744622671.707093      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"id":"6d6083df-7696-4b3e-962c-feb9f15b882c","cell_type":"markdown","source":"## Data Paths and Model Parameters\n\nSet the paths for the training, validation, and test datasets and define the hyperparameters.","metadata":{}},{"id":"31a44ef2-c136-4583-b24f-65ddfbffa774","cell_type":"code","source":"# Data paths\nTRAIN_PATH = r\"/kaggle/input/deception-data/data/train.jsonl\"\nVAL_PATH = r\"/kaggle/input/deception-data/data/validation.jsonl\"\nTEST_PATH = r\"/kaggle/input/deception-data/data/test.jsonl\"\n\n# Model and training parameters\nTRANSFORMER_MODEL = \"roberta-base\"  # Using RoBERTa base\nBATCH_SIZE = 32\nEPOCHS = 5\nLR = 5e-6\nUSE_GAME_SCORES = True\nOVERSAMPLING_FACTOR = 30  # Oversampling factor (tune if necessary)\nTRUTH_FOCAL_WEIGHT = 4.0  # Additional weight for truth class during loss calculation\nEARLY_STOPPING_PATIENCE = 5\nGRADIENT_ACCUMULATION_STEPS = 2","metadata":{"execution":{"iopub.status.busy":"2025-04-14T09:24:41.623845Z","iopub.execute_input":"2025-04-14T09:24:41.624285Z","iopub.status.idle":"2025-04-14T09:24:41.628328Z","shell.execute_reply.started":"2025-04-14T09:24:41.624267Z","shell.execute_reply":"2025-04-14T09:24:41.627715Z"},"trusted":true},"outputs":[],"execution_count":2},{"id":"0e079dcf-b7e4-4e16-977b-41938f902634","cell_type":"markdown","source":"## Data Loading and Preprocessing\n\nWe define a custom collate function and a dataset class that loads the JSONL data. For each conversation, we extract the message, its label, game score, and prior context (up to two previous messages).","metadata":{}},{"id":"6ab59264-6a57-4a91-aa0b-f67f732a7c00","cell_type":"code","source":"def custom_collate_fn(batch):\n    # Collate all keys except 'relative_positions'\n    batch_without_relative = [{k: v for k, v in item.items() if k != 'relative_positions'} for item in batch]\n    collated = torch.utils.data.dataloader.default_collate(batch_without_relative)\n    collated['relative_positions'] = [item['relative_positions'] for item in batch]\n    return collated\n\n\nclass EnhancedDeceptionDataset(Dataset):\n    def __init__(self, path, tokenizer, max_len=128, use_game_scores=True):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.use_game_scores = use_game_scores\n        self.texts = []\n        self.labels = []\n        self.scores = []\n        self.conversation_ids = []  # Track conversation id\n        self.message_positions = []  # Message order in conversation\n        self.prior_context = []  # Prior context (concatenated previous messages)\n        \n        with open(path, 'r', encoding='utf-8') as f:\n            conv_id = 0\n            for line in f:\n                data = json.loads(line.strip())\n                messages = data.get('messages', [])\n                labels = data.get('sender_labels', [])\n                game_scores = data.get('game_score_delta', None) if use_game_scores else None\n                if game_scores is None:\n                    game_scores = [0] * len(messages)\n                \n                for pos, (msg, label, score) in enumerate(zip(messages, labels, game_scores)):\n                    if label in [True, False, \"true\", \"false\", \"True\", \"False\"]:\n                        # Convert string labels to boolean if needed\n                        if isinstance(label, str):\n                            is_lie = label.lower() == \"true\"\n                        else:\n                            is_lie = label\n                        # Convention: 1 = lie, 0 = truth\n                        self.texts.append(msg)\n                        self.labels.append(1 if is_lie else 0)\n                        self.scores.append(float(score))\n                        self.conversation_ids.append(conv_id)\n                        self.message_positions.append(pos)\n                        # Build prior context: use up to two previous messages\n                        context = \"\"\n                        if pos > 0:\n                            context_msgs = messages[max(0, pos-2):pos]\n                            context = \" [SEP] \".join(context_msgs)\n                        self.prior_context.append(context)\n                conv_id += 1\n        \n        self.class_counts = Counter(self.labels)\n        total = len(self.labels)\n        self.truth_indices = [i for i, label in enumerate(self.labels) if label == 0]\n        self.lie_indices = [i for i, label in enumerate(self.labels) if label == 1]\n        \n        # Group messages by conversation for potential graph construction\n        self.conv_to_msgs = defaultdict(list)\n        for i, cid in enumerate(self.conversation_ids):\n            self.conv_to_msgs[cid].append(i)\n        \n        print(f\"Dataset loaded from {path}\")\n        print(f\"Total messages: {total}\")\n        for label, count in sorted(self.class_counts.items()):\n            label_name = \"Truth\" if label == 0 else \"Lie\"\n            print(f\"{label_name}: {count} ({count/total*100:.2f}%)\")\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        score = self.scores[idx]\n        conv_id = self.conversation_ids[idx]\n        position = self.message_positions[idx]\n        context = self.prior_context[idx]\n        \n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt'\n        )\n        \n        if context:\n            context_encoding = self.tokenizer(\n                context,\n                max_length=self.max_len,\n                padding='max_length',\n                truncation=True,\n                return_tensors='pt'\n            )\n        else:\n            context_encoding = {\n                'input_ids': torch.zeros((1, self.max_len), dtype=torch.long),\n                'attention_mask': torch.zeros((1, self.max_len), dtype=torch.long)\n            }\n        \n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'context_input_ids': context_encoding['input_ids'].flatten(),\n            'context_attention_mask': context_encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long),\n            'score': torch.tensor(score, dtype=torch.float),\n            'conv_id': conv_id,\n            'position': position,\n            'relative_positions': []  # Reserved for legacy\n        }","metadata":{"execution":{"iopub.status.busy":"2025-04-14T09:24:41.629140Z","iopub.execute_input":"2025-04-14T09:24:41.629410Z","iopub.status.idle":"2025-04-14T09:24:41.974736Z","shell.execute_reply.started":"2025-04-14T09:24:41.629385Z","shell.execute_reply":"2025-04-14T09:24:41.974195Z"},"trusted":true},"outputs":[],"execution_count":3},{"id":"73263c5b-c12e-4782-9e10-eb39e83c5619","cell_type":"markdown","source":"## Enhanced Balanced Sampler\n\nThis sampler oversamples the truth class to counteract class imbalance.","metadata":{}},{"id":"e0939318-ad48-4d9d-b220-f6e6f385cecd","cell_type":"code","source":"class EnhancedBalancedSampler(torch.utils.data.Sampler):\n    def __init__(self, dataset, oversample_factor=15):\n        self.dataset = dataset\n        self.oversample_factor = oversample_factor\n        self.truth_indices = dataset.truth_indices * oversample_factor  # Oversample truths\n        self.lie_indices = dataset.lie_indices\n        \n        min_samples = max(1000, len(self.truth_indices))\n        target_size = min(len(self.truth_indices), len(self.lie_indices))\n        target_size = max(target_size, min_samples)\n        \n        if len(self.truth_indices) < target_size:\n            self.truth_indices = self.truth_indices * (target_size // len(self.truth_indices) + 1)\n        if len(self.lie_indices) < target_size:\n            self.lie_indices = self.lie_indices * (target_size // len(self.lie_indices) + 1)\n        \n        self.truth_indices = random.sample(self.truth_indices, target_size)\n        self.lie_indices = random.sample(self.lie_indices, target_size)\n        \n        self.indices = self.truth_indices + self.lie_indices\n        random.shuffle(self.indices)\n    \n    def __iter__(self):\n        return iter(self.indices)\n    \n    def __len__(self):\n        return len(self.indices)","metadata":{"execution":{"iopub.status.busy":"2025-04-14T09:24:41.975480Z","iopub.execute_input":"2025-04-14T09:24:41.975763Z","iopub.status.idle":"2025-04-14T09:24:41.993373Z","shell.execute_reply.started":"2025-04-14T09:24:41.975739Z","shell.execute_reply":"2025-04-14T09:24:41.992495Z"},"trusted":true},"outputs":[],"execution_count":4},{"id":"18ce1fbe-e325-4946-baee-4505d2d6168b","cell_type":"markdown","source":"## Model Components\n\nWe define the following model components:\n\n1. **Simple Attention Module** (for potential auxiliary use).\n2. **Graph Attention Layer** (to compute relational features from conversation graphs).\n3. **Context Encoder**: a bidirectional LSTM that summarizes token-level representations of the conversation context. *(Note: This module now clamps any zero-length sequences to have length 1.)*\n4. **ImprovedDeceptionModel**: integrates the current message, context summary, graph features, and game score features; uses an ensemble of classifier heads.\n","metadata":{}},{"id":"982076c5-0d2a-4826-bff3-4da66f6bc4e2","cell_type":"code","source":"class SimpleAttention(nn.Module):\n    def __init__(self, hidden_size, dropout=0.1):\n        super(SimpleAttention, self).__init__()\n        self.hidden_size = hidden_size\n        self.query_proj = nn.Linear(hidden_size, hidden_size)\n        self.key_proj = nn.Linear(hidden_size, hidden_size)\n        self.value_proj = nn.Linear(hidden_size, hidden_size)\n        self.out_proj = nn.Linear(hidden_size, hidden_size)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, query, key, value, mask=None):\n        query = self.query_proj(query)\n        key = self.key_proj(key)\n        value = self.value_proj(value)\n        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.hidden_size ** 0.5)\n        if mask is not None:\n            scores = scores.masked_fill(mask == 0, -1e9)\n        attn_weights = F.softmax(scores, dim=-1)\n        attn_weights = self.dropout(attn_weights)\n        context = torch.matmul(attn_weights, value)\n        output = self.out_proj(context)\n        return output\n\n\nclass GraphAttentionLayer(nn.Module):\n    def __init__(self, in_features, out_features, dropout=0.1, alpha=0.2):\n        super(GraphAttentionLayer, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.dropout = dropout\n        self.alpha = alpha\n        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n        self.leakyrelu = nn.LeakyReLU(self.alpha)\n        \n    def forward(self, features, adj_matrix):\n        h = torch.mm(features, self.W)  # (N, out_features)\n        N = h.size()[0]\n        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)\n        a_input = a_input.view(N, N, 2 * self.out_features)\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n        zero_vec = -9e15 * torch.ones_like(e)\n        attention = torch.where(adj_matrix > 0, e, zero_vec)\n        attention = F.softmax(attention, dim=1)\n        attention = F.dropout(attention, self.dropout, training=self.training)\n        h_prime = torch.matmul(attention, h)\n        return h_prime\n\n\nclass ContextEncoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=True, dropout=0.1):\n        super(ContextEncoder, self).__init__()\n        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, \n                            bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)\n        self.output_dim = hidden_dim * (2 if bidirectional else 1)\n\n    def forward(self, token_embeddings, attention_mask):\n        # Compute lengths from attention mask, clamp to minimum of 1 to avoid zero-length sequences\n        lengths = torch.clamp(attention_mask.sum(dim=1), min=1).cpu()\n        packed = nn.utils.rnn.pack_padded_sequence(token_embeddings, lengths, batch_first=True, enforce_sorted=False)\n        packed_outputs, (h_n, _) = self.lstm(packed)\n        num_directions = 2 if self.lstm.bidirectional else 1\n        h_n = h_n.view(self.lstm.num_layers, num_directions, token_embeddings.size(0), self.lstm.hidden_size)\n        h_final = torch.cat((h_n[-1, 0, :, :], h_n[-1, 1, :, :]), dim=1)\n        return h_final\n\n\nclass ImprovedDeceptionModel(nn.Module):\n    def __init__(self, model_name, use_game_scores=True):\n        super(ImprovedDeceptionModel, self).__init__()\n        # Pretrained transformers for current message and context\n        self.transformer = RobertaModel.from_pretrained(model_name)\n        self.context_transformer = RobertaModel.from_pretrained(model_name)\n        \n        # Freeze first 2 encoder layers for both transformers\n        for layer in self.transformer.encoder.layer[:2]:\n            for param in layer.parameters():\n                param.requires_grad = False\n        for layer in self.context_transformer.encoder.layer[:2]:\n            for param in layer.parameters():\n                param.requires_grad = False\n        \n        self.hidden_size = self.transformer.config.hidden_size\n        self.use_game_scores = use_game_scores\n        \n        self.dropout = nn.Dropout(0.3)\n        self.context_encoder = ContextEncoder(input_dim=self.hidden_size, hidden_dim=self.hidden_size//2, \n                                               num_layers=1, bidirectional=True, dropout=0.1)\n        \n        # Graph attention modules (two-layer GAT)\n        self.gat1 = GraphAttentionLayer(self.hidden_size + self.context_encoder.output_dim, 256)\n        self.gat2 = GraphAttentionLayer(256, 256)\n        \n        # Game score processing branch\n        if use_game_scores:\n            self.score_proj = nn.Sequential(\n                nn.Linear(1, 32),\n                nn.LayerNorm(32),\n                nn.ReLU(),\n                nn.Dropout(0.2)\n            )\n            fusion_dim = self.hidden_size + self.context_encoder.output_dim + 256 + 32\n        else:\n            fusion_dim = self.hidden_size + self.context_encoder.output_dim + 256\n        \n        self.feature_norm = nn.LayerNorm(fusion_dim)\n        \n        # Ensemble of classifier heads\n        self.classifier1 = nn.Linear(fusion_dim, 2)  # from fused features\n        self.classifier2 = nn.Linear(self.hidden_size + self.context_encoder.output_dim, 2)  # current+context\n        self.classifier3 = nn.Linear(256, 2)  # from graph features\n        self.ensemble_weights = nn.Parameter(torch.ones(3))\n    \n    def forward(self, input_ids, attention_mask, context_input_ids=None, \n                context_attention_mask=None, game_scores=None, batch_adj_matrix=None):\n        # Current message representation\n        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n        text_features = self.dropout(pooled_output)\n        \n        # Context processing: encode context tokens and summarize via LSTM\n        if context_input_ids is not None and torch.sum(context_input_ids) > 0:\n            ctx_outputs = self.context_transformer(input_ids=context_input_ids, attention_mask=context_attention_mask)\n            ctx_token_embeddings = ctx_outputs.last_hidden_state  \n            context_summary = self.context_encoder(ctx_token_embeddings, context_attention_mask)\n        else:\n            batch_size = text_features.size(0)\n            context_summary = torch.zeros(batch_size, self.context_encoder.output_dim, device=text_features.device)\n        \n        # Combine current message and context summary\n        combined_features = torch.cat([text_features, context_summary], dim=1)\n        \n        # Graph-based relational features\n        if batch_adj_matrix is not None:\n            graph_features = self.gat1(combined_features, batch_adj_matrix)\n            graph_features = F.elu(graph_features)\n            graph_features = self.gat2(graph_features, batch_adj_matrix)\n        else:\n            graph_features = torch.zeros(combined_features.size(0), 256, device=combined_features.device)\n        \n        # Fuse features: current+context, graph, and optionally game scores\n        if self.use_game_scores and game_scores is not None:\n            score_features = self.score_proj(game_scores.unsqueeze(1))\n            all_features = torch.cat([combined_features, graph_features, score_features], dim=1)\n        else:\n            all_features = torch.cat([combined_features, graph_features], dim=1)\n        \n        all_features = self.feature_norm(all_features)\n        logits1 = self.classifier1(all_features)\n        logits2 = self.classifier2(combined_features)\n        logits3 = self.classifier3(graph_features)\n        ensemble_weights = F.softmax(self.ensemble_weights, dim=0)\n        final_logits = (ensemble_weights[0] * logits1 +\n                        ensemble_weights[1] * logits2 +\n                        ensemble_weights[2] * logits3)\n        return final_logits\n","metadata":{"execution":{"iopub.status.busy":"2025-04-14T09:24:41.994164Z","iopub.execute_input":"2025-04-14T09:24:41.994386Z","iopub.status.idle":"2025-04-14T09:24:42.014507Z","shell.execute_reply.started":"2025-04-14T09:24:41.994368Z","shell.execute_reply":"2025-04-14T09:24:42.013931Z"},"trusted":true},"outputs":[],"execution_count":5},{"id":"71ca6dc5-fddb-4f53-be51-ed873d66e2fd","cell_type":"markdown","source":"## Loss, Batch Preparation, and Training/Evaluation Functions\n\nWe define the focal weighted loss, a batch preparation function (which builds a simple adjacency matrix for conversation graphs), and the training/evaluation loops.","metadata":{}},{"id":"4e75914b-ebd4-4889-84b6-5a04e060c4a0","cell_type":"code","source":"class FocalWeightedLoss(nn.Module):\n    def __init__(self, class_weights, truth_focal_weight=4.0):\n        super(FocalWeightedLoss, self).__init__()\n        self.class_weights = class_weights\n        self.truth_focal_weight = truth_focal_weight\n    \n    def forward(self, logits, targets):\n        ce_loss = F.cross_entropy(logits, targets, weight=self.class_weights, reduction='none')\n        probs = F.softmax(logits, dim=1)\n        truth_probs = probs[:, 0]  # probability for truth class\n        truth_mask = (targets == 0).float()\n        focal_weight = (1 - truth_probs) ** self.truth_focal_weight\n        focal_loss = truth_mask * focal_weight * ce_loss + (1 - truth_mask) * ce_loss\n        return focal_loss.mean()\n\n\ndef prepare_batch_for_model(batch, device):\n    input_ids = batch['input_ids'].to(device)\n    attention_mask = batch['attention_mask'].to(device)\n    context_input_ids = batch['context_input_ids'].to(device) if 'context_input_ids' in batch else None\n    context_attention_mask = batch['context_attention_mask'].to(device) if 'context_attention_mask' in batch else None\n    labels = batch['label'].to(device)\n    scores = batch['score'].to(device) if 'score' in batch else None\n    batch_size = input_ids.size(0)\n    \n    # Build a simple adjacency matrix for messages in the same conversation\n    batch_adj_matrix = torch.zeros((batch_size, batch_size), device=device)\n    conv_ids = batch['conv_id'] if isinstance(batch['conv_id'], list) else batch['conv_id'].tolist()\n    positions = batch['position'] if isinstance(batch['position'], list) else batch['position'].tolist()\n    for i in range(batch_size):\n        for j in range(batch_size):\n            if conv_ids[i] == conv_ids[j]:\n                if i == j:\n                    batch_adj_matrix[i, j] = 1.0\n                else:\n                    distance = abs(positions[i] - positions[j])\n                    batch_adj_matrix[i, j] = 1.0 / (distance + 1)\n    \n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'context_input_ids': context_input_ids,\n        'context_attention_mask': context_attention_mask,\n        'labels': labels,\n        'scores': scores,\n        'batch_adj_matrix': batch_adj_matrix\n    }\n\ndef train(model, dataloader, optimizer, scheduler, device, class_weights, truth_focal_weight=4.0, gradient_accumulation_steps=1):\n    model.train()\n    total_loss = 0\n    all_labels = []\n    all_preds = []\n    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n    optimizer.zero_grad()\n    accumulated_steps = 0\n    \n    for batch in tqdm(dataloader, desc=\"Training\"):\n        batch_data = prepare_batch_for_model(batch, device)\n        outputs = model(\n            input_ids=batch_data['input_ids'], \n            attention_mask=batch_data['attention_mask'],\n            context_input_ids=batch_data['context_input_ids'],\n            context_attention_mask=batch_data['context_attention_mask'],\n            game_scores=batch_data['scores'],\n            batch_adj_matrix=batch_data['batch_adj_matrix']\n        )\n        loss = loss_fn(outputs, batch_data['labels'])\n        loss = loss / gradient_accumulation_steps\n        loss.backward()\n        total_loss += loss.item() * gradient_accumulation_steps\n        _, preds = torch.max(outputs, dim=1)\n        all_labels.extend(batch_data['labels'].cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n        accumulated_steps += 1\n        if accumulated_steps % gradient_accumulation_steps == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            optimizer.zero_grad()\n    \n    if accumulated_steps % gradient_accumulation_steps != 0:\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        if scheduler:\n            scheduler.step()\n    \n    avg_loss = total_loss / len(dataloader)\n    try:\n        truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n        lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    except Exception:\n        truth_f1 = lie_f1 = macro_f1 = 0\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n\ndef evaluate(model, dataloader, device, class_weights, truth_focal_weight=4.0):\n    model.eval()\n    total_loss = 0\n    all_labels = []\n    all_preds = []\n    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            batch_data = prepare_batch_for_model(batch, device)\n            outputs = model(\n                input_ids=batch_data['input_ids'], \n                attention_mask=batch_data['attention_mask'],\n                context_input_ids=batch_data['context_input_ids'],\n                context_attention_mask=batch_data['context_attention_mask'],\n                game_scores=batch_data['scores'],\n                batch_adj_matrix=batch_data['batch_adj_matrix']\n            )\n            loss = loss_fn(outputs, batch_data['labels'])\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, dim=1)\n            all_labels.extend(batch_data['labels'].cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    try:\n        truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n        lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n        print(classification_report(all_labels, all_preds, target_names=['Truth', 'Lie'], digits=4, zero_division=0))\n    except Exception:\n        truth_f1 = lie_f1 = macro_f1 = 0\n    \n    cm = confusion_matrix(all_labels, all_preds)\n    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n\ndef plot_confusion_matrix(cm, epoch, split='val'):\n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n                xticklabels=['Truth', 'Lie'], yticklabels=['Truth', 'Lie'])\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.title(f'Confusion Matrix - {split.capitalize()} (Epoch {epoch+1})')\n    plt.savefig(f'confusion_matrix_{split}_epoch_{epoch+1}.png')\n    plt.close()\n\ndef plot_metrics(train_metric, val_metric, metric_name):\n    plt.figure(figsize=(10, 6))\n    epochs = range(1, len(train_metric) + 1)\n    plt.plot(epochs, train_metric, 'b-', label=f'Train {metric_name}')\n    plt.plot(epochs, val_metric, 'r-', label=f'Val {metric_name}')\n    plt.title(f'{metric_name} over Training')\n    plt.xlabel('Epoch')\n    plt.ylabel(metric_name)\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(f'{metric_name.lower()}_plot.png')\n    plt.close()\n","metadata":{"execution":{"iopub.status.busy":"2025-04-14T09:24:42.015148Z","iopub.execute_input":"2025-04-14T09:24:42.015346Z","iopub.status.idle":"2025-04-14T09:24:42.034857Z","shell.execute_reply.started":"2025-04-14T09:24:42.015331Z","shell.execute_reply":"2025-04-14T09:24:42.034341Z"},"trusted":true},"outputs":[],"execution_count":6},{"id":"2a282430-ea4b-4757-8e4d-7603105ba8b8","cell_type":"markdown","source":"## Main Training Loop\n\nWe load the tokenizer and datasets, define data loaders (with oversampling), set up the optimizer and scheduler, and train the model. We monitor performance on the validation set, apply early stopping, and finally evaluate the best models on the test set.","metadata":{}},{"id":"0640622c-1887-46e2-9a9b-71c32b3dfa01","cell_type":"code","source":"def main():\n    try:\n        print(\"Loading tokenizer...\")\n        tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n        \n        print(\"Loading datasets...\")\n        train_dataset = EnhancedDeceptionDataset(TRAIN_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n        val_dataset = EnhancedDeceptionDataset(VAL_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n        test_dataset = EnhancedDeceptionDataset(TEST_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n        \n        train_sampler = EnhancedBalancedSampler(train_dataset, oversample_factor=OVERSAMPLING_FACTOR)\n        train_loader = DataLoader(\n            train_dataset,\n            batch_size=BATCH_SIZE,\n            sampler=train_sampler,\n            num_workers=2,\n            collate_fn=custom_collate_fn\n        )\n        val_loader = DataLoader(\n            val_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=False,\n            num_workers=2,\n            collate_fn=custom_collate_fn\n        )\n        test_loader = DataLoader(\n            test_dataset,\n            batch_size=BATCH_SIZE,\n            shuffle=False,\n            num_workers=2,\n            collate_fn=custom_collate_fn\n        )\n        \n        train_class_counts = train_dataset.class_counts\n        total_samples = sum(train_class_counts.values())\n        weight_0 = total_samples / (train_class_counts.get(0, 1) * 2)\n        weight_1 = total_samples / (train_class_counts.get(1, 1) * 2)\n        class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(DEVICE)\n        print(f\"Class weights: Truth = {weight_0:.4f}, Lie = {weight_1:.4f}\")\n        \n        print(\"Initializing improved model...\")\n        model = ImprovedDeceptionModel(TRANSFORMER_MODEL, use_game_scores=USE_GAME_SCORES).to(DEVICE)\n        \n        optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n        total_steps = len(train_loader) * EPOCHS // GRADIENT_ACCUMULATION_STEPS\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=int(total_steps * 0.1),\n            num_training_steps=total_steps\n        )\n        \n        print(f\"Starting training for {EPOCHS} epochs...\")\n        best_truth_f1 = 0\n        best_macro_f1 = 0\n        no_improvement_count = 0\n        \n        train_losses = []\n        train_truth_f1s = []\n        train_lie_f1s = []\n        train_macro_f1s = []\n        val_losses = []\n        val_truth_f1s = []\n        val_lie_f1s = []\n        val_macro_f1s = []\n        \n        for epoch in range(EPOCHS):\n            train_loss, train_truth_f1, train_lie_f1, train_macro_f1, train_cm = train(\n                model, train_loader, optimizer, scheduler, DEVICE, class_weights,\n                truth_focal_weight=TRUTH_FOCAL_WEIGHT,\n                gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n            )\n            val_loss, val_truth_f1, val_lie_f1, val_macro_f1, val_cm = evaluate(\n                model, val_loader, DEVICE, class_weights,\n                truth_focal_weight=TRUTH_FOCAL_WEIGHT\n            )\n            \n            train_losses.append(train_loss)\n            train_truth_f1s.append(train_truth_f1)\n            train_lie_f1s.append(train_lie_f1)\n            train_macro_f1s.append(train_macro_f1)\n            \n            val_losses.append(val_loss)\n            val_truth_f1s.append(val_truth_f1)\n            val_lie_f1s.append(val_lie_f1)\n            val_macro_f1s.append(val_macro_f1)\n            \n            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n            print(f\"Train - Loss: {train_loss:.4f}, Truth F1: {train_truth_f1:.4f}, Lie F1: {train_lie_f1:.4f}, Macro F1: {train_macro_f1:.4f}\")\n            print(f\"Val   - Loss: {val_loss:.4f}, Truth F1: {val_truth_f1:.4f}, Lie F1: {val_lie_f1:.4f}, Macro F1: {val_macro_f1:.4f}\")\n            print(\"Confusion Matrix (Val):\")\n            print(val_cm)\n            print(\"-\" * 50)\n            plot_confusion_matrix(val_cm, epoch, 'val')\n            \n            improved = False\n            if val_truth_f1 > best_truth_f1:\n                best_truth_f1 = val_truth_f1\n                torch.save(model.state_dict(), 'best_truth_f1_model.pt')\n                print(f\"Saved new best model with Truth F1: {val_truth_f1:.4f}\")\n                improved = True\n            if val_macro_f1 > best_macro_f1:\n                best_macro_f1 = val_macro_f1\n                torch.save(model.state_dict(), 'best_macro_f1_model.pt')\n                print(f\"Saved new best model with Macro F1: {val_macro_f1:.4f}\")\n                improved = True\n            \n            if not improved:\n                no_improvement_count += 1\n                if no_improvement_count >= EARLY_STOPPING_PATIENCE:\n                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n                    break\n            else:\n                no_improvement_count = 0\n        \n        plot_metrics(train_losses, val_losses, 'Loss')\n        plot_metrics(train_truth_f1s, val_truth_f1s, 'Truth F1')\n        plot_metrics(train_lie_f1s, val_lie_f1s, 'Lie F1')\n        plot_metrics(train_macro_f1s, val_macro_f1s, 'Macro F1')\n        \n        print(\"\\nEvaluating best model (by Truth F1) on test set:\")\n        model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n            model, test_loader, DEVICE, class_weights,\n            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n        )\n        print(f\"\\nTest Results - Truth F1 Model:\")\n        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n        print(\"Confusion Matrix:\")\n        print(test_cm)\n        \n        print(\"\\nEvaluating best model (by Macro F1) on test set:\")\n        model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n            model, test_loader, DEVICE, class_weights,\n            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n        )\n        print(f\"\\nTest Results - Macro F1 Model:\")\n        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n        print(\"Confusion Matrix:\")\n        print(test_cm)\n        \n    except Exception as e:\n        print(f\"An error occurred during execution: {str(e)}\")\n        import traceback\n        traceback.print_exc()\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"execution":{"iopub.status.busy":"2025-04-12T15:16:04.181391Z","iopub.execute_input":"2025-04-12T15:16:04.181600Z","iopub.status.idle":"2025-04-12T16:27:03.566026Z","shell.execute_reply.started":"2025-04-12T15:16:04.181579Z","shell.execute_reply":"2025-04-12T16:27:03.565184Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Loading tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b9e2fe9bf0f4e10a8192bd7b619b0f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5912f123a6d74dddb9534d652e357fd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0188babbb09347889a91793659dcfc62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"932dc54edb3f42ffb3d7ef768fd9200a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f75c17ed91c47b8a2ec86e6c85c1bab"}},"metadata":{}},{"name":"stdout","text":"Loading datasets...\nDataset loaded from /kaggle/input/deception-data/data/train.jsonl\nTotal messages: 13132\nTruth: 591 (4.50%)\nLie: 12541 (95.50%)\nDataset loaded from /kaggle/input/deception-data/data/validation.jsonl\nTotal messages: 1416\nTruth: 56 (3.95%)\nLie: 1360 (96.05%)\nDataset loaded from /kaggle/input/deception-data/data/test.jsonl\nTotal messages: 2741\nTruth: 240 (8.76%)\nLie: 2501 (91.24%)\nClass weights: Truth = 11.1100, Lie = 0.5236\nInitializing improved model...\n","output_type":"stream"},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"070db46253f94754ac1e142b9cc7a4ac"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting training for 5 epochs...\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1109/1109 [13:46<00:00,  1.34it/s]\nEvaluating: 100%|██████████| 45/45 [00:10<00:00,  4.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.0556    0.7321    0.1034        56\n         Lie     0.9779    0.4882    0.6513      1360\n\n    accuracy                         0.4979      1416\n   macro avg     0.5168    0.6102    0.3774      1416\nweighted avg     0.9414    0.4979    0.6296      1416\n\nEpoch 1/5\nTrain - Loss: 0.2762, Truth F1: 0.7111, Lie F1: 0.3555, Macro F1: 0.5333\nVal   - Loss: 0.3908, Truth F1: 0.1034, Lie F1: 0.6513, Macro F1: 0.3774\nConfusion Matrix (Val):\n[[ 41  15]\n [696 664]]\n--------------------------------------------------\nSaved new best model with Truth F1: 0.1034\nSaved new best model with Macro F1: 0.3774\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1109/1109 [13:47<00:00,  1.34it/s]\nEvaluating: 100%|██████████| 45/45 [00:10<00:00,  4.19it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.0946    0.1250    0.1077        56\n         Lie     0.9635    0.9507    0.9571      1360\n\n    accuracy                         0.9181      1416\n   macro avg     0.5290    0.5379    0.5324      1416\nweighted avg     0.9291    0.9181    0.9235      1416\n\nEpoch 2/5\nTrain - Loss: 0.1277, Truth F1: 0.9113, Lie F1: 0.8994, Macro F1: 0.9053\nVal   - Loss: 0.7872, Truth F1: 0.1077, Lie F1: 0.9571, Macro F1: 0.5324\nConfusion Matrix (Val):\n[[   7   49]\n [  67 1293]]\n--------------------------------------------------\nSaved new best model with Truth F1: 0.1077\nSaved new best model with Macro F1: 0.5324\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1109/1109 [13:48<00:00,  1.34it/s]\nEvaluating: 100%|██████████| 45/45 [00:10<00:00,  4.17it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.1034    0.1607    0.1259        56\n         Lie     0.9646    0.9426    0.9535      1360\n\n    accuracy                         0.9117      1416\n   macro avg     0.5340    0.5517    0.5397      1416\nweighted avg     0.9306    0.9117    0.9208      1416\n\nEpoch 3/5\nTrain - Loss: 0.0692, Truth F1: 0.9641, Lie F1: 0.9631, Macro F1: 0.9636\nVal   - Loss: 1.3411, Truth F1: 0.1259, Lie F1: 0.9535, Macro F1: 0.5397\nConfusion Matrix (Val):\n[[   9   47]\n [  78 1282]]\n--------------------------------------------------\nSaved new best model with Truth F1: 0.1259\nSaved new best model with Macro F1: 0.5397\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1109/1109 [13:49<00:00,  1.34it/s]\nEvaluating: 100%|██████████| 45/45 [00:10<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.0930    0.0714    0.0808        56\n         Lie     0.9621    0.9713    0.9667      1360\n\n    accuracy                         0.9357      1416\n   macro avg     0.5276    0.5214    0.5238      1416\nweighted avg     0.9278    0.9357    0.9317      1416\n\nEpoch 4/5\nTrain - Loss: 0.0380, Truth F1: 0.9828, Lie F1: 0.9826, Macro F1: 0.9827\nVal   - Loss: 1.9773, Truth F1: 0.0808, Lie F1: 0.9667, Macro F1: 0.5238\nConfusion Matrix (Val):\n[[   4   52]\n [  39 1321]]\n--------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|██████████| 1109/1109 [13:48<00:00,  1.34it/s]\nEvaluating: 100%|██████████| 45/45 [00:10<00:00,  4.18it/s]\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.0882    0.0536    0.0667        56\n         Lie     0.9616    0.9772    0.9694      1360\n\n    accuracy                         0.9407      1416\n   macro avg     0.5249    0.5154    0.5180      1416\nweighted avg     0.9271    0.9407    0.9337      1416\n\nEpoch 5/5\nTrain - Loss: 0.0270, Truth F1: 0.9898, Lie F1: 0.9897, Macro F1: 0.9898\nVal   - Loss: 2.2377, Truth F1: 0.0667, Lie F1: 0.9694, Macro F1: 0.5180\nConfusion Matrix (Val):\n[[   3   53]\n [  31 1329]]\n--------------------------------------------------\n\nEvaluating best model (by Truth F1) on test set:\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/122680207.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_truth_f1_model.pt'))\nEvaluating: 100%|██████████| 86/86 [00:20<00:00,  4.16it/s]\n/tmp/ipykernel_31/122680207.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.1907    0.1708    0.1802       240\n         Lie     0.9212    0.9304    0.9258      2501\n\n    accuracy                         0.8639      2741\n   macro avg     0.5560    0.5506    0.5530      2741\nweighted avg     0.8573    0.8639    0.8605      2741\n\n\nTest Results - Truth F1 Model:\nLoss: 2.4442, Truth F1: 0.1802, Lie F1: 0.9258, Macro F1: 0.5530\nConfusion Matrix:\n[[  41  199]\n [ 174 2327]]\n\nEvaluating best model (by Macro F1) on test set:\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.16it/s]","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n       Truth     0.1907    0.1708    0.1802       240\n         Lie     0.9212    0.9304    0.9258      2501\n\n    accuracy                         0.8639      2741\n   macro avg     0.5560    0.5506    0.5530      2741\nweighted avg     0.8573    0.8639    0.8605      2741\n\n\nTest Results - Macro F1 Model:\nLoss: 2.4442, Truth F1: 0.1802, Lie F1: 0.9258, Macro F1: 0.5530\nConfusion Matrix:\n[[  41  199]\n [ 174 2327]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":7},{"id":"e8a066da-9884-4b48-aa8a-9e52f3c319d9","cell_type":"code","source":"def evaluate(model, dataloader, device, class_weights, truth_focal_weight=4.0):\n    model.eval()\n    total_loss = 0\n    all_labels = []\n    all_preds = []\n    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n    \n    with torch.no_grad():\n        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n            batch_data = prepare_batch_for_model(batch, device)\n            outputs = model(\n                input_ids=batch_data['input_ids'], \n                attention_mask=batch_data['attention_mask'],\n                context_input_ids=batch_data['context_input_ids'],\n                context_attention_mask=batch_data['context_attention_mask'],\n                game_scores=batch_data['scores'],\n                batch_adj_matrix=batch_data['batch_adj_matrix']\n            )\n            loss = loss_fn(outputs, batch_data['labels'])\n            total_loss += loss.item()\n            _, preds = torch.max(outputs, dim=1)\n            all_labels.extend(batch_data['labels'].cpu().numpy())\n            all_preds.extend(preds.cpu().numpy())\n    \n    avg_loss = total_loss / len(dataloader)\n    \n    # Changed: truth f1 now computed with pos_label=1 and lie f1 with pos_label=0.\n    truth_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n    lie_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n    macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n    \n    # Updated classification report: index 0 is \"Lie\", index 1 is \"Truth\"\n    print(classification_report(all_labels, all_preds, target_names=['Lie', 'Truth'], digits=4, zero_division=0))\n    cm = confusion_matrix(all_labels, all_preds)\n    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n\n\nprint(\"\\nEvaluating best model (by Truth F1) on test set:\")\nmodel.load_state_dict(torch.load('/kaggle/input/novel_modelswithout-conceptnet/pytorch/default/1/best_truth_f1_model.pt'))\ntest_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n    model, test_loader, DEVICE, class_weights,\n    truth_focal_weight=TRUTH_FOCAL_WEIGHT\n)\nprint(f\"\\nTest Results - Truth F1 Model:\")\nprint(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\nprint(\"Confusion Matrix:\")\nprint(test_cm)\n\nprint(\"\\nEvaluating best model (by Macro F1) on test set:\")\nmodel.load_state_dict(torch.load('/kaggle/input/novel_modelswithout-conceptnet/pytorch/default/1/best_macro_f1_model.pt'))\ntest_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n    model, test_loader, DEVICE, class_weights,\n    truth_focal_weight=TRUTH_FOCAL_WEIGHT\n)\nprint(f\"\\nTest Results - Macro F1 Model:\")\nprint(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\nprint(\"Confusion Matrix:\")\nprint(test_cm)\n","metadata":{"execution":{"iopub.status.busy":"2025-04-14T09:28:58.776535Z","iopub.execute_input":"2025-04-14T09:28:58.776853Z","iopub.status.idle":"2025-04-14T09:29:48.883898Z","shell.execute_reply.started":"2025-04-14T09:28:58.776825Z","shell.execute_reply":"2025-04-14T09:29:48.883038Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\nEvaluating best model (by Truth F1) on test set:\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/3462183625.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/novel_modelswithout-conceptnet/pytorch/default/1/best_truth_f1_model.pt'))\nEvaluating: 100%|██████████| 86/86 [00:21<00:00,  4.04it/s]\n/tmp/ipykernel_31/3462183625.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('/kaggle/input/novel_modelswithout-conceptnet/pytorch/default/1/best_macro_f1_model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         Lie     0.1907    0.1708    0.1802       240\n       Truth     0.9212    0.9304    0.9258      2501\n\n    accuracy                         0.8639      2741\n   macro avg     0.5560    0.5506    0.5530      2741\nweighted avg     0.8573    0.8639    0.8605      2741\n\n\nTest Results - Truth F1 Model:\nLoss: 2.4442, Truth F1: 0.9258, Lie F1: 0.1802, Macro F1: 0.5530\nConfusion Matrix:\n[[  41  199]\n [ 174 2327]]\n\nEvaluating best model (by Macro F1) on test set:\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 86/86 [00:20<00:00,  4.23it/s]","output_type":"stream"},{"name":"stdout","text":"              precision    recall  f1-score   support\n\n         Lie     0.1907    0.1708    0.1802       240\n       Truth     0.9212    0.9304    0.9258      2501\n\n    accuracy                         0.8639      2741\n   macro avg     0.5560    0.5506    0.5530      2741\nweighted avg     0.8573    0.8639    0.8605      2741\n\n\nTest Results - Macro F1 Model:\nLoss: 2.4442, Truth F1: 0.9258, Lie F1: 0.1802, Macro F1: 0.5530\nConfusion Matrix:\n[[  41  199]\n [ 174 2327]]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"id":"8a945954-02b1-4215-b3d1-07d3eafe9ddb","cell_type":"code","source":"! zip -r /kaggle/working/archive.zip /kaggle/working/\n","metadata":{"execution":{"iopub.status.busy":"2025-04-12T16:30:15.974615Z","iopub.execute_input":"2025-04-12T16:30:15.975375Z","iopub.status.idle":"2025-04-12T16:33:59.883353Z","shell.execute_reply.started":"2025-04-12T16:30:15.975344Z","shell.execute_reply":"2025-04-12T16:33:59.882563Z"},"trusted":true},"outputs":[{"name":"stdout","text":"  adding: kaggle/working/ (stored 0%)\n  adding: kaggle/working/truth f1_plot.png (deflated 19%)\n  adding: kaggle/working/confusion_matrix_val_epoch_2.png (deflated 21%)\n  adding: kaggle/working/confusion_matrix_val_epoch_5.png (deflated 22%)\n  adding: kaggle/working/confusion_matrix_val_epoch_3.png (deflated 22%)\n  adding: kaggle/working/confusion_matrix_val_epoch_4.png (deflated 22%)\n  adding: kaggle/working/lie f1_plot.png (deflated 14%)\n  adding: kaggle/working/macro f1_plot.png (deflated 14%)\n  adding: kaggle/working/best_truth_f1_model.pt (deflated 19%)\n  adding: kaggle/working/confusion_matrix_val_epoch_1.png (deflated 21%)\n  adding: kaggle/working/best_macro_f1_model.pt (deflated 19%)\n  adding: kaggle/working/.virtual_documents/ (stored 0%)\n  adding: kaggle/working/loss_plot.png (deflated 13%)\n","output_type":"stream"}],"execution_count":11},{"id":"fa2890f9-124b-4245-9cb3-17088415b5c9","cell_type":"markdown","source":"## End of Notebook\n\nThis notebook implements the improved deception detection model with enhanced context encoding, graph attention features, and ensemble classification. Experiment with regularization, learning rates, oversampling, or data augmentation as needed.","metadata":{}}]}