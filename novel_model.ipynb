{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c19470-1dee-4de7-9149-61aa68206de6",
   "metadata": {},
   "source": [
    "# Improved Deception Detection Model\n",
    "\n",
    "This notebook implements an end‐to‐end revised version of the deception detection code. In this version, we incorporate several improvements inspired by the research (e.g. Peskov et al. 2020) and related work on deception detection. In particular, we:\n",
    "\n",
    "- Introduce a new context encoder module that uses a bidirectional LSTM to summarize token‐level representations of the conversation context.\n",
    "- Fuse four streams of information: the current message representation (from RoBERTa), the contextual summary from the LSTM, graph features derived from conversation (via two graph attention layers), and game score features (as a proxy for power dynamics).\n",
    "- Use an ensemble of classifier heads (with learnable weights) to stabilize predictions.\n",
    "\n",
    "Due to limited data size, early transformer layers are frozen and extra regularization is applied.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca79d70f-8202-4d45-bb1f-91448b0bbedb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:55:48.433013Z",
     "iopub.status.busy": "2025-04-11T01:55:48.432768Z",
     "iopub.status.idle": "2025-04-11T01:56:12.827720Z",
     "shell.execute_reply": "2025-04-11T01:56:12.826978Z",
     "shell.execute_reply.started": "2025-04-11T01:55:48.432994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 01:56:01.967811: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744336562.156494      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744336562.211399      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer, get_linear_schedule_with_warmup, RobertaModel\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6083df-7696-4b3e-962c-feb9f15b882c",
   "metadata": {},
   "source": [
    "## Data Paths and Model Parameters\n",
    "\n",
    "Set the paths for the training, validation, and test datasets and define the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31a44ef2-c136-4583-b24f-65ddfbffa774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:56:12.830171Z",
     "iopub.status.busy": "2025-04-11T01:56:12.829708Z",
     "iopub.status.idle": "2025-04-11T01:56:12.834371Z",
     "shell.execute_reply": "2025-04-11T01:56:12.833651Z",
     "shell.execute_reply.started": "2025-04-11T01:56:12.830152Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data paths\n",
    "TRAIN_PATH = r\"/kaggle/input/deception-data/data/train.jsonl\"\n",
    "VAL_PATH = r\"/kaggle/input/deception-data/data/validation.jsonl\"\n",
    "TEST_PATH = r\"/kaggle/input/deception-data/data/test.jsonl\"\n",
    "\n",
    "# Model and training parameters\n",
    "TRANSFORMER_MODEL = \"roberta-base\"  # Using RoBERTa base\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 5\n",
    "LR = 5e-6\n",
    "USE_GAME_SCORES = True\n",
    "OVERSAMPLING_FACTOR = 30  # Oversampling factor (tune if necessary)\n",
    "TRUTH_FOCAL_WEIGHT = 4.0  # Additional weight for truth class during loss calculation\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "GRADIENT_ACCUMULATION_STEPS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e079dcf-b7e4-4e16-977b-41938f902634",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "We define a custom collate function and a dataset class that loads the JSONL data. For each conversation, we extract the message, its label, game score, and prior context (up to two previous messages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ab59264-6a57-4a91-aa0b-f67f732a7c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:56:12.835588Z",
     "iopub.status.busy": "2025-04-11T01:56:12.835167Z",
     "iopub.status.idle": "2025-04-11T01:56:12.872450Z",
     "shell.execute_reply": "2025-04-11T01:56:12.871921Z",
     "shell.execute_reply.started": "2025-04-11T01:56:12.835557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch):\n",
    "    # Collate all keys except 'relative_positions'\n",
    "    batch_without_relative = [{k: v for k, v in item.items() if k != 'relative_positions'} for item in batch]\n",
    "    collated = torch.utils.data.dataloader.default_collate(batch_without_relative)\n",
    "    collated['relative_positions'] = [item['relative_positions'] for item in batch]\n",
    "    return collated\n",
    "\n",
    "\n",
    "class EnhancedDeceptionDataset(Dataset):\n",
    "    def __init__(self, path, tokenizer, max_len=128, use_game_scores=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.use_game_scores = use_game_scores\n",
    "        self.texts = []\n",
    "        self.labels = []\n",
    "        self.scores = []\n",
    "        self.conversation_ids = []  # Track conversation id\n",
    "        self.message_positions = []  # Message order in conversation\n",
    "        self.prior_context = []  # Prior context (concatenated previous messages)\n",
    "        \n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            conv_id = 0\n",
    "            for line in f:\n",
    "                data = json.loads(line.strip())\n",
    "                messages = data.get('messages', [])\n",
    "                labels = data.get('sender_labels', [])\n",
    "                game_scores = data.get('game_score_delta', None) if use_game_scores else None\n",
    "                if game_scores is None:\n",
    "                    game_scores = [0] * len(messages)\n",
    "                \n",
    "                for pos, (msg, label, score) in enumerate(zip(messages, labels, game_scores)):\n",
    "                    if label in [True, False, \"true\", \"false\", \"True\", \"False\"]:\n",
    "                        # Convert string labels to boolean if needed\n",
    "                        if isinstance(label, str):\n",
    "                            is_lie = label.lower() == \"true\"\n",
    "                        else:\n",
    "                            is_lie = label\n",
    "                        # Convention: 1 = lie, 0 = truth\n",
    "                        self.texts.append(msg)\n",
    "                        self.labels.append(1 if is_lie else 0)\n",
    "                        self.scores.append(float(score))\n",
    "                        self.conversation_ids.append(conv_id)\n",
    "                        self.message_positions.append(pos)\n",
    "                        # Build prior context: use up to two previous messages\n",
    "                        context = \"\"\n",
    "                        if pos > 0:\n",
    "                            context_msgs = messages[max(0, pos-2):pos]\n",
    "                            context = \" [SEP] \".join(context_msgs)\n",
    "                        self.prior_context.append(context)\n",
    "                conv_id += 1\n",
    "        \n",
    "        self.class_counts = Counter(self.labels)\n",
    "        total = len(self.labels)\n",
    "        self.truth_indices = [i for i, label in enumerate(self.labels) if label == 0]\n",
    "        self.lie_indices = [i for i, label in enumerate(self.labels) if label == 1]\n",
    "        \n",
    "        # Group messages by conversation for potential graph construction\n",
    "        self.conv_to_msgs = defaultdict(list)\n",
    "        for i, cid in enumerate(self.conversation_ids):\n",
    "            self.conv_to_msgs[cid].append(i)\n",
    "        \n",
    "        print(f\"Dataset loaded from {path}\")\n",
    "        print(f\"Total messages: {total}\")\n",
    "        for label, count in sorted(self.class_counts.items()):\n",
    "            label_name = \"Truth\" if label == 0 else \"Lie\"\n",
    "            print(f\"{label_name}: {count} ({count/total*100:.2f}%)\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        score = self.scores[idx]\n",
    "        conv_id = self.conversation_ids[idx]\n",
    "        position = self.message_positions[idx]\n",
    "        context = self.prior_context[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        if context:\n",
    "            context_encoding = self.tokenizer(\n",
    "                context,\n",
    "                max_length=self.max_len,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "        else:\n",
    "            context_encoding = {\n",
    "                'input_ids': torch.zeros((1, self.max_len), dtype=torch.long),\n",
    "                'attention_mask': torch.zeros((1, self.max_len), dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'context_input_ids': context_encoding['input_ids'].flatten(),\n",
    "            'context_attention_mask': context_encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long),\n",
    "            'score': torch.tensor(score, dtype=torch.float),\n",
    "            'conv_id': conv_id,\n",
    "            'position': position,\n",
    "            'relative_positions': []  # Reserved for legacy\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73263c5b-c12e-4782-9e10-eb39e83c5619",
   "metadata": {},
   "source": [
    "## Enhanced Balanced Sampler\n",
    "\n",
    "This sampler oversamples the truth class to counteract class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0939318-ad48-4d9d-b220-f6e6f385cecd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:56:12.873542Z",
     "iopub.status.busy": "2025-04-11T01:56:12.873239Z",
     "iopub.status.idle": "2025-04-11T01:56:12.892278Z",
     "shell.execute_reply": "2025-04-11T01:56:12.891567Z",
     "shell.execute_reply.started": "2025-04-11T01:56:12.873514Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EnhancedBalancedSampler(torch.utils.data.Sampler):\n",
    "    def __init__(self, dataset, oversample_factor=15):\n",
    "        self.dataset = dataset\n",
    "        self.oversample_factor = oversample_factor\n",
    "        self.truth_indices = dataset.truth_indices * oversample_factor  # Oversample truths\n",
    "        self.lie_indices = dataset.lie_indices\n",
    "        \n",
    "        min_samples = max(1000, len(self.truth_indices))\n",
    "        target_size = min(len(self.truth_indices), len(self.lie_indices))\n",
    "        target_size = max(target_size, min_samples)\n",
    "        \n",
    "        if len(self.truth_indices) < target_size:\n",
    "            self.truth_indices = self.truth_indices * (target_size // len(self.truth_indices) + 1)\n",
    "        if len(self.lie_indices) < target_size:\n",
    "            self.lie_indices = self.lie_indices * (target_size // len(self.lie_indices) + 1)\n",
    "        \n",
    "        self.truth_indices = random.sample(self.truth_indices, target_size)\n",
    "        self.lie_indices = random.sample(self.lie_indices, target_size)\n",
    "        \n",
    "        self.indices = self.truth_indices + self.lie_indices\n",
    "        random.shuffle(self.indices)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ce1fbe-e325-4946-baee-4505d2d6168b",
   "metadata": {},
   "source": [
    "## Model Components\n",
    "\n",
    "We define the following model components:\n",
    "\n",
    "1. **Simple Attention Module** (for potential auxiliary use).\n",
    "2. **Graph Attention Layer** (to compute relational features from conversation graphs).\n",
    "3. **Context Encoder**: a bidirectional LSTM that summarizes token-level representations of the conversation context. *(Note: This module now clamps any zero-length sequences to have length 1.)*\n",
    "4. **ImprovedDeceptionModel**: integrates the current message, context summary, graph features, and game score features; uses an ensemble of classifier heads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982076c5-0d2a-4826-bff3-4da66f6bc4e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:56:12.893422Z",
     "iopub.status.busy": "2025-04-11T01:56:12.893059Z",
     "iopub.status.idle": "2025-04-11T01:56:12.913934Z",
     "shell.execute_reply": "2025-04-11T01:56:12.913392Z",
     "shell.execute_reply.started": "2025-04-11T01:56:12.893404Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, dropout=0.1):\n",
    "        super(SimpleAttention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.key_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.value_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        query = self.query_proj(query)\n",
    "        key = self.key_proj(key)\n",
    "        value = self.value_proj(value)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / (self.hidden_size ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        context = torch.matmul(attn_weights, value)\n",
    "        output = self.out_proj(context)\n",
    "        return output\n",
    "\n",
    "\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1, alpha=0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.dropout = dropout\n",
    "        self.alpha = alpha\n",
    "        self.W = nn.Parameter(torch.zeros(size=(in_features, out_features)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.zeros(size=(2*out_features, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        \n",
    "    def forward(self, features, adj_matrix):\n",
    "        h = torch.mm(features, self.W)  # (N, out_features)\n",
    "        N = h.size()[0]\n",
    "        a_input = torch.cat([h.repeat(1, N).view(N * N, -1), h.repeat(N, 1)], dim=1)\n",
    "        a_input = a_input.view(N, N, 2 * self.out_features)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj_matrix > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = F.dropout(attention, self.dropout, training=self.training)\n",
    "        h_prime = torch.matmul(attention, h)\n",
    "        return h_prime\n",
    "\n",
    "\n",
    "class ContextEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, bidirectional=True, dropout=0.1):\n",
    "        super(ContextEncoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, \n",
    "                            bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "    def forward(self, token_embeddings, attention_mask):\n",
    "        # Compute lengths from attention mask, clamp to minimum of 1 to avoid zero-length sequences\n",
    "        lengths = torch.clamp(attention_mask.sum(dim=1), min=1).cpu()\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(token_embeddings, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_outputs, (h_n, _) = self.lstm(packed)\n",
    "        num_directions = 2 if self.lstm.bidirectional else 1\n",
    "        h_n = h_n.view(self.lstm.num_layers, num_directions, token_embeddings.size(0), self.lstm.hidden_size)\n",
    "        h_final = torch.cat((h_n[-1, 0, :, :], h_n[-1, 1, :, :]), dim=1)\n",
    "        return h_final\n",
    "\n",
    "\n",
    "class ImprovedDeceptionModel(nn.Module):\n",
    "    def __init__(self, model_name, use_game_scores=True):\n",
    "        super(ImprovedDeceptionModel, self).__init__()\n",
    "        # Pretrained transformers for current message and context\n",
    "        self.transformer = RobertaModel.from_pretrained(model_name)\n",
    "        self.context_transformer = RobertaModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Freeze first 2 encoder layers for both transformers\n",
    "        for layer in self.transformer.encoder.layer[:2]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        for layer in self.context_transformer.encoder.layer[:2]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.hidden_size = self.transformer.config.hidden_size\n",
    "        self.use_game_scores = use_game_scores\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.context_encoder = ContextEncoder(input_dim=self.hidden_size, hidden_dim=self.hidden_size//2, \n",
    "                                               num_layers=1, bidirectional=True, dropout=0.1)\n",
    "        \n",
    "        # Graph attention modules (two-layer GAT)\n",
    "        self.gat1 = GraphAttentionLayer(self.hidden_size + self.context_encoder.output_dim, 256)\n",
    "        self.gat2 = GraphAttentionLayer(256, 256)\n",
    "        \n",
    "        # Game score processing branch\n",
    "        if use_game_scores:\n",
    "            self.score_proj = nn.Sequential(\n",
    "                nn.Linear(1, 32),\n",
    "                nn.LayerNorm(32),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.2)\n",
    "            )\n",
    "            fusion_dim = self.hidden_size + self.context_encoder.output_dim + 256 + 32\n",
    "        else:\n",
    "            fusion_dim = self.hidden_size + self.context_encoder.output_dim + 256\n",
    "        \n",
    "        self.feature_norm = nn.LayerNorm(fusion_dim)\n",
    "        \n",
    "        # Ensemble of classifier heads\n",
    "        self.classifier1 = nn.Linear(fusion_dim, 2)  # from fused features\n",
    "        self.classifier2 = nn.Linear(self.hidden_size + self.context_encoder.output_dim, 2)  # current+context\n",
    "        self.classifier3 = nn.Linear(256, 2)  # from graph features\n",
    "        self.ensemble_weights = nn.Parameter(torch.ones(3))\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, context_input_ids=None, \n",
    "                context_attention_mask=None, game_scores=None, batch_adj_matrix=None):\n",
    "        # Current message representation\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "        text_features = self.dropout(pooled_output)\n",
    "        \n",
    "        # Context processing: encode context tokens and summarize via LSTM\n",
    "        if context_input_ids is not None and torch.sum(context_input_ids) > 0:\n",
    "            ctx_outputs = self.context_transformer(input_ids=context_input_ids, attention_mask=context_attention_mask)\n",
    "            ctx_token_embeddings = ctx_outputs.last_hidden_state  \n",
    "            context_summary = self.context_encoder(ctx_token_embeddings, context_attention_mask)\n",
    "        else:\n",
    "            batch_size = text_features.size(0)\n",
    "            context_summary = torch.zeros(batch_size, self.context_encoder.output_dim, device=text_features.device)\n",
    "        \n",
    "        # Combine current message and context summary\n",
    "        combined_features = torch.cat([text_features, context_summary], dim=1)\n",
    "        \n",
    "        # Graph-based relational features\n",
    "        if batch_adj_matrix is not None:\n",
    "            graph_features = self.gat1(combined_features, batch_adj_matrix)\n",
    "            graph_features = F.elu(graph_features)\n",
    "            graph_features = self.gat2(graph_features, batch_adj_matrix)\n",
    "        else:\n",
    "            graph_features = torch.zeros(combined_features.size(0), 256, device=combined_features.device)\n",
    "        \n",
    "        # Fuse features: current+context, graph, and optionally game scores\n",
    "        if self.use_game_scores and game_scores is not None:\n",
    "            score_features = self.score_proj(game_scores.unsqueeze(1))\n",
    "            all_features = torch.cat([combined_features, graph_features, score_features], dim=1)\n",
    "        else:\n",
    "            all_features = torch.cat([combined_features, graph_features], dim=1)\n",
    "        \n",
    "        all_features = self.feature_norm(all_features)\n",
    "        logits1 = self.classifier1(all_features)\n",
    "        logits2 = self.classifier2(combined_features)\n",
    "        logits3 = self.classifier3(graph_features)\n",
    "        ensemble_weights = F.softmax(self.ensemble_weights, dim=0)\n",
    "        final_logits = (ensemble_weights[0] * logits1 +\n",
    "                        ensemble_weights[1] * logits2 +\n",
    "                        ensemble_weights[2] * logits3)\n",
    "        return final_logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ca6dc5-fddb-4f53-be51-ed873d66e2fd",
   "metadata": {},
   "source": [
    "## Loss, Batch Preparation, and Training/Evaluation Functions\n",
    "\n",
    "We define the focal weighted loss, a batch preparation function (which builds a simple adjacency matrix for conversation graphs), and the training/evaluation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e75914b-ebd4-4889-84b6-5a04e060c4a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:56:12.914716Z",
     "iopub.status.busy": "2025-04-11T01:56:12.914549Z",
     "iopub.status.idle": "2025-04-11T01:56:12.939696Z",
     "shell.execute_reply": "2025-04-11T01:56:12.939209Z",
     "shell.execute_reply.started": "2025-04-11T01:56:12.914702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FocalWeightedLoss(nn.Module):\n",
    "    def __init__(self, class_weights, truth_focal_weight=4.0):\n",
    "        super(FocalWeightedLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.truth_focal_weight = truth_focal_weight\n",
    "    \n",
    "    def forward(self, logits, targets):\n",
    "        ce_loss = F.cross_entropy(logits, targets, weight=self.class_weights, reduction='none')\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        truth_probs = probs[:, 0]  # probability for truth class\n",
    "        truth_mask = (targets == 0).float()\n",
    "        focal_weight = (1 - truth_probs) ** self.truth_focal_weight\n",
    "        focal_loss = truth_mask * focal_weight * ce_loss + (1 - truth_mask) * ce_loss\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "def prepare_batch_for_model(batch, device):\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    context_input_ids = batch['context_input_ids'].to(device) if 'context_input_ids' in batch else None\n",
    "    context_attention_mask = batch['context_attention_mask'].to(device) if 'context_attention_mask' in batch else None\n",
    "    labels = batch['label'].to(device)\n",
    "    scores = batch['score'].to(device) if 'score' in batch else None\n",
    "    batch_size = input_ids.size(0)\n",
    "    \n",
    "    # Build a simple adjacency matrix for messages in the same conversation\n",
    "    batch_adj_matrix = torch.zeros((batch_size, batch_size), device=device)\n",
    "    conv_ids = batch['conv_id'] if isinstance(batch['conv_id'], list) else batch['conv_id'].tolist()\n",
    "    positions = batch['position'] if isinstance(batch['position'], list) else batch['position'].tolist()\n",
    "    for i in range(batch_size):\n",
    "        for j in range(batch_size):\n",
    "            if conv_ids[i] == conv_ids[j]:\n",
    "                if i == j:\n",
    "                    batch_adj_matrix[i, j] = 1.0\n",
    "                else:\n",
    "                    distance = abs(positions[i] - positions[j])\n",
    "                    batch_adj_matrix[i, j] = 1.0 / (distance + 1)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'context_input_ids': context_input_ids,\n",
    "        'context_attention_mask': context_attention_mask,\n",
    "        'labels': labels,\n",
    "        'scores': scores,\n",
    "        'batch_adj_matrix': batch_adj_matrix\n",
    "    }\n",
    "\n",
    "def train(model, dataloader, optimizer, scheduler, device, class_weights, truth_focal_weight=4.0, gradient_accumulation_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n",
    "    optimizer.zero_grad()\n",
    "    accumulated_steps = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        batch_data = prepare_batch_for_model(batch, device)\n",
    "        outputs = model(\n",
    "            input_ids=batch_data['input_ids'], \n",
    "            attention_mask=batch_data['attention_mask'],\n",
    "            context_input_ids=batch_data['context_input_ids'],\n",
    "            context_attention_mask=batch_data['context_attention_mask'],\n",
    "            game_scores=batch_data['scores'],\n",
    "            batch_adj_matrix=batch_data['batch_adj_matrix']\n",
    "        )\n",
    "        loss = loss_fn(outputs, batch_data['labels'])\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * gradient_accumulation_steps\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        all_labels.extend(batch_data['labels'].cpu().numpy())\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        accumulated_steps += 1\n",
    "        if accumulated_steps % gradient_accumulation_steps == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            if scheduler:\n",
    "                scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "    if accumulated_steps % gradient_accumulation_steps != 0:\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    try:\n",
    "        truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n",
    "        lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n",
    "        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    except Exception:\n",
    "        truth_f1 = lie_f1 = macro_f1 = 0\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n",
    "\n",
    "def evaluate(model, dataloader, device, class_weights, truth_focal_weight=4.0):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    loss_fn = FocalWeightedLoss(class_weights, truth_focal_weight)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            batch_data = prepare_batch_for_model(batch, device)\n",
    "            outputs = model(\n",
    "                input_ids=batch_data['input_ids'], \n",
    "                attention_mask=batch_data['attention_mask'],\n",
    "                context_input_ids=batch_data['context_input_ids'],\n",
    "                context_attention_mask=batch_data['context_attention_mask'],\n",
    "                game_scores=batch_data['scores'],\n",
    "                batch_adj_matrix=batch_data['batch_adj_matrix']\n",
    "            )\n",
    "            loss = loss_fn(outputs, batch_data['labels'])\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            all_labels.extend(batch_data['labels'].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    try:\n",
    "        truth_f1 = f1_score(all_labels, all_preds, pos_label=0, average='binary', zero_division=0)\n",
    "        lie_f1 = f1_score(all_labels, all_preds, pos_label=1, average='binary', zero_division=0)\n",
    "        macro_f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        print(classification_report(all_labels, all_preds, target_names=['Truth', 'Lie'], digits=4, zero_division=0))\n",
    "    except Exception:\n",
    "        truth_f1 = lie_f1 = macro_f1 = 0\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    return avg_loss, truth_f1, lie_f1, macro_f1, cm\n",
    "\n",
    "def plot_confusion_matrix(cm, epoch, split='val'):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Truth', 'Lie'], yticklabels=['Truth', 'Lie'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix - {split.capitalize()} (Epoch {epoch+1})')\n",
    "    plt.savefig(f'confusion_matrix_{split}_epoch_{epoch+1}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_metrics(train_metric, val_metric, metric_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    epochs = range(1, len(train_metric) + 1)\n",
    "    plt.plot(epochs, train_metric, 'b-', label=f'Train {metric_name}')\n",
    "    plt.plot(epochs, val_metric, 'r-', label=f'Val {metric_name}')\n",
    "    plt.title(f'{metric_name} over Training')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{metric_name.lower()}_plot.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a282430-ea4b-4757-8e4d-7603105ba8b8",
   "metadata": {},
   "source": [
    "## Main Training Loop\n",
    "\n",
    "We load the tokenizer and datasets, define data loaders (with oversampling), set up the optimizer and scheduler, and train the model. We monitor performance on the validation set, apply early stopping, and finally evaluate the best models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0640622c-1887-46e2-9a9b-71c32b3dfa01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T01:56:12.941855Z",
     "iopub.status.busy": "2025-04-11T01:56:12.941640Z",
     "iopub.status.idle": "2025-04-11T03:10:27.729657Z",
     "shell.execute_reply": "2025-04-11T03:10:27.728657Z",
     "shell.execute_reply.started": "2025-04-11T01:56:12.941840Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4186ec5dd4354690a8e96995caf7da55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2701d544f240d9be8569636a78f0a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d8ebd231304ce79087dafdb4a5bd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f416f3872849c89b8b7f3b95752ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ebd232f7c0145989dd168834c809548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Dataset loaded from /kaggle/input/deception-data/data/train.jsonl\n",
      "Total messages: 13132\n",
      "Truth: 591 (4.50%)\n",
      "Lie: 12541 (95.50%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/validation.jsonl\n",
      "Total messages: 1416\n",
      "Truth: 56 (3.95%)\n",
      "Lie: 1360 (96.05%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/test.jsonl\n",
      "Total messages: 2741\n",
      "Truth: 240 (8.76%)\n",
      "Lie: 2501 (91.24%)\n",
      "Class weights: Truth = 11.1100, Lie = 0.5236\n",
      "Initializing improved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b34b75f4274a74924c0bceb264aa85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 5 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2217/2217 [14:28<00:00,  2.55it/s]\n",
      "Evaluating: 100%|██████████| 89/89 [00:10<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0852    0.2679    0.1293        56\n",
      "         Lie     0.9669    0.8816    0.9223      1360\n",
      "\n",
      "    accuracy                         0.8573      1416\n",
      "   macro avg     0.5261    0.5747    0.5258      1416\n",
      "weighted avg     0.9321    0.8573    0.8909      1416\n",
      "\n",
      "Epoch 1/5\n",
      "Train - Loss: 0.2580, Truth F1: 0.7371, Lie F1: 0.4867, Macro F1: 0.6119\n",
      "Val   - Loss: 0.3421, Truth F1: 0.1293, Lie F1: 0.9223, Macro F1: 0.5258\n",
      "Confusion Matrix (Val):\n",
      "[[  15   41]\n",
      " [ 161 1199]]\n",
      "--------------------------------------------------\n",
      "Saved new best model with Truth F1: 0.1293\n",
      "Saved new best model with Macro F1: 0.5258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2217/2217 [14:28<00:00,  2.55it/s]\n",
      "Evaluating: 100%|██████████| 89/89 [00:10<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0864    0.1250    0.1022        56\n",
      "         Lie     0.9633    0.9456    0.9544      1360\n",
      "\n",
      "    accuracy                         0.9131      1416\n",
      "   macro avg     0.5249    0.5353    0.5283      1416\n",
      "weighted avg     0.9286    0.9131    0.9207      1416\n",
      "\n",
      "Epoch 2/5\n",
      "Train - Loss: 0.0961, Truth F1: 0.9508, Lie F1: 0.9490, Macro F1: 0.9499\n",
      "Val   - Loss: 1.4304, Truth F1: 0.1022, Lie F1: 0.9544, Macro F1: 0.5283\n",
      "Confusion Matrix (Val):\n",
      "[[   7   49]\n",
      " [  74 1286]]\n",
      "--------------------------------------------------\n",
      "Saved new best model with Macro F1: 0.5283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2217/2217 [14:29<00:00,  2.55it/s]\n",
      "Evaluating: 100%|██████████| 89/89 [00:10<00:00,  8.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0526    0.0357    0.0426        56\n",
      "         Lie     0.9608    0.9735    0.9671      1360\n",
      "\n",
      "    accuracy                         0.9364      1416\n",
      "   macro avg     0.5067    0.5046    0.5048      1416\n",
      "weighted avg     0.9249    0.9364    0.9306      1416\n",
      "\n",
      "Epoch 3/5\n",
      "Train - Loss: 0.0558, Truth F1: 0.9848, Lie F1: 0.9849, Macro F1: 0.9848\n",
      "Val   - Loss: 2.4548, Truth F1: 0.0426, Lie F1: 0.9671, Macro F1: 0.5048\n",
      "Confusion Matrix (Val):\n",
      "[[   2   54]\n",
      " [  36 1324]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2217/2217 [14:28<00:00,  2.55it/s]\n",
      "Evaluating: 100%|██████████| 89/89 [00:10<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0000    0.0000    0.0000        56\n",
      "         Lie     0.9600    0.9882    0.9739      1360\n",
      "\n",
      "    accuracy                         0.9492      1416\n",
      "   macro avg     0.4800    0.4941    0.4870      1416\n",
      "weighted avg     0.9220    0.9492    0.9354      1416\n",
      "\n",
      "Epoch 4/5\n",
      "Train - Loss: 0.0289, Truth F1: 0.9942, Lie F1: 0.9943, Macro F1: 0.9942\n",
      "Val   - Loss: 3.2291, Truth F1: 0.0000, Lie F1: 0.9739, Macro F1: 0.4870\n",
      "Confusion Matrix (Val):\n",
      "[[   0   56]\n",
      " [  16 1344]]\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 2217/2217 [14:29<00:00,  2.55it/s]\n",
      "Evaluating: 100%|██████████| 89/89 [00:10<00:00,  8.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.0000    0.0000    0.0000        56\n",
      "         Lie     0.9601    0.9897    0.9747      1360\n",
      "\n",
      "    accuracy                         0.9506      1416\n",
      "   macro avg     0.4800    0.4949    0.4873      1416\n",
      "weighted avg     0.9221    0.9506    0.9361      1416\n",
      "\n",
      "Epoch 5/5\n",
      "Train - Loss: 0.0182, Truth F1: 0.9971, Lie F1: 0.9971, Macro F1: 0.9971\n",
      "Val   - Loss: 3.6465, Truth F1: 0.0000, Lie F1: 0.9747, Macro F1: 0.4873\n",
      "Confusion Matrix (Val):\n",
      "[[   0   56]\n",
      " [  14 1346]]\n",
      "--------------------------------------------------\n",
      "\n",
      "Evaluating best model (by Truth F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/122680207.py:121: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n",
      "Evaluating: 100%|██████████| 172/172 [00:20<00:00,  8.24it/s]\n",
      "/tmp/ipykernel_31/122680207.py:132: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1622    0.2750    0.2040       240\n",
      "         Lie     0.9254    0.8637    0.8935      2501\n",
      "\n",
      "    accuracy                         0.8121      2741\n",
      "   macro avg     0.5438    0.5693    0.5488      2741\n",
      "weighted avg     0.8586    0.8121    0.8331      2741\n",
      "\n",
      "\n",
      "Test Results - Truth F1 Model:\n",
      "Loss: 0.5632, Truth F1: 0.2040, Lie F1: 0.8935, Macro F1: 0.5488\n",
      "Confusion Matrix:\n",
      "[[  66  174]\n",
      " [ 341 2160]]\n",
      "\n",
      "Evaluating best model (by Macro F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 172/172 [00:20<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1895    0.1208    0.1476       240\n",
      "         Lie     0.9185    0.9504    0.9342      2501\n",
      "\n",
      "    accuracy                         0.8778      2741\n",
      "   macro avg     0.5540    0.5356    0.5409      2741\n",
      "weighted avg     0.8546    0.8778    0.8653      2741\n",
      "\n",
      "\n",
      "Test Results - Macro F1 Model:\n",
      "Loss: 3.0862, Truth F1: 0.1476, Lie F1: 0.9342, Macro F1: 0.5409\n",
      "Confusion Matrix:\n",
      "[[  29  211]\n",
      " [ 124 2377]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    try:\n",
    "        print(\"Loading tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n",
    "        \n",
    "        print(\"Loading datasets...\")\n",
    "        train_dataset = EnhancedDeceptionDataset(TRAIN_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "        val_dataset = EnhancedDeceptionDataset(VAL_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "        test_dataset = EnhancedDeceptionDataset(TEST_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "        \n",
    "        train_sampler = EnhancedBalancedSampler(train_dataset, oversample_factor=OVERSAMPLING_FACTOR)\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            collate_fn=custom_collate_fn\n",
    "        )\n",
    "        \n",
    "        train_class_counts = train_dataset.class_counts\n",
    "        total_samples = sum(train_class_counts.values())\n",
    "        weight_0 = total_samples / (train_class_counts.get(0, 1) * 2)\n",
    "        weight_1 = total_samples / (train_class_counts.get(1, 1) * 2)\n",
    "        class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(DEVICE)\n",
    "        print(f\"Class weights: Truth = {weight_0:.4f}, Lie = {weight_1:.4f}\")\n",
    "        \n",
    "        print(\"Initializing improved model...\")\n",
    "        model = ImprovedDeceptionModel(TRANSFORMER_MODEL, use_game_scores=USE_GAME_SCORES).to(DEVICE)\n",
    "        \n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
    "        total_steps = len(train_loader) * EPOCHS // GRADIENT_ACCUMULATION_STEPS\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=int(total_steps * 0.1),\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        print(f\"Starting training for {EPOCHS} epochs...\")\n",
    "        best_truth_f1 = 0\n",
    "        best_macro_f1 = 0\n",
    "        no_improvement_count = 0\n",
    "        \n",
    "        train_losses = []\n",
    "        train_truth_f1s = []\n",
    "        train_lie_f1s = []\n",
    "        train_macro_f1s = []\n",
    "        val_losses = []\n",
    "        val_truth_f1s = []\n",
    "        val_lie_f1s = []\n",
    "        val_macro_f1s = []\n",
    "        \n",
    "        for epoch in range(EPOCHS):\n",
    "            train_loss, train_truth_f1, train_lie_f1, train_macro_f1, train_cm = train(\n",
    "                model, train_loader, optimizer, scheduler, DEVICE, class_weights,\n",
    "                truth_focal_weight=TRUTH_FOCAL_WEIGHT,\n",
    "                gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS\n",
    "            )\n",
    "            val_loss, val_truth_f1, val_lie_f1, val_macro_f1, val_cm = evaluate(\n",
    "                model, val_loader, DEVICE, class_weights,\n",
    "                truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    "            )\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            train_truth_f1s.append(train_truth_f1)\n",
    "            train_lie_f1s.append(train_lie_f1)\n",
    "            train_macro_f1s.append(train_macro_f1)\n",
    "            \n",
    "            val_losses.append(val_loss)\n",
    "            val_truth_f1s.append(val_truth_f1)\n",
    "            val_lie_f1s.append(val_lie_f1)\n",
    "            val_macro_f1s.append(val_macro_f1)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "            print(f\"Train - Loss: {train_loss:.4f}, Truth F1: {train_truth_f1:.4f}, Lie F1: {train_lie_f1:.4f}, Macro F1: {train_macro_f1:.4f}\")\n",
    "            print(f\"Val   - Loss: {val_loss:.4f}, Truth F1: {val_truth_f1:.4f}, Lie F1: {val_lie_f1:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
    "            print(\"Confusion Matrix (Val):\")\n",
    "            print(val_cm)\n",
    "            print(\"-\" * 50)\n",
    "            plot_confusion_matrix(val_cm, epoch, 'val')\n",
    "            \n",
    "            improved = False\n",
    "            if val_truth_f1 > best_truth_f1:\n",
    "                best_truth_f1 = val_truth_f1\n",
    "                torch.save(model.state_dict(), 'best_truth_f1_model.pt')\n",
    "                print(f\"Saved new best model with Truth F1: {val_truth_f1:.4f}\")\n",
    "                improved = True\n",
    "            if val_macro_f1 > best_macro_f1:\n",
    "                best_macro_f1 = val_macro_f1\n",
    "                torch.save(model.state_dict(), 'best_macro_f1_model.pt')\n",
    "                print(f\"Saved new best model with Macro F1: {val_macro_f1:.4f}\")\n",
    "                improved = True\n",
    "            \n",
    "            if not improved:\n",
    "                no_improvement_count += 1\n",
    "                if no_improvement_count >= EARLY_STOPPING_PATIENCE:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "            else:\n",
    "                no_improvement_count = 0\n",
    "        \n",
    "        plot_metrics(train_losses, val_losses, 'Loss')\n",
    "        plot_metrics(train_truth_f1s, val_truth_f1s, 'Truth F1')\n",
    "        plot_metrics(train_lie_f1s, val_lie_f1s, 'Lie F1')\n",
    "        plot_metrics(train_macro_f1s, val_macro_f1s, 'Macro F1')\n",
    "        \n",
    "        print(\"\\nEvaluating best model (by Truth F1) on test set:\")\n",
    "        model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n",
    "        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n",
    "            model, test_loader, DEVICE, class_weights,\n",
    "            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    "        )\n",
    "        print(f\"\\nTest Results - Truth F1 Model:\")\n",
    "        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(test_cm)\n",
    "        \n",
    "        print(\"\\nEvaluating best model (by Macro F1) on test set:\")\n",
    "        model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n",
    "        test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n",
    "            model, test_loader, DEVICE, class_weights,\n",
    "            truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    "        )\n",
    "        print(f\"\\nTest Results - Macro F1 Model:\")\n",
    "        print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "        print(\"Confusion Matrix:\")\n",
    "        print(test_cm)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during execution: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a066da-9884-4b48-aa8a-9e52f3c319d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T03:10:27.732045Z",
     "iopub.status.busy": "2025-04-11T03:10:27.731810Z",
     "iopub.status.idle": "2025-04-11T03:11:12.501995Z",
     "shell.execute_reply": "2025-04-11T03:11:12.501065Z",
     "shell.execute_reply.started": "2025-04-11T03:10:27.732022Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Loading datasets...\n",
      "Dataset loaded from /kaggle/input/deception-data/data/train.jsonl\n",
      "Total messages: 13132\n",
      "Truth: 591 (4.50%)\n",
      "Lie: 12541 (95.50%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/validation.jsonl\n",
      "Total messages: 1416\n",
      "Truth: 56 (3.95%)\n",
      "Lie: 1360 (96.05%)\n",
      "Dataset loaded from /kaggle/input/deception-data/data/test.jsonl\n",
      "Total messages: 2741\n",
      "Truth: 240 (8.76%)\n",
      "Lie: 2501 (91.24%)\n",
      "Class weights: Truth = 11.1100, Lie = 0.5236\n",
      "Initializing improved model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating best model (by Truth F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/3638066931.py:43: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n",
      "Evaluating: 100%|██████████| 172/172 [00:20<00:00,  8.25it/s]\n",
      "/tmp/ipykernel_31/3638066931.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1622    0.2750    0.2040       240\n",
      "         Lie     0.9254    0.8637    0.8935      2501\n",
      "\n",
      "    accuracy                         0.8121      2741\n",
      "   macro avg     0.5438    0.5693    0.5488      2741\n",
      "weighted avg     0.8586    0.8121    0.8331      2741\n",
      "\n",
      "\n",
      "Test Results - Truth F1 Model:\n",
      "Loss: 0.5632, Truth F1: 0.2040, Lie F1: 0.8935, Macro F1: 0.5488\n",
      "Confusion Matrix:\n",
      "[[  66  174]\n",
      " [ 341 2160]]\n",
      "\n",
      "Evaluating best model (by Macro F1) on test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 172/172 [00:20<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Truth     0.1895    0.1208    0.1476       240\n",
      "         Lie     0.9185    0.9504    0.9342      2501\n",
      "\n",
      "    accuracy                         0.8778      2741\n",
      "   macro avg     0.5540    0.5356    0.5409      2741\n",
      "weighted avg     0.8546    0.8778    0.8653      2741\n",
      "\n",
      "\n",
      "Test Results - Macro F1 Model:\n",
      "Loss: 3.0862, Truth F1: 0.1476, Lie F1: 0.9342, Macro F1: 0.5409\n",
      "Confusion Matrix:\n",
      "[[  29  211]\n",
      " [ 124 2377]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(TRANSFORMER_MODEL)\n",
    "\n",
    "print(\"Loading datasets...\")\n",
    "train_dataset = EnhancedDeceptionDataset(TRAIN_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "val_dataset = EnhancedDeceptionDataset(VAL_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "test_dataset = EnhancedDeceptionDataset(TEST_PATH, tokenizer, use_game_scores=USE_GAME_SCORES)\n",
    "\n",
    "train_sampler = EnhancedBalancedSampler(train_dataset, oversample_factor=OVERSAMPLING_FACTOR)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler,\n",
    "    num_workers=2,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=custom_collate_fn\n",
    ")\n",
    "\n",
    "train_class_counts = train_dataset.class_counts\n",
    "total_samples = sum(train_class_counts.values())\n",
    "weight_0 = total_samples / (train_class_counts.get(0, 1) * 2)\n",
    "weight_1 = total_samples / (train_class_counts.get(1, 1) * 2)\n",
    "class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(DEVICE)\n",
    "print(f\"Class weights: Truth = {weight_0:.4f}, Lie = {weight_1:.4f}\")\n",
    "\n",
    "print(\"Initializing improved model...\")\n",
    "model = ImprovedDeceptionModel(TRANSFORMER_MODEL, use_game_scores=USE_GAME_SCORES).to(DEVICE)\n",
    "\n",
    "print(\"\\nEvaluating best model (by Truth F1) on test set:\")\n",
    "model.load_state_dict(torch.load('best_truth_f1_model.pt'))\n",
    "test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n",
    "    model, test_loader, DEVICE, class_weights,\n",
    "    truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    ")\n",
    "print(f\"\\nTest Results - Truth F1 Model:\")\n",
    "print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_cm)\n",
    "\n",
    "print(\"\\nEvaluating best model (by Macro F1) on test set:\")\n",
    "model.load_state_dict(torch.load('best_macro_f1_model.pt'))\n",
    "test_loss, test_truth_f1, test_lie_f1, test_macro_f1, test_cm = evaluate(\n",
    "    model, test_loader, DEVICE, class_weights,\n",
    "    truth_focal_weight=TRUTH_FOCAL_WEIGHT\n",
    ")\n",
    "print(f\"\\nTest Results - Macro F1 Model:\")\n",
    "print(f\"Loss: {test_loss:.4f}, Truth F1: {test_truth_f1:.4f}, Lie F1: {test_lie_f1:.4f}, Macro F1: {test_macro_f1:.4f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(test_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a945954-02b1-4215-b3d1-07d3eafe9ddb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-11T03:11:12.690776Z",
     "iopub.status.busy": "2025-04-11T03:11:12.690441Z",
     "iopub.status.idle": "2025-04-11T03:11:12.866640Z",
     "shell.execute_reply": "2025-04-11T03:11:12.865663Z",
     "shell.execute_reply.started": "2025-04-11T03:11:12.690743Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "zip error: Nothing to do! (/kaggle/working/.zip)\n"
     ]
    }
   ],
   "source": [
    "! zip -r /kaggle/working/.zip "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2890f9-124b-4245-9cb3-17088415b5c9",
   "metadata": {},
   "source": [
    "## End of Notebook\n",
    "\n",
    "This notebook implements the improved deception detection model with enhanced context encoding, graph attention features, and ensemble classification. Experiment with regularization, learning rates, oversampling, or data augmentation as needed."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7105583,
     "sourceId": 11354484,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
